{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\lm\\vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Counter, Iterable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import lib.helper as helper\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Define which stemmer to use in the pipeline later\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"disaster\" corpus works likewise, but with keywords relating to natural disasters\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 459\n",
      "9.8 percent of files read.\n",
      "19.6 percent of files read.\n",
      "29.4 percent of files read.\n",
      "39.2 percent of files read.\n",
      "49.0 percent of files read.\n",
      "58.8 percent of files read.\n",
      "68.6 percent of files read.\n",
      "78.4 percent of files read.\n",
      "88.2 percent of files read.\n",
      "98.0 percent of files read.\n",
      "(113120, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts cost</td>\n",
       "      <td>Life-threatening US storm surges are feared, a...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:03:44 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-us-canada-495...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "      <td>They are relatives of a group of Pakistani wom...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:53:17 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-49592540</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' released</td>\n",
       "      <td>A Ukrainian court releases a potentially key w...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:46:06 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49591148</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Article 370: The weddings 'ruined' by Kashmir'...</td>\n",
       "      <td>Indian-administered Kashmir is under a securit...</td>\n",
       "      <td>Thu, 05 Sep 2019 07:32:34 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-49...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Article 70: The weddings 'ruined' by Kashmir's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "      <td>President Erdogan demands international help t...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:11:48 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49599297</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  Hurricane Dorian lashes US as Bahamas counts cost   \n",
       "1         1  Kohistan video murders: Three guilty in 'honou...   \n",
       "2         2   MH17 Ukraine plane crash: 'Key witness' released   \n",
       "3         3  Article 370: The weddings 'ruined' by Kashmir'...   \n",
       "4         4  Syria war: Turkey warns Europe of new migrant ...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Life-threatening US storm surges are feared, a...   \n",
       "1     They are relatives of a group of Pakistani wom...   \n",
       "2     A Ukrainian court releases a potentially key w...   \n",
       "3     Indian-administered Kashmir is under a securit...   \n",
       "4     President Erdogan demands international help t...   \n",
       "\n",
       "                               date  \\\n",
       "node                                  \n",
       "0     Thu, 05 Sep 2019 16:03:44 GMT   \n",
       "1     Thu, 05 Sep 2019 13:53:17 GMT   \n",
       "2     Thu, 05 Sep 2019 13:46:06 GMT   \n",
       "3     Thu, 05 Sep 2019 07:32:34 GMT   \n",
       "4     Thu, 05 Sep 2019 16:11:48 GMT   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0     https://www.bbc.co.uk/news/world-us-canada-495...   \n",
       "1        https://www.bbc.co.uk/news/world-asia-49592540   \n",
       "2      https://www.bbc.co.uk/news/world-europe-49591148   \n",
       "3     https://www.bbc.co.uk/news/world-asia-india-49...   \n",
       "4      https://www.bbc.co.uk/news/world-europe-49599297   \n",
       "\n",
       "                                      source_url         retrieval_timestamp  \\\n",
       "node                                                                           \n",
       "0     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "1     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "2     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "3     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "4     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "\n",
       "        origin                                         clean_text  \n",
       "node                                                               \n",
       "0     rss_feed  Hurricane Dorian lashes US as Bahamas counts c...  \n",
       "1     rss_feed  Kohistan video murders: Three guilty in 'honou...  \n",
       "2     rss_feed  MH17 Ukraine plane crash: 'Key witness' releas...  \n",
       "3     rss_feed  Article 70: The weddings 'ruined' by Kashmir's...  \n",
       "4     rss_feed  Syria war: Turkey warns Europe of new migrant ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\"\n",
    "\n",
    "# Load up\n",
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Additional preprocessing for LDA\n",
    "\n",
    "### TODO:  Dump the stuff below into another \"embedding model\" in lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113115</th>\n",
       "      <td>1241258</td>\n",
       "      <td>Lincolnshire zoos told they can reopen - but i...</td>\n",
       "      <td>Zoos and safari parks in England can reopen fr...</td>\n",
       "      <td>Wed, 10 Jun 2020 14:15:58 +0000</td>\n",
       "      <td>https://www.lincolnshirelive.co.uk/news/local-...</td>\n",
       "      <td>http://lincolnshirelive.co.uk/news/?service=rss</td>\n",
       "      <td>2020-06-10 21:33:35.747199</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Lincolnshire zoos told they can reopen - but i...</td>\n",
       "      <td>[lincolnshir, zoo, told, reopen, won, easi, zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113116</th>\n",
       "      <td>1241260</td>\n",
       "      <td>What the shops are doing to get ready for reop...</td>\n",
       "      <td>It's a whole new world</td>\n",
       "      <td>Wed, 10 Jun 2020 12:45:27 +0000</td>\n",
       "      <td>https://www.lincolnshirelive.co.uk/news/local-...</td>\n",
       "      <td>http://lincolnshirelive.co.uk/news/?service=rss</td>\n",
       "      <td>2020-06-10 21:33:35.747199</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>What the shops are doing to get ready for reop...</td>\n",
       "      <td>[shop, readi, reopen, june, new, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113117</th>\n",
       "      <td>1241261</td>\n",
       "      <td>Employees at firm with branches across Lincoln...</td>\n",
       "      <td>The business says it is 'in the process of mak...</td>\n",
       "      <td>Wed, 10 Jun 2020 12:29:19 +0000</td>\n",
       "      <td>https://www.lincolnshirelive.co.uk/news/local-...</td>\n",
       "      <td>http://lincolnshirelive.co.uk/news/?service=rss</td>\n",
       "      <td>2020-06-10 21:33:35.747199</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Employees at firm with branches across Lincoln...</td>\n",
       "      <td>[employe, firm, branch, lincolnshir, face, red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113118</th>\n",
       "      <td>1241263</td>\n",
       "      <td>Woman heartbroken as brother with ‘huge heart’...</td>\n",
       "      <td>'He was on his own for two days before he died...</td>\n",
       "      <td>Wed, 10 Jun 2020 11:56:09 +0000</td>\n",
       "      <td>https://www.lincolnshirelive.co.uk/news/local-...</td>\n",
       "      <td>http://lincolnshirelive.co.uk/news/?service=rss</td>\n",
       "      <td>2020-06-10 21:33:35.747199</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Woman heartbroken as brother with ‘huge heart’...</td>\n",
       "      <td>[woman, heartbroken, brother, huge, heart, die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113119</th>\n",
       "      <td>1241264</td>\n",
       "      <td>Boy left with life-threatening injuries after ...</td>\n",
       "      <td>Emergency services rushed to the scene.</td>\n",
       "      <td>Wed, 10 Jun 2020 10:00:12 +0000</td>\n",
       "      <td>https://www.lincolnshirelive.co.uk/news/local-...</td>\n",
       "      <td>http://lincolnshirelive.co.uk/news/?service=rss</td>\n",
       "      <td>2020-06-10 21:33:35.747199</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Boy left with life-threatening injuries after ...</td>\n",
       "      <td>[boy, left, life, threaten, injuri, fall, roof...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index                                              title  \\\n",
       "node                                                                 \n",
       "113115  1241258  Lincolnshire zoos told they can reopen - but i...   \n",
       "113116  1241260  What the shops are doing to get ready for reop...   \n",
       "113117  1241261  Employees at firm with branches across Lincoln...   \n",
       "113118  1241263  Woman heartbroken as brother with ‘huge heart’...   \n",
       "113119  1241264  Boy left with life-threatening injuries after ...   \n",
       "\n",
       "                                                  summary  \\\n",
       "node                                                        \n",
       "113115  Zoos and safari parks in England can reopen fr...   \n",
       "113116                             It's a whole new world   \n",
       "113117  The business says it is 'in the process of mak...   \n",
       "113118  'He was on his own for two days before he died...   \n",
       "113119            Emergency services rushed to the scene.   \n",
       "\n",
       "                                   date  \\\n",
       "node                                      \n",
       "113115  Wed, 10 Jun 2020 14:15:58 +0000   \n",
       "113116  Wed, 10 Jun 2020 12:45:27 +0000   \n",
       "113117  Wed, 10 Jun 2020 12:29:19 +0000   \n",
       "113118  Wed, 10 Jun 2020 11:56:09 +0000   \n",
       "113119  Wed, 10 Jun 2020 10:00:12 +0000   \n",
       "\n",
       "                                                     link  \\\n",
       "node                                                        \n",
       "113115  https://www.lincolnshirelive.co.uk/news/local-...   \n",
       "113116  https://www.lincolnshirelive.co.uk/news/local-...   \n",
       "113117  https://www.lincolnshirelive.co.uk/news/local-...   \n",
       "113118  https://www.lincolnshirelive.co.uk/news/local-...   \n",
       "113119  https://www.lincolnshirelive.co.uk/news/local-...   \n",
       "\n",
       "                                             source_url  \\\n",
       "node                                                      \n",
       "113115  http://lincolnshirelive.co.uk/news/?service=rss   \n",
       "113116  http://lincolnshirelive.co.uk/news/?service=rss   \n",
       "113117  http://lincolnshirelive.co.uk/news/?service=rss   \n",
       "113118  http://lincolnshirelive.co.uk/news/?service=rss   \n",
       "113119  http://lincolnshirelive.co.uk/news/?service=rss   \n",
       "\n",
       "               retrieval_timestamp    origin  \\\n",
       "node                                           \n",
       "113115  2020-06-10 21:33:35.747199  rss_feed   \n",
       "113116  2020-06-10 21:33:35.747199  rss_feed   \n",
       "113117  2020-06-10 21:33:35.747199  rss_feed   \n",
       "113118  2020-06-10 21:33:35.747199  rss_feed   \n",
       "113119  2020-06-10 21:33:35.747199  rss_feed   \n",
       "\n",
       "                                               clean_text  \\\n",
       "node                                                        \n",
       "113115  Lincolnshire zoos told they can reopen - but i...   \n",
       "113116  What the shops are doing to get ready for reop...   \n",
       "113117  Employees at firm with branches across Lincoln...   \n",
       "113118  Woman heartbroken as brother with ‘huge heart’...   \n",
       "113119  Boy left with life-threatening injuries after ...   \n",
       "\n",
       "                                                   tokens  \n",
       "node                                                       \n",
       "113115  [lincolnshir, zoo, told, reopen, won, easi, zo...  \n",
       "113116            [shop, readi, reopen, june, new, world]  \n",
       "113117  [employe, firm, branch, lincolnshir, face, red...  \n",
       "113118  [woman, heartbroken, brother, huge, heart, die...  \n",
       "113119  [boy, left, life, threaten, injuri, fall, roof...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick utility function to pre-process the text\n",
    "def preprocess_desc(description):\n",
    "    return( [stemmer.stem(token) for token in simple_preprocess(str(description)) if token not in STOPWORDS] )\n",
    "\n",
    "corpus[\"tokens\"] = corpus[\"clean_text\"].apply(preprocess_desc)\n",
    "\n",
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node\n",
       "0    [(0, 0.5844165488435726), (1, 0.18465724404048...\n",
       "1    [(15, 0.26499582841098673), (16, 0.33914476848...\n",
       "2    [(29, 0.18823052848058422), (30, 0.12822580194...\n",
       "3    [(27, 0.2186338723464504), (42, 0.320511124676...\n",
       "4    [(53, 0.21566407963033624), (54, 0.19632019679...\n",
       "Name: corpus_tfidf, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vocabulary record\n",
    "dictionary = gensim.corpora.Dictionary(corpus['tokens'])\n",
    "\n",
    "# Remove extreme values (words that are too rare, too common)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a BOW model\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus['tokens']]\n",
    "\n",
    "# From that create the TF-IDF model\n",
    "# THIS IS ANOTHER POINT THE CORPUS ORDERING COULD DETATCH FROM THE RAW DATA ORDERING\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus['corpus_tfidf'] = tfidf[bow_corpus]\n",
    "\n",
    "corpus['corpus_tfidf'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing a range of different-sized LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(7)\n",
    "trainset, testset = train_test_split(corpus, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tried 3 topics perplexity = -9.717070055431488 coherence = -3.275418169279624\n",
      "tried 4 topics perplexity = -9.98696780137105 coherence = -3.918239309327092\n",
      "tried 5 topics perplexity = -10.31632201701593 coherence = -5.641808833341678\n",
      "tried 6 topics perplexity = -10.523428508610781 coherence = -6.711458481706693\n",
      "tried 7 topics perplexity = -10.741742861559784 coherence = -7.543547893846557\n",
      "tried 8 topics perplexity = -10.934949187999004 coherence = -7.410943799692644\n",
      "tried 9 topics perplexity = -11.213029324094038 coherence = -8.890806943827421\n",
      "tried 10 topics perplexity = -11.521388104323158 coherence = -8.74814174162655\n",
      "tried 11 topics perplexity = -11.978111746344286 coherence = -8.807782415232436\n",
      "tried 12 topics perplexity = -12.450271408993284 coherence = -9.209376643957297\n",
      "tried 13 topics perplexity = -12.844158813579533 coherence = -8.74743741182825\n",
      "tried 14 topics perplexity = -13.091062029935214 coherence = -9.495897328157296\n",
      "tried 15 topics perplexity = -13.367202813604624 coherence = -9.743250128945936\n",
      "tried 16 topics perplexity = -13.590454213044092 coherence = -10.670348476362024\n",
      "tried 17 topics perplexity = -13.796856167851338 coherence = -10.390331140743454\n",
      "tried 18 topics perplexity = -14.071133564263524 coherence = -9.311097001438869\n",
      "tried 19 topics perplexity = -14.292343674682257 coherence = -10.70635402618658\n",
      "tried 20 topics perplexity = -14.494667977892659 coherence = -10.73515908997075\n",
      "tried 21 topics perplexity = -14.75000466215028 coherence = -12.158459706306754\n",
      "tried 22 topics perplexity = -14.955222481631914 coherence = -11.78737501214992\n",
      "tried 23 topics perplexity = -15.161352061234428 coherence = -10.964051166228382\n",
      "tried 24 topics perplexity = -15.418627549439336 coherence = -11.96266193312971\n",
      "tried 25 topics perplexity = -15.636829730403218 coherence = -11.101826471325047\n",
      "tried 26 topics perplexity = -15.86044957309102 coherence = -12.229565173186963\n",
      "tried 27 topics perplexity = -16.05547251676981 coherence = -11.809365449853257\n",
      "tried 28 topics perplexity = -16.281193939930542 coherence = -11.831395333996747\n",
      "tried 29 topics perplexity = -16.497575660751878 coherence = -12.373151735574694\n",
      "tried 30 topics perplexity = -16.700733929230708 coherence = -13.022095800553467\n",
      "tried 31 topics perplexity = -16.93470151090846 coherence = -13.331328537776242\n",
      "tried 32 topics perplexity = -17.101643377368795 coherence = -13.27886378874792\n",
      "tried 33 topics perplexity = -17.339157873739865 coherence = -13.34853343212664\n",
      "tried 34 topics perplexity = -17.603085043266233 coherence = -13.375652363187097\n",
      "tried 35 topics perplexity = -17.838435009308107 coherence = -13.137782917511789\n"
     ]
    }
   ],
   "source": [
    "# Loop through a number of different topic model sizes\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for num_topics in range(3, 101):\n",
    "\n",
    "    # Fit the lda model, with [num_topics] topics\n",
    "    lda_model_tfidf = LdaModel(trainset['corpus_tfidf'],\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=dictionary,\n",
    "                               passes=2)\n",
    "    \n",
    "    # Get the perplexity\n",
    "    perplexity = lda_model_tfidf.log_perplexity(testset['corpus_tfidf'])\n",
    "    \n",
    "    # Get the coherence\n",
    "    cm = CoherenceModel(model=lda_model_tfidf, corpus=testset['corpus_tfidf'], coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    \n",
    "    # record\n",
    "    results = results.append({\"topics\":num_topics, \"perplexity\":perplexity, \"coherence\":coherence}, ignore_index=True)\n",
    "    \n",
    "    # Report for my convenience\n",
    "    print(\"tried {} topics\".format(num_topics), \"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "results.to_csv(\"working/disaster_lda_stats_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - PERPLEXITY\n",
    "sns.scatterplot(x=\"topics\", y=\"perplexity\", color=\"blue\", data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - COHERENCE\n",
    "sns.scatterplot(x=\"topics\", y=\"coherence\", color=\"red\", data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the \"best\" model using all data\n",
    "\n",
    "And parameters decided by test-time performance on perplexity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity = -47.24009145269341 coherence = -6.788528954290477\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# Fit the final lda model to all data\n",
    "lda_model_tfidf = LdaModel(corpus['corpus_tfidf'],\n",
    "                           num_topics=76,\n",
    "                           id2word=dictionary,\n",
    "                           passes=2)\n",
    "\n",
    "# Get the perplexity, out of curiosity\n",
    "perplexity = lda_model_tfidf.log_perplexity(corpus['corpus_tfidf'])\n",
    "    \n",
    "# Get the coherence, out of curiosity\n",
    "cm = CoherenceModel(model=lda_model_tfidf, corpus=corpus['corpus_tfidf'], coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "\n",
    "print(\"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "lda_model_tfidf.save(\"working/lda_model_tfidf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.119*\"summer\" + 0.097*\"hope\" + 0.073*\"suffer\" + 0.070*\"leav\" + 0.049*\"soon\" + 0.038*\"return\" + 0.038*\"properti\" + 0.032*\"enjoy\" + 0.030*\"sunday\" + 0.030*\"usual\"\n",
      "Topic: 1 Word: 0.151*\"measur\" + 0.146*\"staff\" + 0.066*\"launch\" + 0.057*\"taken\" + 0.053*\"spot\" + 0.047*\"spend\" + 0.043*\"critic\" + 0.043*\"review\" + 0.032*\"new\" + 0.030*\"ensur\"\n",
      "Topic: 2 Word: 0.066*\"lead\" + 0.048*\"given\" + 0.035*\"read\" + 0.032*\"leed\" + 0.028*\"closur\" + 0.024*\"green\" + 0.023*\"urgent\" + 0.023*\"quot\" + 0.022*\"facebook\" + 0.021*\"difficult\"\n",
      "Topic: 3 Word: 0.075*\"model\" + 0.073*\"greater\" + 0.061*\"previous\" + 0.061*\"king\" + 0.053*\"cambridg\" + 0.048*\"improv\" + 0.046*\"shown\" + 0.042*\"digit\" + 0.042*\"combin\" + 0.031*\"card\"\n",
      "Topic: 4 Word: 0.116*\"wildfir\" + 0.061*\"firefight\" + 0.058*\"crew\" + 0.056*\"control\" + 0.052*\"firm\" + 0.042*\"save\" + 0.036*\"blaze\" + 0.033*\"servic\" + 0.027*\"thank\" + 0.026*\"energi\"\n",
      "Topic: 5 Word: 0.280*\"arriv\" + 0.143*\"brit\" + 0.066*\"gener\" + 0.054*\"see\" + 0.049*\"uk\" + 0.036*\"come\" + 0.032*\"poll\" + 0.029*\"zero\" + 0.026*\"elect\" + 0.019*\"throw\"\n",
      "Topic: 6 Word: 0.038*\"call\" + 0.038*\"member\" + 0.031*\"began\" + 0.029*\"cost\" + 0.027*\"devast\" + 0.026*\"damag\" + 0.026*\"thought\" + 0.026*\"insur\" + 0.024*\"flood\" + 0.024*\"join\"\n",
      "Topic: 7 Word: 0.096*\"open\" + 0.080*\"crisi\" + 0.077*\"london\" + 0.070*\"forc\" + 0.061*\"daili\" + 0.038*\"peopl\" + 0.037*\"emerg\" + 0.032*\"attack\" + 0.029*\"disrupt\" + 0.029*\"million\"\n",
      "Topic: 8 Word: 0.092*\"polici\" + 0.068*\"cut\" + 0.066*\"ad\" + 0.056*\"french\" + 0.055*\"farmer\" + 0.043*\"understand\" + 0.043*\"better\" + 0.043*\"seek\" + 0.040*\"miss\" + 0.037*\"account\"\n",
      "Topic: 9 Word: 0.152*\"restrict\" + 0.083*\"parent\" + 0.062*\"struggl\" + 0.058*\"daughter\" + 0.058*\"initi\" + 0.052*\"law\" + 0.045*\"girl\" + 0.044*\"sell\" + 0.035*\"encourag\" + 0.034*\"desper\"\n",
      "Topic: 10 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 11 Word: 0.081*\"airport\" + 0.061*\"flight\" + 0.052*\"royal\" + 0.047*\"watch\" + 0.037*\"suspend\" + 0.034*\"hundr\" + 0.030*\"travel\" + 0.029*\"typhoon\" + 0.024*\"perfect\" + 0.024*\"power\"\n",
      "Topic: 12 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 13 Word: 0.112*\"support\" + 0.085*\"video\" + 0.070*\"take\" + 0.060*\"consid\" + 0.040*\"inform\" + 0.038*\"depart\" + 0.034*\"instagram\" + 0.030*\"view\" + 0.027*\"husband\" + 0.022*\"mike\"\n",
      "Topic: 14 Word: 0.157*\"spread\" + 0.058*\"heart\" + 0.053*\"have\" + 0.047*\"grow\" + 0.044*\"brief\" + 0.039*\"stage\" + 0.027*\"light\" + 0.027*\"perform\" + 0.023*\"festiv\" + 0.022*\"simul\"\n",
      "Topic: 15 Word: 0.070*\"impact\" + 0.070*\"intern\" + 0.062*\"chang\" + 0.057*\"global\" + 0.053*\"climat\" + 0.053*\"threat\" + 0.047*\"suppli\" + 0.040*\"secur\" + 0.032*\"warm\" + 0.030*\"report\"\n",
      "Topic: 16 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 17 Word: 0.107*\"celebr\" + 0.083*\"ahead\" + 0.080*\"film\" + 0.066*\"situat\" + 0.055*\"produc\" + 0.050*\"histori\" + 0.048*\"prompt\" + 0.046*\"monitor\" + 0.044*\"cook\" + 0.043*\"camp\"\n",
      "Topic: 18 Word: 0.105*\"counti\" + 0.088*\"drive\" + 0.085*\"white\" + 0.063*\"driver\" + 0.053*\"saw\" + 0.050*\"problem\" + 0.036*\"derbyshir\" + 0.023*\"thursday\" + 0.021*\"road\" + 0.019*\"snow\"\n",
      "Topic: 19 Word: 0.202*\"outbreak\" + 0.121*\"case\" + 0.119*\"death\" + 0.076*\"plagu\" + 0.053*\"scientist\" + 0.039*\"world\" + 0.037*\"avoid\" + 0.036*\"allow\" + 0.032*\"black\" + 0.031*\"latest\"\n",
      "Topic: 20 Word: 0.136*\"cancer\" + 0.090*\"get\" + 0.085*\"leader\" + 0.076*\"mum\" + 0.062*\"receiv\" + 0.062*\"amp\" + 0.048*\"minut\" + 0.043*\"win\" + 0.043*\"smith\" + 0.039*\"polit\"\n",
      "Topic: 21 Word: 0.118*\"train\" + 0.106*\"cover\" + 0.081*\"pm\" + 0.076*\"friday\" + 0.069*\"longer\" + 0.062*\"extend\" + 0.060*\"tomorrow\" + 0.040*\"probabl\" + 0.026*\"journey\" + 0.021*\"today\"\n",
      "Topic: 22 Word: 0.029*\"snow\" + 0.028*\"warn\" + 0.028*\"weather\" + 0.024*\"weekend\" + 0.023*\"uk\" + 0.022*\"go\" + 0.020*\"offic\" + 0.020*\"storm\" + 0.019*\"forecast\" + 0.019*\"rain\"\n",
      "Topic: 23 Word: 0.095*\"centr\" + 0.091*\"shop\" + 0.088*\"share\" + 0.087*\"peak\" + 0.077*\"total\" + 0.066*\"stori\" + 0.057*\"highest\" + 0.055*\"sheffield\" + 0.044*\"district\" + 0.039*\"headlin\"\n",
      "Topic: 24 Word: 0.072*\"hurrican\" + 0.041*\"australia\" + 0.039*\"island\" + 0.039*\"creat\" + 0.036*\"thing\" + 0.034*\"move\" + 0.033*\"away\" + 0.031*\"parti\" + 0.031*\"safeti\" + 0.026*\"fire\"\n",
      "Topic: 25 Word: 0.289*\"start\" + 0.062*\"ground\" + 0.056*\"averag\" + 0.049*\"sound\" + 0.044*\"decemb\" + 0.028*\"sleet\" + 0.027*\"horizon\" + 0.018*\"slightli\" + 0.016*\"time\" + 0.012*\"year\"\n",
      "Topic: 26 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 27 Word: 0.064*\"minist\" + 0.046*\"school\" + 0.044*\"johnson\" + 0.043*\"bori\" + 0.042*\"research\" + 0.035*\"govern\" + 0.034*\"uk\" + 0.034*\"plan\" + 0.033*\"univers\" + 0.031*\"dr\"\n",
      "Topic: 28 Word: 0.235*\"scotland\" + 0.101*\"rais\" + 0.077*\"littl\" + 0.057*\"comment\" + 0.056*\"hill\" + 0.032*\"horror\" + 0.019*\"odd\" + 0.016*\"come\" + 0.013*\"look\" + 0.012*\"time\"\n",
      "Topic: 29 Word: 0.047*\"secretari\" + 0.047*\"figur\" + 0.041*\"mp\" + 0.039*\"street\" + 0.035*\"govern\" + 0.029*\"believ\" + 0.028*\"local\" + 0.027*\"peopl\" + 0.026*\"offici\" + 0.025*\"demand\"\n",
      "Topic: 30 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 31 Word: 0.136*\"loss\" + 0.100*\"princ\" + 0.062*\"paper\" + 0.044*\"consequ\" + 0.044*\"charl\" + 0.044*\"knock\" + 0.037*\"wit\" + 0.033*\"ethnic\" + 0.033*\"habitat\" + 0.019*\"peopl\"\n",
      "Topic: 32 Word: 0.107*\"mark\" + 0.073*\"high\" + 0.069*\"normal\" + 0.068*\"surg\" + 0.059*\"declar\" + 0.040*\"st\" + 0.039*\"tide\" + 0.036*\"mayor\" + 0.027*\"seen\" + 0.023*\"euro\"\n",
      "Topic: 33 Word: 0.195*\"care\" + 0.069*\"cancel\" + 0.068*\"polic\" + 0.068*\"urg\" + 0.052*\"deal\" + 0.051*\"major\" + 0.048*\"author\" + 0.046*\"current\" + 0.042*\"close\" + 0.038*\"shut\"\n",
      "Topic: 34 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 35 Word: 0.078*\"stay\" + 0.055*\"home\" + 0.044*\"resid\" + 0.040*\"effort\" + 0.032*\"east\" + 0.031*\"refus\" + 0.031*\"space\" + 0.026*\"fell\" + 0.023*\"remov\" + 0.019*\"lane\"\n",
      "Topic: 36 Word: 0.076*\"public\" + 0.058*\"concern\" + 0.057*\"fear\" + 0.051*\"drop\" + 0.045*\"potenti\" + 0.044*\"reduc\" + 0.039*\"despit\" + 0.031*\"worri\" + 0.031*\"peopl\" + 0.030*\"transport\"\n",
      "Topic: 37 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 38 Word: 0.084*\"key\" + 0.084*\"talk\" + 0.065*\"afternoon\" + 0.061*\"chanc\" + 0.059*\"small\" + 0.053*\"point\" + 0.052*\"even\" + 0.032*\"replac\" + 0.026*\"cop\" + 0.021*\"persist\"\n",
      "Topic: 39 Word: 0.061*\"british\" + 0.054*\"work\" + 0.050*\"develop\" + 0.037*\"compani\" + 0.032*\"hand\" + 0.028*\"european\" + 0.028*\"cent\" + 0.027*\"track\" + 0.021*\"busi\" + 0.019*\"jet\"\n",
      "Topic: 40 Word: 0.172*\"air\" + 0.068*\"pressur\" + 0.067*\"fresh\" + 0.066*\"spark\" + 0.066*\"activ\" + 0.043*\"rang\" + 0.041*\"deep\" + 0.037*\"chart\" + 0.037*\"sweep\" + 0.030*\"signal\"\n",
      "Topic: 41 Word: 0.274*\"die\" + 0.068*\"car\" + 0.067*\"got\" + 0.066*\"tri\" + 0.054*\"death\" + 0.046*\"outsid\" + 0.036*\"stuck\" + 0.035*\"buri\" + 0.028*\"richard\" + 0.024*\"dead\"\n",
      "Topic: 42 Word: 0.000*\"hard\" + 0.000*\"engin\" + 0.000*\"friday\" + 0.000*\"import\" + 0.000*\"morn\" + 0.000*\"equip\" + 0.000*\"central\" + 0.000*\"platform\" + 0.000*\"caus\" + 0.000*\"clear\"\n",
      "Topic: 43 Word: 0.264*\"studi\" + 0.082*\"estim\" + 0.063*\"doubl\" + 0.039*\"hectar\" + 0.035*\"flow\" + 0.029*\"ga\" + 0.024*\"emiss\" + 0.020*\"claim\" + 0.013*\"day\" + 0.012*\"burnt\"\n",
      "Topic: 44 Word: 0.144*\"state\" + 0.125*\"unit\" + 0.104*\"prepar\" + 0.100*\"break\" + 0.077*\"american\" + 0.057*\"deliv\" + 0.032*\"cyclon\" + 0.024*\"bomb\" + 0.018*\"reel\" + 0.017*\"citi\"\n",
      "Topic: 45 Word: 0.099*\"plan\" + 0.082*\"travel\" + 0.056*\"team\" + 0.045*\"delay\" + 0.043*\"hotel\" + 0.032*\"new\" + 0.031*\"came\" + 0.029*\"bodi\" + 0.029*\"arriv\" + 0.025*\"late\"\n",
      "Topic: 46 Word: 0.074*\"broke\" + 0.066*\"pictur\" + 0.059*\"angel\" + 0.058*\"getti\" + 0.051*\"paul\" + 0.044*\"art\" + 0.042*\"famou\" + 0.037*\"lo\" + 0.035*\"artist\" + 0.032*\"piec\"\n",
      "Topic: 47 Word: 0.085*\"passeng\" + 0.064*\"oper\" + 0.060*\"order\" + 0.059*\"woman\" + 0.057*\"rescu\" + 0.057*\"advis\" + 0.057*\"product\" + 0.048*\"queen\" + 0.031*\"theatr\" + 0.026*\"escap\"\n",
      "Topic: 48 Word: 0.084*\"season\" + 0.077*\"end\" + 0.067*\"leagu\" + 0.043*\"final\" + 0.038*\"form\" + 0.034*\"day\" + 0.033*\"detail\" + 0.029*\"term\" + 0.029*\"long\" + 0.028*\"insist\"\n",
      "Topic: 49 Word: 0.069*\"help\" + 0.063*\"ask\" + 0.056*\"tackl\" + 0.047*\"pay\" + 0.045*\"effect\" + 0.039*\"safe\" + 0.037*\"director\" + 0.033*\"question\" + 0.027*\"futur\" + 0.025*\"entir\"\n",
      "Topic: 50 Word: 0.164*\"industri\" + 0.105*\"data\" + 0.088*\"human\" + 0.050*\"describ\" + 0.040*\"money\" + 0.037*\"metr\" + 0.036*\"bn\" + 0.035*\"pain\" + 0.024*\"alarm\" + 0.022*\"spokesperson\"\n",
      "Topic: 51 Word: 0.118*\"earli\" + 0.066*\"appear\" + 0.063*\"shock\" + 0.063*\"past\" + 0.061*\"man\" + 0.048*\"ve\" + 0.043*\"happen\" + 0.040*\"claim\" + 0.033*\"coupl\" + 0.032*\"show\"\n",
      "Topic: 52 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 53 Word: 0.069*\"itali\" + 0.060*\"franc\" + 0.054*\"kill\" + 0.053*\"thousand\" + 0.051*\"stop\" + 0.049*\"anim\" + 0.034*\"intens\" + 0.033*\"countri\" + 0.033*\"travel\" + 0.031*\"lose\"\n",
      "Topic: 54 Word: 0.071*\"toll\" + 0.059*\"old\" + 0.044*\"year\" + 0.036*\"son\" + 0.035*\"ago\" + 0.034*\"age\" + 0.034*\"manchest\" + 0.034*\"line\" + 0.033*\"friend\" + 0.032*\"life\"\n",
      "Topic: 55 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 56 Word: 0.039*\"rate\" + 0.035*\"scheme\" + 0.034*\"europ\" + 0.033*\"challeng\" + 0.031*\"base\" + 0.029*\"market\" + 0.029*\"matt\" + 0.027*\"new\" + 0.023*\"econom\" + 0.020*\"face\"\n",
      "Topic: 57 Word: 0.171*\"tell\" + 0.109*\"speak\" + 0.040*\"told\" + 0.039*\"rapid\" + 0.038*\"murder\" + 0.033*\"god\" + 0.033*\"israel\" + 0.025*\"infant\" + 0.022*\"truth\" + 0.014*\"fear\"\n",
      "Topic: 58 Word: 0.107*\"june\" + 0.087*\"updat\" + 0.083*\"sign\" + 0.071*\"tsunami\" + 0.061*\"park\" + 0.057*\"relat\" + 0.028*\"glasgow\" + 0.027*\"partner\" + 0.021*\"success\" + 0.020*\"crash\"\n",
      "Topic: 59 Word: 0.156*\"yesterday\" + 0.139*\"hour\" + 0.071*\"field\" + 0.070*\"photo\" + 0.037*\"norfolk\" + 0.035*\"reveal\" + 0.031*\"wreak\" + 0.029*\"havoc\" + 0.025*\"day\" + 0.025*\"cambridgeshir\"\n",
      "Topic: 60 Word: 0.098*\"result\" + 0.077*\"low\" + 0.065*\"rainfal\" + 0.056*\"lake\" + 0.053*\"risen\" + 0.052*\"land\" + 0.042*\"spokesman\" + 0.038*\"lie\" + 0.029*\"level\" + 0.021*\"upper\"\n",
      "Topic: 61 Word: 0.169*\"children\" + 0.121*\"need\" + 0.092*\"prevent\" + 0.087*\"protect\" + 0.060*\"think\" + 0.049*\"head\" + 0.023*\"day\" + 0.022*\"look\" + 0.020*\"time\" + 0.014*\"ve\"\n",
      "Topic: 62 Word: 0.086*\"food\" + 0.086*\"half\" + 0.083*\"provid\" + 0.078*\"find\" + 0.068*\"import\" + 0.059*\"farm\" + 0.047*\"sourc\" + 0.046*\"incom\" + 0.043*\"famili\" + 0.035*\"adapt\"\n",
      "Topic: 63 Word: 0.062*\"boss\" + 0.052*\"game\" + 0.047*\"play\" + 0.045*\"admit\" + 0.039*\"fan\" + 0.038*\"quot\" + 0.038*\"seri\" + 0.033*\"good\" + 0.031*\"star\" + 0.029*\"wors\"\n",
      "Topic: 64 Word: 0.110*\"run\" + 0.075*\"citi\" + 0.056*\"separ\" + 0.056*\"garden\" + 0.052*\"event\" + 0.045*\"window\" + 0.039*\"grand\" + 0.033*\"live\" + 0.024*\"height\" + 0.022*\"blog\"\n",
      "Topic: 65 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 66 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 67 Word: 0.106*\"trump\" + 0.047*\"donald\" + 0.047*\"border\" + 0.045*\"biggest\" + 0.045*\"protest\" + 0.045*\"press\" + 0.040*\"spring\" + 0.039*\"associ\" + 0.036*\"moment\" + 0.033*\"equip\"\n",
      "Topic: 68 Word: 0.066*\"manag\" + 0.061*\"bank\" + 0.043*\"flood\" + 0.043*\"town\" + 0.037*\"reportedli\" + 0.031*\"dozen\" + 0.031*\"danger\" + 0.028*\"water\" + 0.028*\"burst\" + 0.027*\"flash\"\n",
      "Topic: 69 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 70 Word: 0.000*\"slide\" + 0.000*\"antrim\" + 0.000*\"flank\" + 0.000*\"slid\" + 0.000*\"gateshead\" + 0.000*\"sharp\" + 0.000*\"krakatau\" + 0.000*\"untreat\" + 0.000*\"doomsday\" + 0.000*\"tyron\"\n",
      "Topic: 71 Word: 0.208*\"suggest\" + 0.142*\"begin\" + 0.064*\"cross\" + 0.057*\"mid\" + 0.052*\"spell\" + 0.047*\"mountain\" + 0.023*\"theme\" + 0.021*\"snap\" + 0.016*\"meteorologist\" + 0.014*\"day\"\n",
      "Topic: 72 Word: 0.082*\"action\" + 0.072*\"requir\" + 0.066*\"alert\" + 0.064*\"huge\" + 0.050*\"agenc\" + 0.049*\"flood\" + 0.049*\"possibl\" + 0.044*\"mean\" + 0.043*\"environ\" + 0.041*\"issu\"\n",
      "Topic: 73 Word: 0.084*\"group\" + 0.059*\"chariti\" + 0.050*\"right\" + 0.041*\"mr\" + 0.038*\"natur\" + 0.038*\"donat\" + 0.030*\"disast\" + 0.027*\"hear\" + 0.026*\"hair\" + 0.025*\"singl\"\n",
      "Topic: 74 Word: 0.055*\"turn\" + 0.051*\"dri\" + 0.045*\"februari\" + 0.041*\"region\" + 0.038*\"fall\" + 0.036*\"snow\" + 0.035*\"temperatur\" + 0.034*\"winter\" + 0.031*\"record\" + 0.028*\"day\"\n",
      "Topic: 75 Word: 0.219*\"social\" + 0.106*\"wale\" + 0.099*\"media\" + 0.084*\"known\" + 0.051*\"post\" + 0.044*\"map\" + 0.043*\"arctic\" + 0.037*\"blast\" + 0.016*\"latest\" + 0.014*\"pictur\"\n"
     ]
    }
   ],
   "source": [
    "# Let's take a quick look at the topics picked out\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summer hope suffer leav soon</td>\n",
       "      <td>X0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>measur staff launch taken spot</td>\n",
       "      <td>X1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lead given read leed closur</td>\n",
       "      <td>X2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model greater previous king cambridg</td>\n",
       "      <td>X3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wildfir firefight crew control firm</td>\n",
       "      <td>X4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>suggest begin cross mid spell</td>\n",
       "      <td>X71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>action requir alert huge agenc</td>\n",
       "      <td>X72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>group chariti right mr natur</td>\n",
       "      <td>X73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>turn dri februari region fall</td>\n",
       "      <td>X74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>social wale media known post</td>\n",
       "      <td>X75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                keywords topic\n",
       "0           summer hope suffer leav soon    X0\n",
       "1         measur staff launch taken spot    X1\n",
       "2            lead given read leed closur    X2\n",
       "3   model greater previous king cambridg    X3\n",
       "4    wildfir firefight crew control firm    X4\n",
       "..                                   ...   ...\n",
       "71         suggest begin cross mid spell   X71\n",
       "72        action requir alert huge agenc   X72\n",
       "73          group chariti right mr natur   X73\n",
       "74         turn dri februari region fall   X74\n",
       "75          social wale media known post   X75\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a quick look at words the topics picked out\n",
    "topic_word_dist = pd.DataFrame()\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    # Record the topic index in a format R will like\n",
    "    topic_index = \"X\" + str(idx)\n",
    "    \n",
    "    # Record the topic's key words as a single string\n",
    "    # Split the words with a newline character while we're at it!\n",
    "    topic_words = \" \".join([x.split(\"*\")[1].replace('\"', '').strip() for x in topic.split(\"+\")][0:5])\n",
    "    \n",
    "    # Dump them to the results dataframe\n",
    "    topic_word_dist = topic_word_dist.append({\"topic\":topic_index, \"keywords\":topic_words}, ignore_index=True)\n",
    "    \n",
    "topic_word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0.320047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0.328762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>0.344483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>0.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>0.235525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cluster     score\n",
       "node                   \n",
       "0          22  0.320047\n",
       "1          72  0.328762\n",
       "2          74  0.344483\n",
       "3          72  0.370900\n",
       "4          22  0.235525"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign topic to document, also store the probability of that topic\n",
    "doc_topics = []\n",
    "\n",
    "count = 0\n",
    "for index, row in corpus.iterrows():\n",
    "    \n",
    "    # Extract list of tuples of (topic, score) from the model for each doc\n",
    "    topics = [x for x in lda_model_tfidf.get_document_topics(row['corpus_tfidf']) ]\n",
    "    \n",
    "    # Find the highest probability topic\n",
    "    highest_scoring = sorted(topics, key=lambda x: x[1], reverse=True)[0]\n",
    "    \n",
    "    temp = {\"node\": int(index),\n",
    "            \"cluster\": highest_scoring[0],\n",
    "            \"score\": highest_scoring[1]}\n",
    "    \n",
    "    # Report processing progress!\n",
    "    count = count + 1\n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    doc_topics.append(temp)\n",
    "\n",
    "doc_topic_dist = pd.DataFrame(doc_topics).set_index(\"node\")\n",
    "\n",
    "doc_topic_dist.to_csv(\"working/doc_topic_dist_backup.csv\")\n",
    "\n",
    "doc_topic_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.join(doc_topic_dist).to_csv(\"working/RSS_clustered_lda.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
