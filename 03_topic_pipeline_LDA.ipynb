{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\lm\\vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Counter, Iterable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lib.helper as helper\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Define which stemmer to use in the pipeline later\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"disaster\" corpus works likewise, but with keywords relating to natural disasters\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 495\n",
      "9.9 percent of files read.\n",
      "19.8 percent of files read.\n",
      "29.7 percent of files read.\n",
      "39.6 percent of files read.\n",
      "49.5 percent of files read.\n",
      "59.4 percent of files read.\n",
      "69.3 percent of files read.\n",
      "79.2 percent of files read.\n",
      "89.1 percent of files read.\n",
      "99.0 percent of files read.\n",
      "(31393, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts cost</td>\n",
       "      <td>Life-threatening US storm surges are feared, a...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:03:44 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-us-canada-495...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "      <td>They are relatives of a group of Pakistani wom...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:53:17 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-49592540</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' released</td>\n",
       "      <td>A Ukrainian court releases a potentially key w...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:46:06 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49591148</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Article 370: The weddings 'ruined' by Kashmir'...</td>\n",
       "      <td>Indian-administered Kashmir is under a securit...</td>\n",
       "      <td>Thu, 05 Sep 2019 07:32:34 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-49...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Article 70: The weddings 'ruined' by Kashmir's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "      <td>President Erdogan demands international help t...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:11:48 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49599297</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  Hurricane Dorian lashes US as Bahamas counts cost   \n",
       "1         1  Kohistan video murders: Three guilty in 'honou...   \n",
       "2         2   MH17 Ukraine plane crash: 'Key witness' released   \n",
       "3         3  Article 370: The weddings 'ruined' by Kashmir'...   \n",
       "4         4  Syria war: Turkey warns Europe of new migrant ...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Life-threatening US storm surges are feared, a...   \n",
       "1     They are relatives of a group of Pakistani wom...   \n",
       "2     A Ukrainian court releases a potentially key w...   \n",
       "3     Indian-administered Kashmir is under a securit...   \n",
       "4     President Erdogan demands international help t...   \n",
       "\n",
       "                               date  \\\n",
       "node                                  \n",
       "0     Thu, 05 Sep 2019 16:03:44 GMT   \n",
       "1     Thu, 05 Sep 2019 13:53:17 GMT   \n",
       "2     Thu, 05 Sep 2019 13:46:06 GMT   \n",
       "3     Thu, 05 Sep 2019 07:32:34 GMT   \n",
       "4     Thu, 05 Sep 2019 16:11:48 GMT   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0     https://www.bbc.co.uk/news/world-us-canada-495...   \n",
       "1        https://www.bbc.co.uk/news/world-asia-49592540   \n",
       "2      https://www.bbc.co.uk/news/world-europe-49591148   \n",
       "3     https://www.bbc.co.uk/news/world-asia-india-49...   \n",
       "4      https://www.bbc.co.uk/news/world-europe-49599297   \n",
       "\n",
       "                                      source_url         retrieval_timestamp  \\\n",
       "node                                                                           \n",
       "0     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "1     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "2     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "3     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "4     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "\n",
       "        origin                                         clean_text  \n",
       "node                                                               \n",
       "0     rss_feed  Hurricane Dorian lashes US as Bahamas counts c...  \n",
       "1     rss_feed  Kohistan video murders: Three guilty in 'honou...  \n",
       "2     rss_feed  MH17 Ukraine plane crash: 'Key witness' releas...  \n",
       "3     rss_feed  Article 70: The weddings 'ruined' by Kashmir's...  \n",
       "4     rss_feed  Syria war: Turkey warns Europe of new migrant ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\"\n",
    "\n",
    "# Load up\n",
    "corpus = helper.load_clean_world_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Additional preprocessing for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get simple cleaned + stem tokens\n",
    "corpus[\"tokens\"] = corpus[\"clean_text\"].apply(helper.preprocess_description)\n",
    "\n",
    "# Get phrase-conjoined, stem tokens\n",
    "corpus['phrase_tokens'] = helper.get_phrased_nouns(corpus['clean_text'])\n",
    "\n",
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.sample(5)[['title', 'summary', 'tokens', 'phrase_tokens']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the vocabulary record\n",
    "dictionary = gensim.corpora.Dictionary(corpus['phrase_tokens'])\n",
    "\n",
    "# Remove extreme values (words that are too rare, too common)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a BOW model\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus['tokens']]\n",
    "\n",
    "# From that create the TF-IDF model\n",
    "# THIS IS ANOTHER POINT THE CORPUS ORDERING COULD DETATCH FROM THE RAW DATA ORDERING\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus['corpus_tfidf'] = tfidf[bow_corpus]\n",
    "\n",
    "corpus['corpus_tfidf'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing a range of different-sized LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(7)\n",
    "trainset, testset = train_test_split(corpus, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through a number of different topic model sizes\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for num_topics in range(40, 121):\n",
    "\n",
    "    # Fit the lda model, with [num_topics] topics\n",
    "    lda_model_tfidf = LdaModel(trainset['corpus_tfidf'],\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=dictionary,\n",
    "                               passes=2)\n",
    "    \n",
    "    # Get the perplexity\n",
    "    perplexity = lda_model_tfidf.log_perplexity(testset['corpus_tfidf'])\n",
    "    \n",
    "    # Get the coherence - using Umass here because it's fast to calculate\n",
    "    cm = CoherenceModel(model=lda_model_tfidf, corpus=testset['corpus_tfidf'], coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    \n",
    "    # record\n",
    "    results = results.append({\"topics\":num_topics, \"perplexity\":perplexity, \"coherence\":coherence}, ignore_index=True)\n",
    "    \n",
    "    # Report for my convenience\n",
    "    print(\"tried {} topics\".format(num_topics), \"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "results.to_csv(\"working/disaster_lda_stats_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - PERPLEXITY\n",
    "sns.scatterplot(x=\"topics\", y=\"perplexity\", color=\"blue\", data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - COHERENCE\n",
    "sns.scatterplot(x=\"topics\", y=\"coherence\", color=\"red\", data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the \"best\" model using all data\n",
    "\n",
    "And parameters decided by test-time performance on perplexity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Fit the final lda model to all data\n",
    "lda_model_tfidf = LdaModel(corpus['corpus_tfidf'],\n",
    "                           num_topics=70,\n",
    "                           id2word=dictionary,\n",
    "                           passes=2)\n",
    "\n",
    "# Get the perplexity, out of curiosity\n",
    "perplexity = lda_model_tfidf.log_perplexity(corpus['corpus_tfidf'])\n",
    "    \n",
    "# Get the coherence, out of curiosity\n",
    "cm = CoherenceModel(model=lda_model_tfidf, corpus=corpus['corpus_tfidf'], coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "\n",
    "print(\"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "#lda_model_tfidf.save(\"working/lda_model_tfidf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the topics picked out\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a quick look at words the topics picked out\n",
    "topic_word_dist = pd.DataFrame()\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    # Record the topic index in a format R will like\n",
    "    topic_index = \"X\" + str(idx)\n",
    "    \n",
    "    # Record the topic's key words as a single string\n",
    "    # Split the words with a newline character while we're at it!\n",
    "    topic_words = \" \".join([x.split(\"*\")[1].replace('\"', '').strip() for x in topic.split(\"+\")][0:5])\n",
    "    \n",
    "    # Dump them to the results dataframe\n",
    "    topic_word_dist = topic_word_dist.append({\"topic\":topic_index, \"keywords\":topic_words}, ignore_index=True)\n",
    "    \n",
    "topic_word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign topic to document, also store the probability of that topic\n",
    "doc_topics = []\n",
    "\n",
    "count = 0\n",
    "for index, row in corpus.iterrows():\n",
    "    \n",
    "    # Extract list of tuples of (topic, score) from the model for each doc\n",
    "    topics = [x for x in lda_model_tfidf.get_document_topics(row['corpus_tfidf']) ]\n",
    "    \n",
    "    # Find the highest probability topic\n",
    "    highest_scoring = sorted(topics, key=lambda x: x[1], reverse=True)[0]\n",
    "    \n",
    "    temp = {\"node\": int(index),\n",
    "            \"cluster\": highest_scoring[0],\n",
    "            \"score\": highest_scoring[1]}\n",
    "    \n",
    "    # Report processing progress!\n",
    "    count = count + 1\n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    doc_topics.append(temp)\n",
    "\n",
    "doc_topic_dist = pd.DataFrame(doc_topics).set_index(\"node\")\n",
    "\n",
    "corpus = corpus.join(doc_topic_dist)\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)\n",
    "\n",
    "corpus.to_csv(\"working/RSS_clustered_lda.csv\", index=False)\n",
    "\n",
    "# What percentage are now classed as outliers?\n",
    "print(\"Percent outlier: \", 100.0 * sum(corpus['cluster']==-1) / corpus.shape[0])\n",
    "\n",
    "# How many unique clusters after all this?\n",
    "print(\"Number of clusters: \", len(pd.unique(corpus['cluster'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the topic distribution, there's no obvious way to draw a line and cut off outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['score'].hist(bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(helper)\n",
    "coherences = helper.report_corpus_model_coherence(\"working/RSS_clustered_lda.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
