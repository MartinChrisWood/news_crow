{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import lib.helper as helper\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Define which stemmer to use in the pipeline later\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"disaster\" corpus works likewise, but with keywords relating to natural disasters\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 155\n",
      "9.7 percent of files read.\n",
      "19.4 percent of files read.\n",
      "29.0 percent of files read.\n",
      "38.7 percent of files read.\n",
      "48.4 percent of files read.\n",
      "58.1 percent of files read.\n",
      "67.7 percent of files read.\n",
      "77.4 percent of files read.\n",
      "87.1 percent of files read.\n",
      "96.8 percent of files read.\n",
      "(12592, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>West Midlands &lt;b&gt;flood&lt;/b&gt; warnings prompt &amp;#3...</td>\n",
       "      <td>Residents have been warned to &amp;quot;remain vig...</td>\n",
       "      <td>2019-11-17T17:35:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-50451817</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278878</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>West Midlands flood warnings prompt ;remain vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>New &lt;b&gt;flood&lt;/b&gt; warnings issued with more hom...</td>\n",
       "      <td>The Environment Agency has a number of &lt;b&gt;floo...</td>\n",
       "      <td>2019-11-17T18:35:00.0000000Z</td>\n",
       "      <td>https://www.hulldailymail.co.uk/news/hull-east...</td>\n",
       "      <td>www.hulldailymail.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278928</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>New flood warnings issued with more homes at r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>UK weather forecast – More than 100 &lt;b&gt;flood&lt;/...</td>\n",
       "      <td>&lt;b&gt;FLOOD&lt;/b&gt;-ravaged villages in the UK have b...</td>\n",
       "      <td>2019-11-17T13:45:00.0000000Z</td>\n",
       "      <td>https://www.thesun.co.uk/news/10342583/uk-weat...</td>\n",
       "      <td>www.thesun.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278953</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast – More than 100 flood aler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>UK &lt;b&gt;flood&lt;/b&gt; warning map: &lt;b&gt;Flood&lt;/b&gt; chao...</td>\n",
       "      <td>The Environment Agency has issued 57 &lt;b&gt;flood&lt;...</td>\n",
       "      <td>2019-11-17T16:38:00.0000000Z</td>\n",
       "      <td>https://www.express.co.uk/news/weather/1205629...</td>\n",
       "      <td>www.express.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279028</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK flood warning map: Flood chaos to continue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>UK weather forecast: &lt;b&gt;Flood&lt;/b&gt; chaos contin...</td>\n",
       "      <td>Despite some areas enduring their &amp;#39;wettest...</td>\n",
       "      <td>2019-11-17T18:32:00.0000000Z</td>\n",
       "      <td>https://www.mirror.co.uk/news/uk-news/uk-weath...</td>\n",
       "      <td>www.mirror.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279047</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast: Flood chaos continues wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  West Midlands <b>flood</b> warnings prompt &#3...   \n",
       "1         1  New <b>flood</b> warnings issued with more hom...   \n",
       "2         2  UK weather forecast – More than 100 <b>flood</...   \n",
       "3         5  UK <b>flood</b> warning map: <b>Flood</b> chao...   \n",
       "4         6  UK weather forecast: <b>Flood</b> chaos contin...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Residents have been warned to &quot;remain vig...   \n",
       "1     The Environment Agency has a number of <b>floo...   \n",
       "2     <b>FLOOD</b>-ravaged villages in the UK have b...   \n",
       "3     The Environment Agency has issued 57 <b>flood<...   \n",
       "4     Despite some areas enduring their &#39;wettest...   \n",
       "\n",
       "                              date  \\\n",
       "node                                 \n",
       "0     2019-11-17T17:35:00.0000000Z   \n",
       "1     2019-11-17T18:35:00.0000000Z   \n",
       "2     2019-11-17T13:45:00.0000000Z   \n",
       "3     2019-11-17T16:38:00.0000000Z   \n",
       "4     2019-11-17T18:32:00.0000000Z   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0        https://www.bbc.co.uk/news/uk-england-50451817   \n",
       "1     https://www.hulldailymail.co.uk/news/hull-east...   \n",
       "2     https://www.thesun.co.uk/news/10342583/uk-weat...   \n",
       "3     https://www.express.co.uk/news/weather/1205629...   \n",
       "4     https://www.mirror.co.uk/news/uk-news/uk-weath...   \n",
       "\n",
       "                   source_url         retrieval_timestamp         origin  \\\n",
       "node                                                                       \n",
       "0               www.bbc.co.uk  2019-11-17 19:50:58.278878  bing_news_api   \n",
       "1     www.hulldailymail.co.uk  2019-11-17 19:50:58.278928  bing_news_api   \n",
       "2            www.thesun.co.uk  2019-11-17 19:50:58.278953  bing_news_api   \n",
       "3           www.express.co.uk  2019-11-17 19:50:58.279028  bing_news_api   \n",
       "4            www.mirror.co.uk  2019-11-17 19:50:58.279047  bing_news_api   \n",
       "\n",
       "                                             clean_text  \n",
       "node                                                     \n",
       "0     West Midlands flood warnings prompt ;remain vi...  \n",
       "1     New flood warnings issued with more homes at r...  \n",
       "2     UK weather forecast – More than 100 flood aler...  \n",
       "3     UK flood warning map: Flood chaos to continue ...  \n",
       "4     UK weather forecast: Flood chaos continues wit...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"disaster\"\n",
    "\n",
    "# Load up\n",
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Additional preprocessing for LDA\n",
    "\n",
    "### TODO:  Dump the stuff below into another \"embedding model\" in lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12587</td>\n",
       "      <td>125772</td>\n",
       "      <td>Government announces funds for homeless during...</td>\n",
       "      <td>In addition, Ministers are also taking action ...</td>\n",
       "      <td>2020-03-22T06:26:00.0000000Z</td>\n",
       "      <td>https://www.southwalesargus.co.uk/news/1832241...</td>\n",
       "      <td>www.southwalesargus.co.uk</td>\n",
       "      <td>2020-03-22 10:33:56.152099</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Government announces funds for homeless during...</td>\n",
       "      <td>[govern, announc, fund, homeless, coronaviru, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12588</td>\n",
       "      <td>125774</td>\n",
       "      <td>Coronavirus in Wales: Live updates on the &lt;b&gt;o...</td>\n",
       "      <td>You need one to watch live TV on any channel o...</td>\n",
       "      <td>2020-03-22T07:15:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/live/uk-wales-51993900</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2020-03-22 10:33:56.152099</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Coronavirus in Wales: Live updates on the outb...</td>\n",
       "      <td>[coronaviru, wale, live, updat, outbreak, need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12589</td>\n",
       "      <td>125779</td>\n",
       "      <td>CORONAVIRUS: The latest on the &lt;b&gt;outbreak&lt;/b&gt;...</td>\n",
       "      <td>Follow updates on the Coronavirus &lt;b&gt;outbreak&lt;...</td>\n",
       "      <td>2020-03-22T08:16:00.0000000Z</td>\n",
       "      <td>https://www.yorkpress.co.uk/news/18325082.coro...</td>\n",
       "      <td>www.yorkpress.co.uk</td>\n",
       "      <td>2020-03-22 10:33:56.152099</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>CORONAVIRUS: The latest on the outbreak from a...</td>\n",
       "      <td>[coronaviru, latest, outbreak, york, north, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12590</td>\n",
       "      <td>125781</td>\n",
       "      <td>Calderdale barber is cut above the rest during...</td>\n",
       "      <td>As well as the recent &lt;b&gt;outbreak&lt;/b&gt;, Mr Faul...</td>\n",
       "      <td>2020-03-22T07:36:00.0000000Z</td>\n",
       "      <td>https://www.halifaxcourier.co.uk/business/cald...</td>\n",
       "      <td>www.halifaxcourier.co.uk</td>\n",
       "      <td>2020-03-22 10:33:56.152099</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Calderdale barber is cut above the rest during...</td>\n",
       "      <td>[calderdal, barber, cut, rest, covid, outbreak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12591</td>\n",
       "      <td>125784</td>\n",
       "      <td>Sport on TV: Yes there is stuff to watch on BT...</td>\n",
       "      <td>Sorry, there seem to be some issues. Please tr...</td>\n",
       "      <td>2020-03-22T06:14:00.0000000Z</td>\n",
       "      <td>https://inews.co.uk/sport/live-sport-tv-this-w...</td>\n",
       "      <td>inews.co.uk</td>\n",
       "      <td>2020-03-22 10:33:56.152099</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Sport on TV: Yes there is stuff to watch on BT...</td>\n",
       "      <td>[sport, tv, ye, stuff, watch, bt, sport, sky, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                              title  \\\n",
       "node                                                               \n",
       "12587  125772  Government announces funds for homeless during...   \n",
       "12588  125774  Coronavirus in Wales: Live updates on the <b>o...   \n",
       "12589  125779  CORONAVIRUS: The latest on the <b>outbreak</b>...   \n",
       "12590  125781  Calderdale barber is cut above the rest during...   \n",
       "12591  125784  Sport on TV: Yes there is stuff to watch on BT...   \n",
       "\n",
       "                                                 summary  \\\n",
       "node                                                       \n",
       "12587  In addition, Ministers are also taking action ...   \n",
       "12588  You need one to watch live TV on any channel o...   \n",
       "12589  Follow updates on the Coronavirus <b>outbreak<...   \n",
       "12590  As well as the recent <b>outbreak</b>, Mr Faul...   \n",
       "12591  Sorry, there seem to be some issues. Please tr...   \n",
       "\n",
       "                               date  \\\n",
       "node                                  \n",
       "12587  2020-03-22T06:26:00.0000000Z   \n",
       "12588  2020-03-22T07:15:00.0000000Z   \n",
       "12589  2020-03-22T08:16:00.0000000Z   \n",
       "12590  2020-03-22T07:36:00.0000000Z   \n",
       "12591  2020-03-22T06:14:00.0000000Z   \n",
       "\n",
       "                                                    link  \\\n",
       "node                                                       \n",
       "12587  https://www.southwalesargus.co.uk/news/1832241...   \n",
       "12588  https://www.bbc.co.uk/news/live/uk-wales-51993900   \n",
       "12589  https://www.yorkpress.co.uk/news/18325082.coro...   \n",
       "12590  https://www.halifaxcourier.co.uk/business/cald...   \n",
       "12591  https://inews.co.uk/sport/live-sport-tv-this-w...   \n",
       "\n",
       "                      source_url         retrieval_timestamp         origin  \\\n",
       "node                                                                          \n",
       "12587  www.southwalesargus.co.uk  2020-03-22 10:33:56.152099  bing_news_api   \n",
       "12588              www.bbc.co.uk  2020-03-22 10:33:56.152099  bing_news_api   \n",
       "12589        www.yorkpress.co.uk  2020-03-22 10:33:56.152099  bing_news_api   \n",
       "12590   www.halifaxcourier.co.uk  2020-03-22 10:33:56.152099  bing_news_api   \n",
       "12591                inews.co.uk  2020-03-22 10:33:56.152099  bing_news_api   \n",
       "\n",
       "                                              clean_text  \\\n",
       "node                                                       \n",
       "12587  Government announces funds for homeless during...   \n",
       "12588  Coronavirus in Wales: Live updates on the outb...   \n",
       "12589  CORONAVIRUS: The latest on the outbreak from a...   \n",
       "12590  Calderdale barber is cut above the rest during...   \n",
       "12591  Sport on TV: Yes there is stuff to watch on BT...   \n",
       "\n",
       "                                                  tokens  \n",
       "node                                                      \n",
       "12587  [govern, announc, fund, homeless, coronaviru, ...  \n",
       "12588  [coronaviru, wale, live, updat, outbreak, need...  \n",
       "12589  [coronaviru, latest, outbreak, york, north, ea...  \n",
       "12590  [calderdal, barber, cut, rest, covid, outbreak...  \n",
       "12591  [sport, tv, ye, stuff, watch, bt, sport, sky, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick utility function to pre-process the text\n",
    "def preprocess_desc(description):\n",
    "    return( [stemmer.stem(token) for token in simple_preprocess(str(description)) if token not in STOPWORDS] )\n",
    "\n",
    "corpus[\"tokens\"] = corpus[\"clean_text\"].apply(preprocess_desc)\n",
    "\n",
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node\n",
       "0    [(0, 0.10732103413067742), (1, 0.1099548957934...\n",
       "1    [(1, 0.2223928282539802), (2, 0.11023891082478...\n",
       "2    [(2, 0.21081590555015936), (8, 0.1121556551974...\n",
       "3    [(1, 0.12680314842705304), (2, 0.1257112568017...\n",
       "4    [(8, 0.09090098726639546), (9, 0.2319776122541...\n",
       "Name: corpus_tfidf, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vocabulary record\n",
    "dictionary = gensim.corpora.Dictionary(corpus['tokens'])\n",
    "\n",
    "# Remove extreme values (words that are too rare, too common)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a BOW model\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus['tokens']]\n",
    "\n",
    "# From that create the TF-IDF model\n",
    "# THIS IS ANOTHER POINT THE CORPUS ORDERING COULD DETATCH FROM THE RAW DATA ORDERING\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus['corpus_tfidf'] = tfidf[bow_corpus]\n",
    "\n",
    "corpus['corpus_tfidf'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing a range of different-sized LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(7)\n",
    "trainset, testset = train_test_split(corpus, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through a number of different topic model sizes\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for num_topics in range(3, 101):\n",
    "\n",
    "    # Fit the lda model, with [num_topics] topics\n",
    "    lda_model_tfidf = LdaModel(trainset['corpus_tfidf'],\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=dictionary,\n",
    "                               passes=2)\n",
    "    \n",
    "    # Get the perplexity\n",
    "    perplexity = lda_model_tfidf.log_perplexity(testset['corpus_tfidf'])\n",
    "    \n",
    "    # Get the coherence\n",
    "    cm = CoherenceModel(model=lda_model_tfidf, corpus=testset['corpus_tfidf'], coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    \n",
    "    # record\n",
    "    results = results.append({\"topics\":num_topics, \"perplexity\":perplexity, \"coherence\":coherence}, ignore_index=True)\n",
    "    \n",
    "    # Report for my convenience\n",
    "    print(\"tried {} topics\".format(num_topics), \"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "results.to_csv(\"working/disaster_lda_stats_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - PERPLEXITY\n",
    "sns.scatterplot(x=\"topics\", y=\"perplexity\", color=\"blue\", data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results - COHERENCE\n",
    "sns.scatterplot(x=\"topics\", y=\"coherence\", color=\"red\", data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the \"best\" model using all data\n",
    "\n",
    "And parameters decided by test-time performance on perplexity and coherence.\n",
    "\n",
    "For RSS; 62 topics looks good.  Getting suspicious though, got similar answer (~70) for wine descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity = -47.74249598698768 coherence = -7.71839795996229\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# Fit the final lda model to all data\n",
    "lda_model_tfidf = LdaModel(corpus['corpus_tfidf'],\n",
    "                           num_topics=62,\n",
    "                           id2word=dictionary,\n",
    "                           passes=2)\n",
    "\n",
    "# Get the perplexity, out of curiosity\n",
    "perplexity = lda_model_tfidf.log_perplexity(corpus['corpus_tfidf'])\n",
    "    \n",
    "# Get the coherence, out of curiosity\n",
    "cm = CoherenceModel(model=lda_model_tfidf, corpus=corpus['corpus_tfidf'], coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "\n",
    "print(\"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "lda_model_tfidf.save(\"working/lda_model_tfidf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.095*\"girl\" + 0.084*\"start\" + 0.070*\"thursday\" + 0.048*\"give\" + 0.043*\"member\" + 0.040*\"wait\" + 0.034*\"year\" + 0.034*\"old\" + 0.025*\"central\" + 0.023*\"amid\"\n",
      "Topic: 1 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 2 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 3 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 4 Word: 0.165*\"citi\" + 0.106*\"pm\" + 0.098*\"histori\" + 0.076*\"far\" + 0.065*\"fall\" + 0.060*\"see\" + 0.045*\"smoke\" + 0.043*\"fire\" + 0.039*\"breach\" + 0.035*\"right\"\n",
      "Topic: 5 Word: 0.159*\"know\" + 0.134*\"local\" + 0.093*\"head\" + 0.088*\"deadliest\" + 0.077*\"storm\" + 0.063*\"villag\" + 0.057*\"western\" + 0.041*\"wind\" + 0.027*\"eastern\" + 0.016*\"occur\"\n",
      "Topic: 6 Word: 0.087*\"ban\" + 0.067*\"store\" + 0.055*\"outsid\" + 0.050*\"assault\" + 0.048*\"continu\" + 0.046*\"incid\" + 0.037*\"sell\" + 0.032*\"onlin\" + 0.032*\"boss\" + 0.029*\"clear\"\n",
      "Topic: 7 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 8 Word: 0.160*\"pictur\" + 0.155*\"night\" + 0.061*\"photo\" + 0.060*\"beauti\" + 0.050*\"greatest\" + 0.046*\"sky\" + 0.043*\"captur\" + 0.030*\"incred\" + 0.027*\"wild\" + 0.027*\"amaz\"\n",
      "Topic: 9 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 10 Word: 0.172*\"offic\" + 0.122*\"worker\" + 0.046*\"weather\" + 0.037*\"brother\" + 0.034*\"tweet\" + 0.033*\"complet\" + 0.032*\"fake\" + 0.026*\"map\" + 0.025*\"mock\" + 0.024*\"back\"\n",
      "Topic: 11 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 12 Word: 0.135*\"cancel\" + 0.088*\"coupl\" + 0.077*\"owner\" + 0.065*\"admit\" + 0.064*\"venu\" + 0.056*\"video\" + 0.045*\"shock\" + 0.039*\"post\" + 0.032*\"wed\" + 0.026*\"show\"\n",
      "Topic: 13 Word: 0.129*\"hous\" + 0.077*\"go\" + 0.068*\"person\" + 0.061*\"respons\" + 0.057*\"cut\" + 0.046*\"white\" + 0.041*\"plane\" + 0.039*\"control\" + 0.039*\"decid\" + 0.031*\"tax\"\n",
      "Topic: 14 Word: 0.115*\"hospit\" + 0.081*\"emerg\" + 0.049*\"gun\" + 0.036*\"peopl\" + 0.035*\"have\" + 0.034*\"mass\" + 0.032*\"raid\" + 0.032*\"bear\" + 0.031*\"saturday\" + 0.030*\"defend\"\n",
      "Topic: 15 Word: 0.115*\"parent\" + 0.100*\"stop\" + 0.073*\"ask\" + 0.068*\"get\" + 0.065*\"author\" + 0.062*\"protect\" + 0.058*\"forc\" + 0.045*\"box\" + 0.042*\"accord\" + 0.041*\"pupil\"\n",
      "Topic: 16 Word: 0.112*\"need\" + 0.097*\"number\" + 0.083*\"plan\" + 0.082*\"concern\" + 0.073*\"case\" + 0.068*\"public\" + 0.029*\"peopl\" + 0.027*\"mp\" + 0.025*\"new\" + 0.024*\"danger\"\n",
      "Topic: 17 Word: 0.137*\"travel\" + 0.109*\"children\" + 0.096*\"park\" + 0.071*\"spain\" + 0.057*\"water\" + 0.054*\"british\" + 0.053*\"holiday\" + 0.051*\"intern\" + 0.050*\"brit\" + 0.042*\"apart\"\n",
      "Topic: 18 Word: 0.099*\"follow\" + 0.091*\"custom\" + 0.090*\"set\" + 0.084*\"road\" + 0.082*\"car\" + 0.079*\"crash\" + 0.061*\"major\" + 0.051*\"hear\" + 0.037*\"describ\" + 0.032*\"wit\"\n",
      "Topic: 19 Word: 0.094*\"warn\" + 0.083*\"quot\" + 0.055*\"counti\" + 0.039*\"battl\" + 0.037*\"compani\" + 0.034*\"inform\" + 0.033*\"black\" + 0.033*\"europ\" + 0.032*\"featur\" + 0.032*\"film\"\n",
      "Topic: 20 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 21 Word: 0.126*\"kent\" + 0.097*\"world\" + 0.091*\"bodi\" + 0.079*\"book\" + 0.068*\"record\" + 0.045*\"shed\" + 0.042*\"perform\" + 0.036*\"garden\" + 0.036*\"man\" + 0.030*\"multipl\"\n",
      "Topic: 22 Word: 0.202*\"school\" + 0.113*\"student\" + 0.067*\"monday\" + 0.067*\"teacher\" + 0.066*\"coast\" + 0.060*\"celebr\" + 0.052*\"high\" + 0.046*\"california\" + 0.044*\"host\" + 0.043*\"birthday\"\n",
      "Topic: 23 Word: 0.078*\"china\" + 0.048*\"order\" + 0.046*\"stock\" + 0.045*\"averag\" + 0.043*\"market\" + 0.039*\"iran\" + 0.038*\"war\" + 0.037*\"industri\" + 0.034*\"delay\" + 0.029*\"decis\"\n",
      "Topic: 24 Word: 0.149*\"help\" + 0.086*\"social\" + 0.078*\"buy\" + 0.077*\"shop\" + 0.062*\"chief\" + 0.061*\"nh\" + 0.054*\"street\" + 0.044*\"peopl\" + 0.043*\"teenag\" + 0.043*\"media\"\n",
      "Topic: 25 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 26 Word: 0.110*\"close\" + 0.070*\"town\" + 0.036*\"month\" + 0.033*\"lost\" + 0.033*\"save\" + 0.032*\"care\" + 0.030*\"son\" + 0.023*\"emot\" + 0.022*\"carri\" + 0.021*\"turn\"\n",
      "Topic: 27 Word: 0.096*\"australia\" + 0.080*\"hit\" + 0.058*\"flight\" + 0.056*\"arrest\" + 0.052*\"tuesday\" + 0.048*\"run\" + 0.046*\"driver\" + 0.044*\"airport\" + 0.037*\"morn\" + 0.029*\"attempt\"\n",
      "Topic: 28 Word: 0.061*\"stay\" + 0.046*\"place\" + 0.045*\"univers\" + 0.043*\"group\" + 0.036*\"drug\" + 0.033*\"tell\" + 0.028*\"instead\" + 0.026*\"product\" + 0.022*\"suppli\" + 0.022*\"church\"\n",
      "Topic: 29 Word: 0.152*\"test\" + 0.103*\"posit\" + 0.069*\"minist\" + 0.067*\"australian\" + 0.055*\"prime\" + 0.047*\"shut\" + 0.023*\"morrison\" + 0.022*\"scott\" + 0.020*\"die\" + 0.018*\"mr\"\n",
      "Topic: 30 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 31 Word: 0.179*\"toilet\" + 0.160*\"dog\" + 0.131*\"share\" + 0.129*\"stand\" + 0.045*\"catch\" + 0.042*\"pet\" + 0.027*\"bizarr\" + 0.022*\"lover\" + 0.021*\"joke\" + 0.017*\"refer\"\n",
      "Topic: 32 Word: 0.123*\"passeng\" + 0.121*\"drive\" + 0.107*\"trial\" + 0.082*\"late\" + 0.070*\"tycoon\" + 0.064*\"properti\" + 0.062*\"convict\" + 0.056*\"martin\" + 0.054*\"provid\" + 0.026*\"brain\"\n",
      "Topic: 33 Word: 0.142*\"amid\" + 0.139*\"urg\" + 0.111*\"bid\" + 0.088*\"council\" + 0.066*\"talk\" + 0.055*\"leader\" + 0.053*\"refus\" + 0.043*\"violent\" + 0.024*\"govern\" + 0.023*\"agre\"\n",
      "Topic: 34 Word: 0.141*\"hour\" + 0.108*\"mum\" + 0.095*\"babi\" + 0.086*\"daughter\" + 0.075*\"took\" + 0.048*\"heard\" + 0.040*\"text\" + 0.030*\"pose\" + 0.029*\"time\" + 0.027*\"happi\"\n",
      "Topic: 35 Word: 0.138*\"health\" + 0.110*\"staff\" + 0.067*\"servic\" + 0.054*\"live\" + 0.041*\"worst\" + 0.040*\"human\" + 0.038*\"caus\" + 0.033*\"home\" + 0.029*\"deadli\" + 0.028*\"mental\"\n",
      "Topic: 36 Word: 0.080*\"visit\" + 0.058*\"york\" + 0.051*\"week\" + 0.044*\"new\" + 0.043*\"good\" + 0.039*\"harri\" + 0.036*\"today\" + 0.035*\"yesterday\" + 0.034*\"touch\" + 0.034*\"bar\"\n",
      "Topic: 37 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 38 Word: 0.153*\"confirm\" + 0.049*\"mask\" + 0.049*\"husband\" + 0.049*\"dead\" + 0.049*\"wife\" + 0.048*\"die\" + 0.038*\"shot\" + 0.038*\"condit\" + 0.032*\"age\" + 0.030*\"driveway\"\n",
      "Topic: 39 Word: 0.263*\"uk\" + 0.109*\"row\" + 0.081*\"remain\" + 0.073*\"parti\" + 0.063*\"support\" + 0.051*\"present\" + 0.029*\"discuss\" + 0.028*\"activist\" + 0.028*\"broke\" + 0.023*\"forward\"\n",
      "Topic: 40 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 41 Word: 0.071*\"use\" + 0.059*\"make\" + 0.059*\"david\" + 0.051*\"slam\" + 0.044*\"sunday\" + 0.041*\"thiev\" + 0.037*\"sign\" + 0.034*\"florida\" + 0.034*\"nightmar\" + 0.027*\"steal\"\n",
      "Topic: 42 Word: 0.110*\"trump\" + 0.075*\"presid\" + 0.059*\"donald\" + 0.054*\"sentenc\" + 0.051*\"prison\" + 0.050*\"commun\" + 0.050*\"democrat\" + 0.049*\"apo\" + 0.045*\"releas\" + 0.035*\"secur\"\n",
      "Topic: 43 Word: 0.107*\"total\" + 0.099*\"toll\" + 0.087*\"offici\" + 0.079*\"rise\" + 0.075*\"miss\" + 0.069*\"death\" + 0.068*\"bring\" + 0.060*\"near\" + 0.052*\"thousand\" + 0.032*\"island\"\n",
      "Topic: 44 Word: 0.121*\"busi\" + 0.080*\"london\" + 0.055*\"wall\" + 0.047*\"million\" + 0.046*\"event\" + 0.044*\"money\" + 0.043*\"firm\" + 0.040*\"larg\" + 0.038*\"trip\" + 0.033*\"omar\"\n",
      "Topic: 45 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 46 Word: 0.121*\"friend\" + 0.111*\"air\" + 0.107*\"later\" + 0.081*\"went\" + 0.067*\"second\" + 0.064*\"jump\" + 0.041*\"wrong\" + 0.036*\"plant\" + 0.026*\"ramp\" + 0.025*\"valu\"\n",
      "Topic: 47 Word: 0.099*\"return\" + 0.067*\"scene\" + 0.059*\"devast\" + 0.051*\"damag\" + 0.049*\"peopl\" + 0.044*\"spark\" + 0.043*\"inspir\" + 0.037*\"scale\" + 0.037*\"sever\" + 0.028*\"wrongli\"\n",
      "Topic: 48 Word: 0.119*\"investig\" + 0.103*\"launch\" + 0.098*\"figur\" + 0.085*\"demand\" + 0.079*\"rais\" + 0.078*\"question\" + 0.057*\"polit\" + 0.052*\"financi\" + 0.047*\"billionair\" + 0.030*\"answer\"\n",
      "Topic: 49 Word: 0.036*\"boy\" + 0.035*\"shopper\" + 0.034*\"life\" + 0.031*\"cent\" + 0.031*\"attack\" + 0.030*\"suffer\" + 0.029*\"secretari\" + 0.028*\"year\" + 0.028*\"power\" + 0.028*\"sick\"\n",
      "Topic: 50 Word: 0.110*\"call\" + 0.095*\"fear\" + 0.092*\"south\" + 0.063*\"arm\" + 0.059*\"killer\" + 0.055*\"flood\" + 0.039*\"end\" + 0.039*\"american\" + 0.035*\"move\" + 0.034*\"surg\"\n",
      "Topic: 51 Word: 0.094*\"distanc\" + 0.088*\"charg\" + 0.061*\"current\" + 0.058*\"sex\" + 0.055*\"child\" + 0.049*\"line\" + 0.048*\"result\" + 0.045*\"rule\" + 0.042*\"epstein\" + 0.035*\"andrew\"\n",
      "Topic: 52 Word: 0.183*\"govern\" + 0.112*\"food\" + 0.082*\"offer\" + 0.065*\"deal\" + 0.049*\"eu\" + 0.044*\"brexit\" + 0.043*\"land\" + 0.039*\"trade\" + 0.038*\"possibl\" + 0.035*\"promis\"\n",
      "Topic: 53 Word: 0.251*\"man\" + 0.075*\"sent\" + 0.067*\"leav\" + 0.065*\"drop\" + 0.059*\"moment\" + 0.044*\"show\" + 0.042*\"footag\" + 0.038*\"identifi\" + 0.029*\"face\" + 0.028*\"fli\"\n",
      "Topic: 54 Word: 0.134*\"women\" + 0.108*\"appear\" + 0.093*\"rate\" + 0.063*\"threaten\" + 0.058*\"stori\" + 0.057*\"threat\" + 0.051*\"journalist\" + 0.041*\"administr\" + 0.033*\"low\" + 0.029*\"report\"\n",
      "Topic: 55 Word: 0.137*\"doctor\" + 0.132*\"home\" + 0.104*\"look\" + 0.091*\"centr\" + 0.086*\"medic\" + 0.069*\"develop\" + 0.067*\"like\" + 0.042*\"fail\" + 0.038*\"check\" + 0.033*\"blame\"\n",
      "Topic: 56 Word: 0.139*\"outbreak\" + 0.071*\"state\" + 0.071*\"latest\" + 0.058*\"countri\" + 0.058*\"seven\" + 0.048*\"caught\" + 0.044*\"pass\" + 0.042*\"hand\" + 0.040*\"tv\" + 0.036*\"law\"\n",
      "Topic: 57 Word: 0.097*\"risk\" + 0.066*\"build\" + 0.063*\"border\" + 0.045*\"rescu\" + 0.040*\"peopl\" + 0.037*\"drink\" + 0.035*\"trap\" + 0.034*\"oper\" + 0.033*\"collaps\" + 0.031*\"region\"\n",
      "Topic: 58 Word: 0.086*\"victim\" + 0.070*\"franc\" + 0.058*\"pack\" + 0.054*\"watch\" + 0.053*\"search\" + 0.048*\"blaze\" + 0.037*\"northern\" + 0.036*\"desper\" + 0.034*\"renov\" + 0.031*\"giant\"\n",
      "Topic: 59 Word: 0.000*\"klux\" + 0.000*\"singalong\" + 0.000*\"herapo\" + 0.000*\"foetal\" + 0.000*\"officerapo\" + 0.000*\"ku\" + 0.000*\"aramco\" + 0.000*\"gilbert\" + 0.000*\"pineappl\" + 0.000*\"altar\"\n",
      "Topic: 60 Word: 0.147*\"work\" + 0.138*\"away\" + 0.123*\"seen\" + 0.066*\"includ\" + 0.056*\"crime\" + 0.052*\"cover\" + 0.040*\"mile\" + 0.038*\"light\" + 0.034*\"music\" + 0.029*\"small\"\n",
      "Topic: 61 Word: 0.154*\"case\" + 0.083*\"time\" + 0.051*\"campaign\" + 0.051*\"murder\" + 0.047*\"abus\" + 0.042*\"court\" + 0.040*\"rape\" + 0.038*\"sexual\" + 0.033*\"hundr\" + 0.033*\"given\"\n"
     ]
    }
   ],
   "source": [
    "# Let's take a quick look at the topics picked out\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>girl start thursday give member</td>\n",
       "      <td>X0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>klux singalong herapo foetal officerapo</td>\n",
       "      <td>X1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>klux singalong herapo foetal officerapo</td>\n",
       "      <td>X2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>klux singalong herapo foetal officerapo</td>\n",
       "      <td>X3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>citi pm histori far fall</td>\n",
       "      <td>X4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>risk build border rescu peopl</td>\n",
       "      <td>X57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>victim franc pack watch search</td>\n",
       "      <td>X58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>klux singalong herapo foetal officerapo</td>\n",
       "      <td>X59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>work away seen includ crime</td>\n",
       "      <td>X60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>case time campaign murder abus</td>\n",
       "      <td>X61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   keywords topic\n",
       "0           girl start thursday give member    X0\n",
       "1   klux singalong herapo foetal officerapo    X1\n",
       "2   klux singalong herapo foetal officerapo    X2\n",
       "3   klux singalong herapo foetal officerapo    X3\n",
       "4                  citi pm histori far fall    X4\n",
       "..                                      ...   ...\n",
       "57            risk build border rescu peopl   X57\n",
       "58           victim franc pack watch search   X58\n",
       "59  klux singalong herapo foetal officerapo   X59\n",
       "60              work away seen includ crime   X60\n",
       "61           case time campaign murder abus   X61\n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a quick look at words the topics picked out\n",
    "topic_word_dist = pd.DataFrame()\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    # Record the topic index in a format R will like\n",
    "    topic_index = \"X\" + str(idx)\n",
    "    \n",
    "    # Record the topic's key words as a single string\n",
    "    # Split the words with a newline character while we're at it!\n",
    "    topic_words = \" \".join([x.split(\"*\")[1].replace('\"', '').strip() for x in topic.split(\"+\")][0:5])\n",
    "    \n",
    "    # Dump them to the results dataframe\n",
    "    topic_word_dist = topic_word_dist.append({\"topic\":topic_index, \"keywords\":topic_words}, ignore_index=True)\n",
    "    \n",
    "topic_word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n"
     ]
    }
   ],
   "source": [
    "# Assign topic to document, also store the probability of that topic\n",
    "doc_topics = []\n",
    "\n",
    "count = 0\n",
    "for index, row in corpus.iterrows():\n",
    "    \n",
    "    # Extract list of tuples of (topic, score) from the model for each doc\n",
    "    topics = [x for x in lda_model_tfidf.get_document_topics(row['corpus_tfidf']) ]\n",
    "    \n",
    "    # Find the highest probability topic\n",
    "    highest_scoring = sorted(topics, key=lambda x: x[1], reverse=True)[0]\n",
    "    \n",
    "    temp = {\"node\": int(index),\n",
    "            \"cluster\": highest_scoring[0],\n",
    "            \"score\": highest_scoring[1]}\n",
    "    \n",
    "    # Report processing progress!\n",
    "    count = count + 1\n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    doc_topics.append(temp)\n",
    "\n",
    "doc_topic_dist = pd.DataFrame(doc_topics).set_index(\"node\")\n",
    "\n",
    "doc_topic_dist.to_csv(\"working/doc_topic_dist_backup.csv\")\n",
    "\n",
    "doc_topic_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.join(doc_topic_dist).to_csv(\"working/disaster_clustered_lda.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
