{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN should work well on embedding representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 325\n",
      "9.8 percent of files read.\n",
      "19.7 percent of files read.\n",
      "29.5 percent of files read.\n",
      "39.4 percent of files read.\n",
      "49.2 percent of files read.\n",
      "59.1 percent of files read.\n",
      "68.9 percent of files read.\n",
      "78.8 percent of files read.\n",
      "88.6 percent of files read.\n",
      "98.5 percent of files read.\n",
      "(79873, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts cost</td>\n",
       "      <td>Life-threatening US storm surges are feared, a...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:03:44 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-us-canada-495...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Hurricane Dorian lashes US as Bahamas counts c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "      <td>They are relatives of a group of Pakistani wom...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:53:17 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-49592540</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Kohistan video murders: Three guilty in 'honou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' released</td>\n",
       "      <td>A Ukrainian court releases a potentially key w...</td>\n",
       "      <td>Thu, 05 Sep 2019 13:46:06 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49591148</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>MH17 Ukraine plane crash: 'Key witness' releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Article 370: The weddings 'ruined' by Kashmir'...</td>\n",
       "      <td>Indian-administered Kashmir is under a securit...</td>\n",
       "      <td>Thu, 05 Sep 2019 07:32:34 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-49...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Article 70: The weddings 'ruined' by Kashmir's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "      <td>President Erdogan demands international help t...</td>\n",
       "      <td>Thu, 05 Sep 2019 16:11:48 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-49599297</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-09-05 21:35:06.925873</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Syria war: Turkey warns Europe of new migrant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  Hurricane Dorian lashes US as Bahamas counts cost   \n",
       "1         1  Kohistan video murders: Three guilty in 'honou...   \n",
       "2         2   MH17 Ukraine plane crash: 'Key witness' released   \n",
       "3         3  Article 370: The weddings 'ruined' by Kashmir'...   \n",
       "4         4  Syria war: Turkey warns Europe of new migrant ...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Life-threatening US storm surges are feared, a...   \n",
       "1     They are relatives of a group of Pakistani wom...   \n",
       "2     A Ukrainian court releases a potentially key w...   \n",
       "3     Indian-administered Kashmir is under a securit...   \n",
       "4     President Erdogan demands international help t...   \n",
       "\n",
       "                               date  \\\n",
       "node                                  \n",
       "0     Thu, 05 Sep 2019 16:03:44 GMT   \n",
       "1     Thu, 05 Sep 2019 13:53:17 GMT   \n",
       "2     Thu, 05 Sep 2019 13:46:06 GMT   \n",
       "3     Thu, 05 Sep 2019 07:32:34 GMT   \n",
       "4     Thu, 05 Sep 2019 16:11:48 GMT   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0     https://www.bbc.co.uk/news/world-us-canada-495...   \n",
       "1        https://www.bbc.co.uk/news/world-asia-49592540   \n",
       "2      https://www.bbc.co.uk/news/world-europe-49591148   \n",
       "3     https://www.bbc.co.uk/news/world-asia-india-49...   \n",
       "4      https://www.bbc.co.uk/news/world-europe-49599297   \n",
       "\n",
       "                                      source_url         retrieval_timestamp  \\\n",
       "node                                                                           \n",
       "0     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "1     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "2     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "3     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "4     http://feeds.bbci.co.uk/news/world/rss.xml  2019-09-05 21:35:06.925873   \n",
       "\n",
       "        origin                                         clean_text  \n",
       "node                                                               \n",
       "0     rss_feed  Hurricane Dorian lashes US as Bahamas counts c...  \n",
       "1     rss_feed  Kohistan video murders: Three guilty in 'honou...  \n",
       "2     rss_feed  MH17 Ukraine plane crash: 'Key witness' releas...  \n",
       "3     rss_feed  Article 70: The weddings 'ruined' by Kashmir's...  \n",
       "4     rss_feed  Syria war: Turkey warns Europe of new migrant ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\"\n",
    "\n",
    "# Load up\n",
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Build Text Model (Representation, eg; word2vec, entities list...)\n",
    "\n",
    "- Trying with the world corpus and with the bing corpus, neither worked with InferSent.  Suspect the problem lies in the PCA step, which may not be working well on this high-dimensional (vector length = 4096) form.\n",
    "- Summed keywords works rather better with the world corpus.\n",
    "- Summed keywords still fail the bing/home office corpus, giving me a cluster about \"immigration\" and a cluster for the American Supreme Court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows didn't play nicely with the vector datasets, Some obscure encoding problem (python in Conda\n",
    "# kept trying to decode using cp1252 regardless of whatever other options I specified!)\n",
    "# Solution; rewrite file and drop any characters the Windows encoder refuses to recognise.\n",
    "# I shouldn't loose too much info.\n",
    "with open('./lib/InferSent/dataset/fastText/crawl-300d-2M.vec', \"r\", encoding=\"cp1252\", errors=\"ignore\") as infile:\n",
    "    with open('./lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec', \"wb\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line.encode('cp1252'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent = reps.InferSentModel(list(corpus['clean_text']),\n",
    "                                list(corpus['clean_text']),\n",
    "                                W2V_PATH = './lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec')\n",
    "\n",
    "embeddings = infersent.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Whereas this worked first time!\n",
    "glove = reps.NounGloveWordModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "embeddings = glove.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn that into a DF for me\n",
    "embeddings_df = pd.DataFrame({\"clean_text\": list(embeddings.keys()),\n",
    "                              \"embeddings\": list(embeddings.values())})\n",
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a.  Try a really simple averaged word vector model!\n",
    "\n",
    "With a complex noun extraction function 'cause that part's slow so I multi-threaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrased_nouns(sentences):\n",
    "    \"\"\" Use spacy to get all of the actual entities, conjoin bigram nouns. \"\"\"\n",
    "\n",
    "    # Get the lists of nouns\n",
    "    noun_lists = []\n",
    "    for doc in sentences:\n",
    "        parsed = nlp(doc)\n",
    "        noun_lists.append([token.lemma_ for token in parsed if token.pos_ == 'PROPN'])\n",
    "\n",
    "    # Build the phrase model\n",
    "    phrases = Phrases(noun_lists, min_count=5, threshold=0.5)\n",
    "\n",
    "    # Get the set of phrases present in the model\n",
    "    results = []\n",
    "    for nouns in noun_lists:\n",
    "        results.append(phrases[nouns])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Get phrase-conjoined, lemmatized tokens\n",
    "test = get_phrased_nouns(corpus['clean_text'][0:10000])\n",
    "\n",
    "# Detect and conjoin bigrams\n",
    "model = Word2Vec(test, size=100, window=5, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_vec(token_list, model):\n",
    "    \n",
    "    vecs = []\n",
    "    for token in token_list:\n",
    "        try:\n",
    "            vector = model.wv[token]\n",
    "        except: \n",
    "            vector = np.zeros(100)\n",
    "        vecs.append(vector)\n",
    "    \n",
    "    if len(vecs) > 0:\n",
    "        return np.mean(np.asarray(vecs), axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [get_averaged_vec(tokens, model) for tokens in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.3463317e-03,  2.5220399e-03, -4.6524671e-03,  3.4887677e-03,\n",
       "         8.2033938e-03, -4.9915398e-03, -8.0874393e-04,  1.8334268e-03,\n",
       "         2.5391961e-03, -2.2249611e-03, -3.1220273e-03,  1.7007247e-04,\n",
       "         2.2594405e-03, -3.5947412e-03,  1.4751510e-03,  1.8720523e-03,\n",
       "        -1.4218759e-03, -1.0979190e-03, -3.1866290e-05,  7.8581972e-04,\n",
       "         2.2808164e-03,  5.5785262e-04, -2.0109962e-03,  4.2855716e-03,\n",
       "        -1.6455781e-03, -6.6904543e-04,  1.2567308e-03,  2.8988707e-03,\n",
       "        -1.5578925e-03,  3.7417940e-03,  1.3559783e-03,  4.6948856e-04,\n",
       "        -1.9674024e-03,  1.6709786e-03,  2.9800681e-03,  4.1622436e-03,\n",
       "         4.9548876e-03,  3.3579834e-03,  3.5455704e-04, -4.6961778e-03,\n",
       "        -2.1982871e-03, -2.5425551e-03,  2.2553939e-03, -3.8203963e-03,\n",
       "        -2.0974192e-03,  6.3258543e-04,  7.0130778e-04, -5.7315041e-04,\n",
       "         6.3947425e-03,  6.9253738e-03,  2.3834405e-03,  1.6529089e-03,\n",
       "         4.8593953e-04,  9.5911942e-05, -3.9122058e-03,  4.0279669e-03,\n",
       "        -3.0390087e-03, -5.2068047e-03, -3.1554690e-03,  2.5992875e-03,\n",
       "         6.7591839e-03,  3.6250837e-03,  5.1911236e-03,  1.4794132e-03,\n",
       "         3.3533697e-03, -3.5065245e-03,  2.1937145e-03,  7.4742008e-03,\n",
       "        -2.2100115e-03,  4.0634009e-03,  1.3759323e-04, -4.0605161e-03,\n",
       "         4.6296623e-03, -2.2091160e-03, -2.6017609e-03, -2.0179967e-03,\n",
       "        -1.7785192e-03,  2.3162486e-03, -1.9435135e-04,  1.3367459e-03,\n",
       "        -1.3443325e-03, -4.5956871e-03, -1.7783825e-03,  3.9012430e-03,\n",
       "         5.6705014e-03, -4.5719193e-04, -1.2154684e-03, -2.7317230e-03,\n",
       "         3.9976235e-03, -4.2011472e-03, -2.6390476e-03, -1.7353697e-03,\n",
       "         3.0992420e-03, -6.3967369e-03,  7.6179155e-03, -2.0379450e-03,\n",
       "        -1.9364875e-03,  1.9563104e-03,  2.1598013e-03, -6.2779803e-03],\n",
       "       dtype=float32),\n",
       " array([-1.8539741e-03, -4.6639703e-03,  1.6879817e-03, -1.3819973e-03,\n",
       "        -4.2147012e-03, -2.7358690e-03,  3.3387670e-03,  2.2160292e-03,\n",
       "        -4.2317635e-03, -4.7988435e-03,  2.6988105e-03,  1.8037674e-03,\n",
       "         1.5244739e-03,  4.7350698e-03,  1.5558936e-03,  1.6376314e-03,\n",
       "         1.4201775e-03, -2.2318251e-03,  1.1889454e-03, -4.4604237e-03,\n",
       "        -4.2943349e-03,  4.6739862e-03, -4.8692375e-03, -1.6266701e-03,\n",
       "        -2.0073922e-03,  2.5225906e-03, -2.7697315e-04,  2.5437237e-04,\n",
       "         4.2712162e-03, -1.6569041e-03, -3.6061320e-03, -1.3154802e-03,\n",
       "         2.0890497e-03, -4.1807815e-03, -1.0733699e-03,  9.0510113e-04,\n",
       "         2.0582620e-03,  4.6415957e-05,  2.1846103e-03, -3.4933591e-03,\n",
       "        -1.4505248e-03,  3.8382681e-03, -2.7355086e-03, -1.9755464e-03,\n",
       "        -1.0486965e-03,  4.2334035e-05,  2.4738682e-03, -4.2510061e-03,\n",
       "        -3.4643689e-03,  4.6495222e-03, -2.0290588e-03, -1.0626264e-03,\n",
       "        -4.3392209e-03, -4.4039972e-03, -1.1408228e-03, -2.3937458e-03,\n",
       "        -1.5929695e-03, -4.4294735e-03, -4.3989406e-03,  3.5445618e-03,\n",
       "         3.7825243e-03,  4.2928900e-03, -4.1255164e-03, -2.2648869e-03,\n",
       "        -3.3862800e-03,  2.2981176e-03, -4.1934070e-03,  3.1450791e-03,\n",
       "        -3.2420938e-03, -4.2807711e-03, -4.5899581e-03,  2.0600441e-03,\n",
       "         1.1031931e-03, -2.1913152e-03, -3.3327562e-03, -4.7709490e-03,\n",
       "         2.4655836e-03, -3.4299288e-03, -3.1243430e-03,  4.9135201e-03,\n",
       "        -3.7691859e-03, -1.0312137e-03,  3.3854202e-03,  3.5883149e-03,\n",
       "         3.3216365e-03, -4.6502003e-03, -1.4105274e-03,  3.0812058e-03,\n",
       "        -3.8844219e-03, -6.7672681e-04,  4.5095710e-03, -3.2383450e-03,\n",
       "         4.5517981e-03, -4.7677135e-04,  2.0541877e-03, -2.3831406e-03,\n",
       "         4.0866043e-03,  5.5078410e-05,  3.5967890e-03,  1.1825969e-03],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster Text\n",
    "\n",
    "This is the part where the pipelines get a little more experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.vstack(vectors)\n",
    "#embeddings_array = np.vstack(embeddings_df['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, PCA the data\n",
    "pca = PCA(n_components=20, svd_solver='full')\n",
    "\n",
    "embeddings_pca = pca.fit_transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37893687 0.02608303 0.02410494 0.01998329 0.01535319 0.01497381\n",
      " 0.01334177 0.01224024 0.01178881 0.01097897 0.01063203 0.01043303\n",
      " 0.01002969 0.00953764 0.00937127 0.00906266 0.0088502  0.0085055\n",
      " 0.00822424 0.00807814]\n",
      "[1.39724507 0.3665795  0.35240507 0.3208652  0.28124736 0.27775079\n",
      " 0.26217776 0.2511216  0.24644738 0.23783183 0.23404385 0.23184325\n",
      " 0.22731753 0.22167133 0.21972947 0.21608122 0.21353336 0.20933361\n",
      " 0.20584339 0.20400684]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1, 128,  19, 116,  71, 100,   4,  76,   3,   2,  18,  39,   6,\n",
       "        65,  78,  48,  32,   5,  50,  77,  38,  57,  51,  94, 104,  22,\n",
       "         9,  69,  17,  75, 118,  20,  31, 101, 109, 119,  98,  97, 107,\n",
       "       103,  99, 110,  93,  49,  45,  15,  72,  74,  52, 106,  87,  44,\n",
       "       129, 111,  59,  84,  29,  43,  53,  34,  40,  16,  36, 115, 117,\n",
       "        61, 130,  26,  60,   8,  23,  25,  73,  41, 102, 112,  42,   7,\n",
       "        62,  55, 108,  47, 105,  67,  89,  10,  14,  27, 126, 125,  96,\n",
       "        79,  88,  12,  13,  63, 124,  64,  30,  95,  33,  66, 114,  21,\n",
       "        70,  83,  28,  92,  85, 127,  81, 113,  68,  46,  56,  35,  90,\n",
       "       122,  37,  24,  80, 121,   1,  54, 123,  11,  82,   0,  91,  58,\n",
       "        86, 120], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1)\n",
    "clusterer.fit(embeddings_array)\n",
    "pd.unique(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 8328,\n",
       "         128: 628,\n",
       "         19: 8,\n",
       "         116: 8,\n",
       "         71: 5,\n",
       "         100: 5,\n",
       "         4: 30,\n",
       "         76: 20,\n",
       "         3: 8,\n",
       "         2: 7,\n",
       "         18: 6,\n",
       "         39: 5,\n",
       "         6: 6,\n",
       "         65: 6,\n",
       "         78: 5,\n",
       "         48: 20,\n",
       "         32: 5,\n",
       "         5: 15,\n",
       "         50: 5,\n",
       "         77: 7,\n",
       "         38: 6,\n",
       "         57: 7,\n",
       "         51: 6,\n",
       "         94: 6,\n",
       "         104: 5,\n",
       "         22: 6,\n",
       "         9: 8,\n",
       "         69: 9,\n",
       "         17: 6,\n",
       "         75: 5,\n",
       "         118: 5,\n",
       "         20: 5,\n",
       "         31: 5,\n",
       "         101: 6,\n",
       "         109: 10,\n",
       "         119: 5,\n",
       "         98: 7,\n",
       "         97: 7,\n",
       "         107: 5,\n",
       "         103: 9,\n",
       "         99: 6,\n",
       "         110: 5,\n",
       "         93: 8,\n",
       "         49: 7,\n",
       "         45: 5,\n",
       "         15: 19,\n",
       "         72: 8,\n",
       "         74: 9,\n",
       "         52: 6,\n",
       "         106: 5,\n",
       "         87: 6,\n",
       "         44: 8,\n",
       "         129: 27,\n",
       "         111: 20,\n",
       "         59: 5,\n",
       "         84: 6,\n",
       "         29: 7,\n",
       "         43: 6,\n",
       "         53: 39,\n",
       "         34: 14,\n",
       "         40: 22,\n",
       "         16: 5,\n",
       "         36: 29,\n",
       "         115: 11,\n",
       "         117: 10,\n",
       "         61: 5,\n",
       "         130: 11,\n",
       "         26: 6,\n",
       "         60: 6,\n",
       "         8: 6,\n",
       "         23: 5,\n",
       "         25: 8,\n",
       "         73: 13,\n",
       "         41: 5,\n",
       "         102: 5,\n",
       "         112: 8,\n",
       "         42: 10,\n",
       "         7: 10,\n",
       "         62: 5,\n",
       "         55: 5,\n",
       "         108: 5,\n",
       "         47: 11,\n",
       "         105: 6,\n",
       "         67: 6,\n",
       "         89: 11,\n",
       "         10: 11,\n",
       "         14: 8,\n",
       "         27: 5,\n",
       "         126: 16,\n",
       "         125: 7,\n",
       "         96: 9,\n",
       "         79: 5,\n",
       "         88: 12,\n",
       "         12: 6,\n",
       "         13: 13,\n",
       "         63: 5,\n",
       "         124: 8,\n",
       "         64: 6,\n",
       "         30: 8,\n",
       "         95: 6,\n",
       "         33: 5,\n",
       "         66: 5,\n",
       "         114: 5,\n",
       "         21: 5,\n",
       "         70: 5,\n",
       "         83: 5,\n",
       "         28: 5,\n",
       "         92: 5,\n",
       "         85: 5,\n",
       "         127: 6,\n",
       "         81: 6,\n",
       "         113: 5,\n",
       "         68: 5,\n",
       "         46: 5,\n",
       "         56: 5,\n",
       "         35: 5,\n",
       "         90: 5,\n",
       "         122: 5,\n",
       "         37: 6,\n",
       "         24: 7,\n",
       "         80: 5,\n",
       "         121: 5,\n",
       "         1: 8,\n",
       "         54: 5,\n",
       "         123: 8,\n",
       "         11: 5,\n",
       "         82: 5,\n",
       "         0: 9,\n",
       "         91: 5,\n",
       "         58: 9,\n",
       "         86: 6,\n",
       "         120: 5})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
