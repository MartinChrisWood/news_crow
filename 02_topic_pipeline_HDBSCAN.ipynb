{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN should work well on embedding representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 495\n",
      "9.9 percent of files read.\n",
      "19.8 percent of files read.\n"
     ]
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\"\n",
    "\n",
    "# Load up\n",
    "corpus = helper.load_clean_world_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Build Text Model (Representation, eg; word2vec, entities list...)\n",
    "\n",
    "- Trying with the world corpus and with the bing corpus, neither worked with InferSent.  Suspect the problem lies in the PCA step, which may not be working well on this high-dimensional (vector length = 4096) form.\n",
    "- Summed keywords works rather better with the world corpus.\n",
    "- Summed keywords still fail the bing/home office corpus, giving me a cluster about \"immigration\" and a cluster for the American Supreme Court."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Windows didn't play nicely with the vector datasets, Some obscure encoding problem (python in Conda\n",
    "# kept trying to decode using cp1252 regardless of whatever other options I specified!)\n",
    "# Solution; rewrite file and drop any characters the Windows encoder refuses to recognise.\n",
    "# I shouldn't loose too much info.\n",
    "with open('./lib/InferSent/dataset/fastText/crawl-300d-2M.vec', \"r\", encoding=\"cp1252\", errors=\"ignore\") as infile:\n",
    "    with open('./lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec', \"wb\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line.encode('cp1252'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "infersent = reps.InferSentModel(list(corpus['clean_text']),\n",
    "                                list(corpus['clean_text']),\n",
    "                                W2V_PATH = './lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec')\n",
    "\n",
    "embeddings = infersent.get_embeddings()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reload(reps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Whereas this worked first time!\n",
    "glove = reps.GloveWordModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "embeddings = glove.get_embeddings()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Turn that into a DF for me\n",
    "embeddings_df = pd.DataFrame({\"clean_text\": list(embeddings.keys()),\n",
    "                              \"embeddings\": list(embeddings.values())})\n",
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus.merge(embeddings_df, how=\"left\", on=\"clean_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a.  Try a really simple averaged word vector model!\n",
    "\n",
    "With a complex noun extraction function 'cause that part's slow so I multi-threaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 100\n",
    "\n",
    "# Get phrase-conjoined, stem tokens\n",
    "corpus['phrase_tokens'] = helper.get_phrased_nouns(corpus['clean_text'])\n",
    "\n",
    "# Detect and conjoin bigrams\n",
    "model = Word2Vec(corpus['phrase_tokens'], size=vec_size, window=5, min_count=5, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_vec(token_list, model):\n",
    "    \n",
    "    vecs = []\n",
    "    for token in token_list:\n",
    "        try:\n",
    "            vector = model.wv[token]\n",
    "        except: \n",
    "            vector = np.zeros(vec_size)\n",
    "        vecs.append(vector)\n",
    "    \n",
    "    if len(vecs) > 0:\n",
    "        return np.mean(np.asarray(vecs), axis=0)\n",
    "    else:\n",
    "        return np.zeros(vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [get_averaged_vec(tokens, model) for tokens in corpus['phrase_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster Text\n",
    "\n",
    "This is the part where the pipelines get a little more experimental\n",
    "- First; PCA (HDBSCAN prefers < 50 dimensions if possible) (also, try just fitting 50 vectors!)\n",
    "- Second; HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.vstack(vectors)\n",
    "\n",
    "# First, PCA the data\n",
    "pca = PCA(n_components=20, svd_solver='full')\n",
    "\n",
    "# Fit and check\n",
    "embeddings_pca = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Diagnostic stats - both should show exponential decay\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(1, 15):\n",
    "    print(\"\\n testing min_samples: \", i)\n",
    "    # Clusterer fitting\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=i)\n",
    "    clusterer.fit(embeddings_pca)\n",
    "    \n",
    "    # Diagnostic - calculate percentage of records assigned \"outlier\"\n",
    "    print(\"Percent outlier: \", 100.0 * sum(clusterer.labels_ == -1) / clusterer.labels_.shape[0])\n",
    "    \n",
    "    # number of clusters created\n",
    "    print(\"Number of clusters: \", len(pd.unique(clusterer.labels_)))\n",
    "    \n",
    "    # Top populated \n",
    "    a = Counter(clusterer.labels_)\n",
    "    a_list = sorted(a.items(), key = lambda a:(a[1], a[0]), reverse=True)\n",
    "    print(\"Largest 5 clusters: \", a_list[:5])\n",
    "    \n",
    "    # Median cluster size\n",
    "    print(\"Median cluster size: \", sorted(list(a.values()))[int(len(a.values())/2)])\n",
    "    print(\"Mean cluster size: \", np.mean(list(a.values())))\n",
    "    print(\"Percent 'really' clustered :\", 100.0 * (len(clusterer.labels_) - (a_list[0][1] + a_list[1][1])) / len(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clusterer fitting\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=10)\n",
    "clusterer.fit(embeddings_pca)\n",
    "\n",
    "# Diagnostic - calculate percentage of records assigned \"outlier\"\n",
    "print(\"Percent outlier: \", 100.0 * sum(clusterer.labels_ == -1) / clusterer.labels_.shape[0])\n",
    "\n",
    "# number of clusters created\n",
    "print(\"Number of clusters: \", len(pd.unique(clusterer.labels_)))\n",
    "\n",
    "# Record cluster each was assigned to\n",
    "corpus['cluster'] = clusterer.labels_\n",
    "\n",
    "# Record the reverse of outlier score (therefore, higher number = higher certainty of membership)\n",
    "corpus['score'] = 1.0 - clusterer.outlier_scores_\n",
    "\n",
    "corpus.to_csv(\"working/RSS_clustered_w2v_pca.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating silhouette score, a gauge of how well separated clusters\n",
    "# are in some spatial representation\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out the outliers\n",
    "embeddings_score = embeddings_pca[clusterer.labels_ != -1]\n",
    "labels_score = clusterer.labels_[clusterer.labels_ != -1]\n",
    "\n",
    "score = silhouette_score(embeddings_score, labels_score, metric=\"cosine\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"working/RSS_clustered_w2v_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(helper)\n",
    "coherences = helper.report_corpus_model_coherence(\"working/RSS_clustered_w2v_pca.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster text with Spectral Clustering\n",
    "Not working on high-dimensional data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create an adjacency matrix from cosine distances between document vectors\n",
    "embeddings_array = np.vstack(vectors)\n",
    "adjacencies = cosine_similarity(embeddings_array)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create and fit the clustering model\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "clustering = SpectralClustering(n_clusters=66)\n",
    "labels = clustering.fit_predict(adjacencies)\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(embeddings_array)\n",
    "labels = ap.labels_\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster text with Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#kmc = KMeansClusterer(num_means=60, distance=cosine_similarity)\n",
    "#clusters = kmc.cluster(embeddings_array, True, trace=True)\n",
    "#clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=200, random_state=0).fit(embeddings_array)\n",
    "\n",
    "# Diagnostic - calculate percentage of records assigned \"outlier\"\n",
    "print(\"Percent outlier: \", 100.0 * sum(kmeans.labels_ == -1) / kmeans.labels_.shape[0])\n",
    "\n",
    "# number of clusters created\n",
    "print(\"Number of clusters: \", len(pd.unique(kmeans.labels_)))\n",
    "\n",
    "# Record cluster each was assigned to\n",
    "corpus['cluster'] = kmeans.labels_\n",
    "\n",
    "# Record the reverse of outlier score (therefore, higher number = higher certainty of membership)\n",
    "corpus['score'] = 1.0\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)\n",
    "\n",
    "corpus.to_csv(\"working/RSS_kmeans_w2v_direct.csv\", index=False)\n",
    "\n",
    "coherences = helper.report_corpus_model_coherence(\"working/RSS_kmeans_w2v_direct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#kmc = KMeansClusterer(num_means=60, distance=cosine_similarity)\n",
    "#clusters = kmc.cluster(embeddings_array, True, trace=True)\n",
    "#clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=200, random_state=0).fit(embeddings_pca)\n",
    "\n",
    "# Diagnostic - calculate percentage of records assigned \"outlier\"\n",
    "print(\"Percent outlier: \", 100.0 * sum(kmeans.labels_ == -1) / kmeans.labels_.shape[0])\n",
    "\n",
    "# number of clusters created\n",
    "print(\"Number of clusters: \", len(pd.unique(kmeans.labels_)))\n",
    "\n",
    "# Record cluster each was assigned to\n",
    "corpus['cluster'] = kmeans.labels_\n",
    "\n",
    "# Record the reverse of outlier score (therefore, higher number = higher certainty of membership)\n",
    "corpus['score'] = 1.0\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)\n",
    "\n",
    "corpus.to_csv(\"working/RSS_kmeans_w2v_pca.csv\", index=False)\n",
    "\n",
    "coherences = helper.report_corpus_model_coherence(\"working/RSS_kmeans_w2v_pca.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
