{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"C:/Users/Martin/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"bing_cor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, get a list of all the news dumps created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 153\n",
      "Loading file: bing_corpus_2019-09-05_2135.json\n",
      "Loading file: bing_corpus_2019-09-06_0019.json\n",
      "Loading file: bing_corpus_2019-09-06_1221.json\n",
      "Loading file: bing_corpus_2019-09-07_0019.json\n",
      "Loading file: bing_corpus_2019-09-07_1221.json\n",
      "Loading file: bing_corpus_2019-09-08_0019.json\n",
      "Loading file: bing_corpus_2019-09-08_1221.json\n",
      "Loading file: bing_corpus_2019-09-09_0019.json\n",
      "Loading file: bing_corpus_2019-09-09_1221.json\n",
      "Loading file: bing_corpus_2019-09-10_0019.json\n",
      "Loading file: bing_corpus_2019-09-10_1221.json\n",
      "Loading file: bing_corpus_2019-09-11_0019.json\n",
      "Loading file: bing_corpus_2019-09-11_1221.json\n",
      "Loading file: bing_corpus_2019-09-12_0019.json\n",
      "Loading file: bing_corpus_2019-09-12_1221.json\n",
      "Loading file: bing_corpus_2019-09-13_0019.json\n",
      "Loading file: bing_corpus_2019-09-13_1221.json\n",
      "Loading file: bing_corpus_2019-09-14_0019.json\n",
      "Loading file: bing_corpus_2019-09-14_1221.json\n",
      "Loading file: bing_corpus_2019-09-15_0019.json\n",
      "Loading file: bing_corpus_2019-09-15_2059.json\n",
      "Loading file: bing_corpus_2019-09-16_1221.json\n",
      "Loading file: bing_corpus_2019-09-17_0019.json\n",
      "Loading file: bing_corpus_2019-09-17_1221.json\n",
      "Loading file: bing_corpus_2019-09-18_0019.json\n",
      "Loading file: bing_corpus_2019-09-18_1221.json\n",
      "Loading file: bing_corpus_2019-09-19_0019.json\n",
      "Loading file: bing_corpus_2019-09-19_1221.json\n",
      "Loading file: bing_corpus_2019-09-20_0019.json\n",
      "Loading file: bing_corpus_2019-09-20_1221.json\n",
      "Loading file: bing_corpus_2019-09-21_0019.json\n",
      "Loading file: bing_corpus_2019-09-21_1221.json\n",
      "Loading file: bing_corpus_2019-09-22_0019.json\n",
      "Loading file: bing_corpus_2019-09-22_1221.json\n",
      "Loading file: bing_corpus_2019-09-23_0019.json\n",
      "Loading file: bing_corpus_2019-09-23_1222.json\n",
      "Loading file: bing_corpus_2019-09-24_0019.json\n",
      "Loading file: bing_corpus_2019-09-24_1222.json\n",
      "Loading file: bing_corpus_2019-09-25_0019.json\n",
      "Loading file: bing_corpus_2019-09-25_1222.json\n",
      "Loading file: bing_corpus_2019-09-26_0019.json\n",
      "Loading file: bing_corpus_2019-09-26_1222.json\n",
      "Loading file: bing_corpus_2019-09-27_0019.json\n",
      "Loading file: bing_corpus_2019-09-27_1222.json\n",
      "Loading file: bing_corpus_2019-09-28_0019.json\n",
      "Loading file: bing_corpus_2019-09-28_1222.json\n",
      "Loading file: bing_corpus_2019-09-29_0019.json\n",
      "Loading file: bing_corpus_2019-09-29_1222.json\n",
      "Loading file: bing_corpus_2019-09-30_0019.json\n",
      "Loading file: bing_corpus_2019-09-30_1222.json\n",
      "Loading file: bing_corpus_2019-10-01_0019.json\n",
      "Loading file: bing_corpus_2019-10-01_1222.json\n",
      "Loading file: bing_corpus_2019-10-02_0019.json\n",
      "Loading file: bing_corpus_2019-10-02_1222.json\n",
      "Loading file: bing_corpus_2019-10-03_0020.json\n",
      "Loading file: bing_corpus_2019-10-03_1222.json\n",
      "Loading file: bing_corpus_2019-10-04_0019.json\n",
      "Loading file: bing_corpus_2019-10-04_1222.json\n",
      "Loading file: bing_corpus_2019-10-06_1222.json\n",
      "Loading file: bing_corpus_2019-10-07_0019.json\n",
      "Loading file: bing_corpus_2019-10-07_0741.json\n",
      "Loading file: bing_corpus_2019-10-07_1222.json\n",
      "Loading file: bing_corpus_2019-10-08_0019.json\n",
      "Loading file: bing_corpus_2019-10-08_1222.json\n",
      "Loading file: bing_corpus_2019-10-09_0019.json\n",
      "Loading file: bing_corpus_2019-10-09_1222.json\n",
      "Loading file: bing_corpus_2019-10-10_0019.json\n",
      "Loading file: bing_corpus_2019-10-10_1222.json\n",
      "Loading file: bing_corpus_2019-10-11_0019.json\n",
      "Loading file: bing_corpus_2019-10-11_1222.json\n",
      "Loading file: bing_corpus_2019-10-12_0019.json\n",
      "Loading file: bing_corpus_2019-10-12_1222.json\n",
      "Loading file: bing_corpus_2019-10-14_0906.json\n",
      "Loading file: bing_corpus_2019-10-14_2040.json\n",
      "Loading file: bing_corpus_2019-10-14_2051.json\n",
      "Loading file: bing_corpus_2019-10-15_0021.json\n",
      "Loading file: bing_corpus_2019-10-15_1221.json\n",
      "Loading file: bing_corpus_2019-10-16_0022.json\n",
      "Loading file: bing_corpus_2019-10-16_0806.json\n",
      "Loading file: bing_corpus_2019-10-16_1221.json\n",
      "Loading file: bing_corpus_2019-10-17_0021.json\n",
      "Loading file: bing_corpus_2019-10-17_1221.json\n",
      "Loading file: bing_corpus_2019-10-18_0021.json\n",
      "Loading file: bing_corpus_2019-10-18_1221.json\n",
      "Loading file: bing_corpus_2019-10-19_0022.json\n",
      "Loading file: bing_corpus_2019-10-20_0021.json\n",
      "Loading file: bing_corpus_2019-10-20_1221.json\n",
      "Loading file: bing_corpus_2019-10-21_0021.json\n",
      "Loading file: bing_corpus_2019-10-21_1221.json\n",
      "Loading file: bing_corpus_2019-10-22_0021.json\n",
      "Loading file: bing_corpus_2019-10-22_1221.json\n",
      "Loading file: bing_corpus_2019-10-23_0021.json\n",
      "Loading file: bing_corpus_2019-10-23_1221.json\n",
      "Loading file: bing_corpus_2019-10-24_0021.json\n",
      "Loading file: bing_corpus_2019-10-24_1221.json\n",
      "Loading file: bing_corpus_2019-10-25_0021.json\n",
      "Loading file: bing_corpus_2019-10-25_1221.json\n",
      "Loading file: bing_corpus_2019-10-26_0021.json\n",
      "Loading file: bing_corpus_2019-10-26_1221.json\n",
      "Loading file: bing_corpus_2019-10-27_0021.json\n",
      "Loading file: bing_corpus_2019-10-27_1221.json\n",
      "Loading file: bing_corpus_2019-10-28_0021.json\n",
      "Loading file: bing_corpus_2019-10-28_1221.json\n",
      "Loading file: bing_corpus_2019-10-29_0021.json\n",
      "Loading file: bing_corpus_2019-10-29_1221.json\n",
      "Loading file: bing_corpus_2019-10-30_0021.json\n",
      "Loading file: bing_corpus_2019-10-30_1221.json\n",
      "Loading file: bing_corpus_2019-10-31_0647.json\n",
      "Loading file: bing_corpus_2019-10-31_1221.json\n",
      "Loading file: bing_corpus_2019-11-01_0021.json\n",
      "Loading file: bing_corpus_2019-11-01_1423.json\n",
      "Loading file: bing_corpus_2019-11-02_1017.json\n",
      "Loading file: bing_corpus_2019-11-02_1221.json\n",
      "Loading file: bing_corpus_2019-11-03_2202.json\n",
      "Loading file: bing_corpus_2019-11-04_0021.json\n",
      "Loading file: bing_corpus_2019-11-04_1221.json\n",
      "Loading file: bing_corpus_2019-11-05_0021.json\n",
      "Loading file: bing_corpus_2019-11-05_1222.json\n",
      "Loading file: bing_corpus_2019-11-06_0021.json\n",
      "Loading file: bing_corpus_2019-11-07_1000.json\n",
      "Loading file: bing_corpus_2019-11-10_1848.json\n",
      "Loading file: bing_corpus_2019-11-11_0021.json\n",
      "Loading file: bing_corpus_2019-11-11_1221.json\n",
      "Loading file: bing_corpus_2019-11-12_0021.json\n",
      "Loading file: bing_corpus_2019-11-12_1221.json\n",
      "Loading file: bing_corpus_2019-11-13_0852.json\n",
      "Loading file: bing_corpus_2019-11-13_1221.json\n",
      "Loading file: bing_corpus_2019-11-14_0021.json\n",
      "Loading file: bing_corpus_2019-11-14_1221.json\n",
      "Loading file: bing_corpus_2019-11-15_0021.json\n",
      "Loading file: bing_corpus_2019-11-15_1221.json\n",
      "Loading file: bing_corpus_2019-11-16_0021.json\n",
      "Loading file: bing_corpus_2019-11-16_1221.json\n",
      "Loading file: bing_corpus_2019-11-17_0021.json\n",
      "Loading file: bing_corpus_2019-11-17_1221.json\n",
      "Loading file: bing_corpus_2019-11-17_1955.json\n",
      "Loading file: bing_corpus_2019-11-18_0021.json\n",
      "Loading file: bing_corpus_2019-11-18_1221.json\n",
      "Loading file: bing_corpus_2019-11-19_0021.json\n",
      "Loading file: bing_corpus_2019-11-19_1221.json\n",
      "Loading file: bing_corpus_2019-11-22_1539.json\n",
      "Loading file: bing_corpus_2019-11-23_0021.json\n",
      "Loading file: bing_corpus_2019-11-23_1221.json\n",
      "Loading file: bing_corpus_2019-11-24_1221.json\n",
      "Loading file: bing_corpus_2019-11-24_1629.json\n",
      "Loading file: bing_corpus_2019-11-25_0021.json\n",
      "Loading file: bing_corpus_2019-11-25_1221.json\n",
      "Loading file: bing_corpus_2019-11-29_1734.json\n",
      "Loading file: bing_corpus_2019-11-30_0021.json\n",
      "Loading file: bing_corpus_2019-11-30_1221.json\n",
      "Loading file: bing_corpus_2019-12-01_0021.json\n",
      "Loading file: bing_corpus_2019-12-01_1221.json\n",
      "Loading file: bing_corpus_2019-12-02_0021.json\n"
     ]
    }
   ],
   "source": [
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>origin</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>source_url</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-09-05T15:12:00.0000000Z</td>\n",
       "      <td>https://www.gov.uk/government/news/government-...</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>2019-09-05 21:35:05.106001</td>\n",
       "      <td>www.gov.uk</td>\n",
       "      <td>New border controls that will make it harder f...</td>\n",
       "      <td>Government announces &lt;b&gt;immigration&lt;/b&gt; plans ...</td>\n",
       "      <td>Government announces immigration plans for no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-09-05T08:23:00.0000000Z</td>\n",
       "      <td>https://www.thesun.co.uk/news/9865413/home-sec...</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>2019-09-05 21:35:05.107007</td>\n",
       "      <td>www.thesun.co.uk</td>\n",
       "      <td>PRITI PATEL tonight conceded unlimited EU &lt;b&gt;i...</td>\n",
       "      <td>Home Secretary Priti Patel admits No Deal Brex...</td>\n",
       "      <td>Home Secretary Priti Patel admits No Deal Brex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-09-05T16:54:00.0000000Z</td>\n",
       "      <td>https://www.thetelegraphandargus.co.uk/news/17...</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>2019-09-05 21:35:05.108030</td>\n",
       "      <td>www.thetelegraphandargus.co.uk</td>\n",
       "      <td>A STUDENT from Bradford has helped create a sh...</td>\n",
       "      <td>Student film on &lt;b&gt;immigration&lt;/b&gt; focuses on ...</td>\n",
       "      <td>Student film on immigration focuses on those m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019-09-05T14:42:00.0000000Z</td>\n",
       "      <td>https://www.gov.uk/government/publications/no-...</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>2019-09-05 21:35:05.108030</td>\n",
       "      <td>www.gov.uk</td>\n",
       "      <td>The United Kingdom will be leaving the Europea...</td>\n",
       "      <td>No deal &lt;b&gt;immigration&lt;/b&gt; arrangements for EU...</td>\n",
       "      <td>No deal immigration arrangements for EU citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019-09-04T22:12:11.0000000Z</td>\n",
       "      <td>https://www.dailymail.co.uk/wires/ap/article-7...</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>2019-09-05 21:35:05.108030</td>\n",
       "      <td>www.dailymail.co.uk</td>\n",
       "      <td>MEXICO CITY (AP) - Since last year&amp;#39;s carav...</td>\n",
       "      <td>AP EXPLAINS: What changed in 90 days of &lt;b&gt;imm...</td>\n",
       "      <td>AP EXPLAINS: What changed in 0 days of immigra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            date  \\\n",
       "14  2019-09-05T15:12:00.0000000Z   \n",
       "16  2019-09-05T08:23:00.0000000Z   \n",
       "28  2019-09-05T16:54:00.0000000Z   \n",
       "30  2019-09-05T14:42:00.0000000Z   \n",
       "31  2019-09-04T22:12:11.0000000Z   \n",
       "\n",
       "                                                 link         origin  \\\n",
       "14  https://www.gov.uk/government/news/government-...  bing_news_api   \n",
       "16  https://www.thesun.co.uk/news/9865413/home-sec...  bing_news_api   \n",
       "28  https://www.thetelegraphandargus.co.uk/news/17...  bing_news_api   \n",
       "30  https://www.gov.uk/government/publications/no-...  bing_news_api   \n",
       "31  https://www.dailymail.co.uk/wires/ap/article-7...  bing_news_api   \n",
       "\n",
       "           retrieval_timestamp                      source_url  \\\n",
       "14  2019-09-05 21:35:05.106001                      www.gov.uk   \n",
       "16  2019-09-05 21:35:05.107007                www.thesun.co.uk   \n",
       "28  2019-09-05 21:35:05.108030  www.thetelegraphandargus.co.uk   \n",
       "30  2019-09-05 21:35:05.108030                      www.gov.uk   \n",
       "31  2019-09-05 21:35:05.108030             www.dailymail.co.uk   \n",
       "\n",
       "                                              summary  \\\n",
       "14  New border controls that will make it harder f...   \n",
       "16  PRITI PATEL tonight conceded unlimited EU <b>i...   \n",
       "28  A STUDENT from Bradford has helped create a sh...   \n",
       "30  The United Kingdom will be leaving the Europea...   \n",
       "31  MEXICO CITY (AP) - Since last year&#39;s carav...   \n",
       "\n",
       "                                                title  \\\n",
       "14  Government announces <b>immigration</b> plans ...   \n",
       "16  Home Secretary Priti Patel admits No Deal Brex...   \n",
       "28  Student film on <b>immigration</b> focuses on ...   \n",
       "30  No deal <b>immigration</b> arrangements for EU...   \n",
       "31  AP EXPLAINS: What changed in 90 days of <b>imm...   \n",
       "\n",
       "                                           clean_text  \n",
       "14  Government announces immigration plans for no ...  \n",
       "16  Home Secretary Priti Patel admits No Deal Brex...  \n",
       "28  Student film on immigration focuses on those m...  \n",
       "30  No deal immigration arrangements for EU citize...  \n",
       "31  AP EXPLAINS: What changed in 0 days of immigra...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Build Text Model (Representation, eg; word2vec, entities list...)\n",
    "\n",
    "- Trying with the world corpus and with the bing corpus, neither worked with InferSent.  Suspect the problem lies in the PCA step, which may not be working well on this high-dimensional (vector length = 4096) form.\n",
    "- Summed keywords works rather better with the world corpus.\n",
    "- Summed keywords still fail the bing/home office corpus, giving me a cluster about \"immigration\" and a cluster for the American Supreme Court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows didn't play nicely with the vector datasets, Some obscure encoding problem (python in Conda\n",
    "# kept trying to decode using cp1252 regardless of whatever other options I specified!)\n",
    "# Solution; rewrite file and drop any characters the Windows encoder refuses to recognise.\n",
    "# I shouldn't loose too much info.\n",
    "with open('./lib/InferSent/dataset/fastText/crawl-300d-2M.vec', \"r\", encoding=\"cp1252\", errors=\"ignore\") as infile:\n",
    "    with open('./lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec', \"wb\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line.encode('cp1252'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16344(/17591) words with w2v vectors\n",
      "Vocab size : 16344\n"
     ]
    }
   ],
   "source": [
    "infersent = reps.InferSentModel(list(corpus['clean_text']),\n",
    "                                list(corpus['clean_text']),\n",
    "                                W2V_PATH = './lib/InferSent/dataset/fastText/crawl-300d-2M_win.vec')\n",
    "\n",
    "embeddings = infersent.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Whereas this worked first time!\n",
    "glove = reps.GloveWordModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "embeddings = glove.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn that into a DF for me\n",
    "embeddings_df = pd.DataFrame({\"clean_text\": list(embeddings.keys()),\n",
    "                              \"embeddings\": list(embeddings.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Government announces immigration plans for no ...</td>\n",
       "      <td>[0.062002, 0.03705715, 0.042470187, 0.00532962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home Secretary Priti Patel admits No Deal Brex...</td>\n",
       "      <td>[0.05766312, 0.0521763, 0.101948425, 0.0049104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student film on immigration focuses on those m...</td>\n",
       "      <td>[0.04688829, 0.059768386, 0.0808005, -0.017053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No deal immigration arrangements for EU citize...</td>\n",
       "      <td>[0.065198794, 0.017605491, 0.091646925, 0.0291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP EXPLAINS: What changed in 0 days of immigra...</td>\n",
       "      <td>[0.06322765, 0.08380522, 0.037862558, 0.018137...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  \\\n",
       "0  Government announces immigration plans for no ...   \n",
       "1  Home Secretary Priti Patel admits No Deal Brex...   \n",
       "2  Student film on immigration focuses on those m...   \n",
       "3  No deal immigration arrangements for EU citize...   \n",
       "4  AP EXPLAINS: What changed in 0 days of immigra...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.062002, 0.03705715, 0.042470187, 0.00532962...  \n",
       "1  [0.05766312, 0.0521763, 0.101948425, 0.0049104...  \n",
       "2  [0.04688829, 0.059768386, 0.0808005, -0.017053...  \n",
       "3  [0.065198794, 0.017605491, 0.091646925, 0.0291...  \n",
       "4  [0.06322765, 0.08380522, 0.037862558, 0.018137...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster Text\n",
    "\n",
    "This is the part where the pipelines get a little more experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.vstack(embeddings_df['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4583, 4096)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, PCA the data\n",
    "pca = PCA(n_components=20, svd_solver='full')\n",
    "\n",
    "embeddings_pca = pca.fit_transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4583, 20)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07990959 0.04421469 0.0269681  0.02210961 0.01783179 0.01462192\n",
      " 0.01230171 0.0114431  0.00995354 0.00972468 0.00835635 0.00819447\n",
      " 0.00742902 0.00729862 0.00666714 0.00633519 0.00582932 0.00562423\n",
      " 0.00552668 0.00536528]\n",
      "[25.707434  19.122408  14.934285  13.522275  12.143858  10.996676\n",
      " 10.086535   9.7281685  9.072938   8.968028   8.313187   8.232273\n",
      "  7.838355   7.7692585  7.425558   7.2383437  6.943335   6.820099\n",
      "  6.7606955  6.6612444]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  0, -1,  1], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=3)\n",
    "clusterer.fit(embeddings_pca)\n",
    "pd.unique(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 3026, 0: 1473, -1: 67, 1: 17})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
