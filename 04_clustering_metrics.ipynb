{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauge how well the clusters separate the data\n",
    "\n",
    "#### Suggested approach:\n",
    "\n",
    "Coherence is not meant to work with clusters, only topics. It can only work with either \n",
    "- an existing LDA model, in which case it does something clever\n",
    "- A corpus, a dictionary and a list of [sets of?] key words representing topics\n",
    "\n",
    "To make models from different sources comparable, it could be quite tricky\n",
    "\n",
    "The other tool I could use is Silhouette scores, but they'll only work really for flat clustering models.\n",
    "\n",
    "TODO\n",
    "- Implement coherence against tokens\n",
    "- Implement Silhouette scoring against TF-IDF representations\n",
    "- Implement relevance scoring for retrieval of top tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Define which stemmer to use in the pipeline later\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_desc(description):\n",
    "    \"\"\" Helper, tokeniser \"\"\"\n",
    "    return( [stemmer.stem(token) for token in simple_preprocess(str(description)) if token not in STOPWORDS] )\n",
    "\n",
    "\n",
    "def get_stats(df):\n",
    "    \"\"\" Helper, for printing basic info on corpus extent \"\"\"\n",
    "    \n",
    "    df['doc_size'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    print(np.mean(df['doc_size']))\n",
    "    print(df.shape[0])\n",
    "    print(max(df['date']))\n",
    "    print(min(df['date']))\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_keyword_stats(df, search_term_path = \"D:/Dropbox/news_crow/scrape_settings.json\"):\n",
    "    \"\"\"Retrive the set of search terms used for Bing, sum stories that contain them \"\"\"\n",
    "    with open(search_term_path, \"r\") as f:\n",
    "        scrape_config = json.load(f)\n",
    "        \n",
    "    search_terms = scrape_config['disaster_search_list']\n",
    "    search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "    search_terms = set(search_terms)\n",
    "    \n",
    "    term_results = {}\n",
    "    \n",
    "    for term in search_terms:\n",
    "        term_results[term] = sum(df['clean_text'].apply(lambda x: term in x.lower()))\n",
    "    \n",
    "    return(term_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_model_coherence(df, cluster_column=\"cluster\", tokens_column=\"tokens\"):\n",
    "    \"\"\" Helper, encapsulates entire coherence model-building process for (flat) models \"\"\"\n",
    "    # Create the vocabulary record\n",
    "    bow_dictionary = gensim.corpora.Dictionary(list(df[tokens_column]))\n",
    "    \n",
    "    # Create a BOW model\n",
    "    bow_corpus = [bow_dictionary.doc2bow(doc) for doc in df[tokens_column]]\n",
    "    \n",
    "    # Flattened list of all tokens for all documents for each \"topic\"\n",
    "    topics = {}\n",
    "    for topic in pd.unique(df['cluster']):\n",
    "        subset = df[df['cluster'] == topic]\n",
    "        topics[topic] = flatten(list(subset['tokens']))\n",
    "    \n",
    "    # Calculate ALL THE COHERENCE\n",
    "    coherences = {}\n",
    "    \n",
    "    # c_v is most performant indirect confirmation measure\n",
    "    cm = CoherenceModel(topics=list(topics.values()),\n",
    "                        texts=list(df['tokens']),\n",
    "                        dictionary=bow_dictionary,\n",
    "                        coherence='c_v')\n",
    "    coherences['c_v'] = cm.get_coherence()\n",
    "    \n",
    "    # c_npmi is most performant direct confirmation measure (that I don't have to implement myself)\n",
    "    cm = CoherenceModel(topics=list(topics.values()),\n",
    "                        texts=list(df['tokens']),\n",
    "                        dictionary=bow_dictionary,\n",
    "                        coherence='c_npmi')\n",
    "    coherences['c_npmi'] = cm.get_coherence()\n",
    "    \n",
    "    # Redundant Measure\n",
    "    #cm = CoherenceModel(topics=list(topics.values()),\n",
    "    #                    corpus=bow_corpus,\n",
    "    #                    dictionary=bow_dictionary,\n",
    "    #                    coherence='u_mass')\n",
    "    #coherences['u_mass'] = cm.get_coherence()\n",
    "        \n",
    "    return(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluate_corpus(data_path):\n",
    "    \"\"\" Helper - process a corpus csv, return its coherence scores \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    df[\"tokens\"] = df[\"clean_text\"].apply(preprocess_desc)\n",
    "    \n",
    "    # Restore \"tokens\" from horrific string format\n",
    "    # REF CHECK THIS FOR COMPATIBILITY WITH OTHER CLEANING STEPS\n",
    "    #df['tokens'] = df['tokens'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9,]\", \"\", x).split(\",\"))\n",
    "    \n",
    "    return get_corpus_model_coherence(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing requirements of manual coherence scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus, dictionary and lists of all tokens for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [\"disaster_clustered_lda.csv\",\n",
    "                \"disaster_clustered_w2v.csv\",\n",
    "                \"disaster_clustered_cliques.csv\",\n",
    "                \"disaster_clustered_louvain.csv\",\n",
    "                \"disaster_clustered_bigclam.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8a9a2ac0c580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcoherences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_evaluate_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"working/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-d8720cedec55>\u001b[0m in \u001b[0;36mload_evaluate_corpus\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#df['tokens'] = df['tokens'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9,]\", \"\", x).split(\",\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_corpus_model_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-82f5d92587b9>\u001b[0m in \u001b[0;36mget_corpus_model_coherence\u001b[1;34m(df, cluster_column, tokens_column)\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbow_dictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                         coherence='c_v')\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcoherences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c_v'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# c_npmi is most performant direct confirmation measure (that I don't have to implement myself)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \"\"\"\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mconfirmed_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[1;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoherence\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c_npmi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maggregate_measures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_coherences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(segmented_topics, accumulator, topics, measure, gamma, with_std, with_support)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0msegment_sims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_segments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_star\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_segments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mw_prime_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mw_star_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0msegment_sims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_cossim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_prime_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_star_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_context_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_context_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_word_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py\u001b[0m in \u001b[0;36mcompute_context_vector\u001b[1;34m(self, segment_word_ids, topic_word_ids)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_vector_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontext_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mcontext_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_seg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_word_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_vector_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontext_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py\u001b[0m in \u001b[0;36m_make_seg\u001b[1;34m(self, segment_word_ids, topic_word_ids)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mcontext_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__iter__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0msegment_word_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msegment_word_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized lil_matrix constructor usage'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coherences = {}\n",
    "\n",
    "for file in corpus_files:\n",
    "    print(\"Processing: {}\".format(file))\n",
    "    coherences[file.strip(\".csv\")] = load_evaluate_corpus(\"working/\" + file)\n",
    "    print(coherences[file.strip(\".csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
