{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 315\n",
      "9.8 percent of files read.\n",
      "19.7 percent of files read.\n",
      "29.5 percent of files read.\n",
      "39.4 percent of files read.\n",
      "49.2 percent of files read.\n",
      "59.0 percent of files read.\n",
      "68.9 percent of files read.\n",
      "78.7 percent of files read.\n",
      "88.6 percent of files read.\n",
      "98.4 percent of files read.\n",
      "(22786, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>West Midlands &lt;b&gt;flood&lt;/b&gt; warnings prompt &amp;#3...</td>\n",
       "      <td>Residents have been warned to &amp;quot;remain vig...</td>\n",
       "      <td>2019-11-17T17:35:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-50451817</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278878</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>West Midlands flood warnings prompt ;remain vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New &lt;b&gt;flood&lt;/b&gt; warnings issued with more hom...</td>\n",
       "      <td>The Environment Agency has a number of &lt;b&gt;floo...</td>\n",
       "      <td>2019-11-17T18:35:00.0000000Z</td>\n",
       "      <td>https://www.hulldailymail.co.uk/news/hull-east...</td>\n",
       "      <td>www.hulldailymail.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278928</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>New flood warnings issued with more homes at r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UK weather forecast – More than 100 &lt;b&gt;flood&lt;/...</td>\n",
       "      <td>&lt;b&gt;FLOOD&lt;/b&gt;-ravaged villages in the UK have b...</td>\n",
       "      <td>2019-11-17T13:45:00.0000000Z</td>\n",
       "      <td>https://www.thesun.co.uk/news/10342583/uk-weat...</td>\n",
       "      <td>www.thesun.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278953</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast – More than 100 flood aler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>UK &lt;b&gt;flood&lt;/b&gt; warning map: &lt;b&gt;Flood&lt;/b&gt; chao...</td>\n",
       "      <td>The Environment Agency has issued 57 &lt;b&gt;flood&lt;...</td>\n",
       "      <td>2019-11-17T16:38:00.0000000Z</td>\n",
       "      <td>https://www.express.co.uk/news/weather/1205629...</td>\n",
       "      <td>www.express.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279028</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK flood warning map: Flood chaos to continue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>UK weather forecast: &lt;b&gt;Flood&lt;/b&gt; chaos contin...</td>\n",
       "      <td>Despite some areas enduring their &amp;#39;wettest...</td>\n",
       "      <td>2019-11-17T18:32:00.0000000Z</td>\n",
       "      <td>https://www.mirror.co.uk/news/uk-news/uk-weath...</td>\n",
       "      <td>www.mirror.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279047</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast: Flood chaos continues wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  West Midlands <b>flood</b> warnings prompt &#3...   \n",
       "1         1  New <b>flood</b> warnings issued with more hom...   \n",
       "2         2  UK weather forecast – More than 100 <b>flood</...   \n",
       "3         5  UK <b>flood</b> warning map: <b>Flood</b> chao...   \n",
       "4         6  UK weather forecast: <b>Flood</b> chaos contin...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Residents have been warned to &quot;remain vig...   \n",
       "1     The Environment Agency has a number of <b>floo...   \n",
       "2     <b>FLOOD</b>-ravaged villages in the UK have b...   \n",
       "3     The Environment Agency has issued 57 <b>flood<...   \n",
       "4     Despite some areas enduring their &#39;wettest...   \n",
       "\n",
       "                              date  \\\n",
       "node                                 \n",
       "0     2019-11-17T17:35:00.0000000Z   \n",
       "1     2019-11-17T18:35:00.0000000Z   \n",
       "2     2019-11-17T13:45:00.0000000Z   \n",
       "3     2019-11-17T16:38:00.0000000Z   \n",
       "4     2019-11-17T18:32:00.0000000Z   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0        https://www.bbc.co.uk/news/uk-england-50451817   \n",
       "1     https://www.hulldailymail.co.uk/news/hull-east...   \n",
       "2     https://www.thesun.co.uk/news/10342583/uk-weat...   \n",
       "3     https://www.express.co.uk/news/weather/1205629...   \n",
       "4     https://www.mirror.co.uk/news/uk-news/uk-weath...   \n",
       "\n",
       "                   source_url         retrieval_timestamp         origin  \\\n",
       "node                                                                       \n",
       "0               www.bbc.co.uk  2019-11-17 19:50:58.278878  bing_news_api   \n",
       "1     www.hulldailymail.co.uk  2019-11-17 19:50:58.278928  bing_news_api   \n",
       "2            www.thesun.co.uk  2019-11-17 19:50:58.278953  bing_news_api   \n",
       "3           www.express.co.uk  2019-11-17 19:50:58.279028  bing_news_api   \n",
       "4            www.mirror.co.uk  2019-11-17 19:50:58.279047  bing_news_api   \n",
       "\n",
       "                                             clean_text  \n",
       "node                                                     \n",
       "0     West Midlands flood warnings prompt ;remain vi...  \n",
       "1     New flood warnings issued with more homes at r...  \n",
       "2     UK weather forecast – More than 100 flood aler...  \n",
       "3     UK flood warning map: Flood chaos to continue ...  \n",
       "4     UK weather forecast: Flood chaos continues wit...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"disaster\"\n",
    "\n",
    "# There's a helper function to go find and drag out the various JSON files created by the scrapers.\n",
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Detected Nouns to create a Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the set of search terms used for Bing, so we can remove them before\n",
    "# clustering.\n",
    "with open(\"D:/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "    scrape_config = json.load(f)\n",
    "\n",
    "search_terms = scrape_config['disaster_search_list']\n",
    "search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quot;Dave</th>\n",
       "      <th>Timo</th>\n",
       "      <th>Fyrish</th>\n",
       "      <th>Tylorstown</th>\n",
       "      <th>North_Korea</th>\n",
       "      <th>Rob_Burrow</th>\n",
       "      <th>Ysgol</th>\n",
       "      <th>Hyson</th>\n",
       "      <th>GNAAS</th>\n",
       "      <th>GoCompare</th>\n",
       "      <th>...</th>\n",
       "      <th>Wasps</th>\n",
       "      <th>Cha</th>\n",
       "      <th>Goodwood</th>\n",
       "      <th>DIGITLIGHT</th>\n",
       "      <th>HolidayCottages.co.uk</th>\n",
       "      <th>coronvarius</th>\n",
       "      <th>Vladimir</th>\n",
       "      <th>East_Derbyshire</th>\n",
       "      <th>Gleave</th>\n",
       "      <th>Chris_Hemsworth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>West Midlands flood warnings prompt ;remain vigilant; alert. Residents have been warned to quot;remain vigilantquot; as up to 20 flood warnings are in place in the West Midlands with more rain forecast. There are 1 warnings affecting Worcestershire, along the River Severn, Avon and Teme, and six in Shropshire. Flood defences were put up in Ironbridge on Saturday evening. The Environment Agency (EA) said river ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New flood warnings issued with more homes at risk. The Environment Agency has a number of flood alerts and warnings in place More residents are being told that floodwater could enter their homes as new red warnings are put in place. The Environment Agency has updated the flood risk for Hull and East Yorkshire this afternoon with four red flood warnings now in force. A red warning indicates that ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK weather forecast – More than 100 flood alerts across Britain as villages are cut off for days and more storms hit. FLOOD-ravaged villages in the UK have been cut off for days as Atlantic storms threaten to unleash another deluge early next week. Swathes of Britain that were left devastated by torrential downpours will face yet more floods - with more than 100 alerts in place. It comes amid predictions bitterly cold weather will largely hold out for the rest ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK flood warning map: Flood chaos to continue - Where is under threat of flooding?. The Environment Agency has issued 57 flood warnings at the time of writing, meaning flooding is expected and immediate action is required. There are also 0 flood alerts in place across the country, warning flooding is possible and to be prepared, READ MORE: Snow maps latest forecast: Arctic blast to hit UK with snow and sleet To see a full ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK weather forecast: Flood chaos continues with -5C freeze to follow ;wettest autumn;. Despite some areas enduring their ;wettest ever autumns;, much-needed relief from heavy rainfall has been forecast for flood-hit areas in the coming days</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    quot;Dave  Timo  Fyrish  \\\n",
       "West Midlands flood warnings prompt ;remain vig...          0     0       0   \n",
       "New flood warnings issued with more homes at ri...          0     0       0   \n",
       "UK weather forecast – More than 100 flood alert...          0     0       0   \n",
       "UK flood warning map: Flood chaos to continue -...          0     0       0   \n",
       "UK weather forecast: Flood chaos continues with...          0     0       0   \n",
       "\n",
       "                                                    Tylorstown  North_Korea  \\\n",
       "West Midlands flood warnings prompt ;remain vig...           0            0   \n",
       "New flood warnings issued with more homes at ri...           0            0   \n",
       "UK weather forecast – More than 100 flood alert...           0            0   \n",
       "UK flood warning map: Flood chaos to continue -...           0            0   \n",
       "UK weather forecast: Flood chaos continues with...           0            0   \n",
       "\n",
       "                                                    Rob_Burrow  Ysgol  Hyson  \\\n",
       "West Midlands flood warnings prompt ;remain vig...           0      0      0   \n",
       "New flood warnings issued with more homes at ri...           0      0      0   \n",
       "UK weather forecast – More than 100 flood alert...           0      0      0   \n",
       "UK flood warning map: Flood chaos to continue -...           0      0      0   \n",
       "UK weather forecast: Flood chaos continues with...           0      0      0   \n",
       "\n",
       "                                                    GNAAS  GoCompare  ...  \\\n",
       "West Midlands flood warnings prompt ;remain vig...      0          0  ...   \n",
       "New flood warnings issued with more homes at ri...      0          0  ...   \n",
       "UK weather forecast – More than 100 flood alert...      0          0  ...   \n",
       "UK flood warning map: Flood chaos to continue -...      0          0  ...   \n",
       "UK weather forecast: Flood chaos continues with...      0          0  ...   \n",
       "\n",
       "                                                    Wasps  Cha  Goodwood  \\\n",
       "West Midlands flood warnings prompt ;remain vig...      0    0         0   \n",
       "New flood warnings issued with more homes at ri...      0    0         0   \n",
       "UK weather forecast – More than 100 flood alert...      0    0         0   \n",
       "UK flood warning map: Flood chaos to continue -...      0    0         0   \n",
       "UK weather forecast: Flood chaos continues with...      0    0         0   \n",
       "\n",
       "                                                    DIGITLIGHT  \\\n",
       "West Midlands flood warnings prompt ;remain vig...           0   \n",
       "New flood warnings issued with more homes at ri...           0   \n",
       "UK weather forecast – More than 100 flood alert...           0   \n",
       "UK flood warning map: Flood chaos to continue -...           0   \n",
       "UK weather forecast: Flood chaos continues with...           0   \n",
       "\n",
       "                                                    HolidayCottages.co.uk  \\\n",
       "West Midlands flood warnings prompt ;remain vig...                      0   \n",
       "New flood warnings issued with more homes at ri...                      0   \n",
       "UK weather forecast – More than 100 flood alert...                      0   \n",
       "UK flood warning map: Flood chaos to continue -...                      0   \n",
       "UK weather forecast: Flood chaos continues with...                      0   \n",
       "\n",
       "                                                    coronvarius  Vladimir  \\\n",
       "West Midlands flood warnings prompt ;remain vig...            0         0   \n",
       "New flood warnings issued with more homes at ri...            0         0   \n",
       "UK weather forecast – More than 100 flood alert...            0         0   \n",
       "UK flood warning map: Flood chaos to continue -...            0         0   \n",
       "UK weather forecast: Flood chaos continues with...            0         0   \n",
       "\n",
       "                                                    East_Derbyshire  Gleave  \\\n",
       "West Midlands flood warnings prompt ;remain vig...                0       0   \n",
       "New flood warnings issued with more homes at ri...                0       0   \n",
       "UK weather forecast – More than 100 flood alert...                0       0   \n",
       "UK flood warning map: Flood chaos to continue -...                0       0   \n",
       "UK weather forecast: Flood chaos continues with...                0       0   \n",
       "\n",
       "                                                    Chris_Hemsworth  \n",
       "West Midlands flood warnings prompt ;remain vig...                0  \n",
       "New flood warnings issued with more homes at ri...                0  \n",
       "UK weather forecast – More than 100 flood alert...                0  \n",
       "UK flood warning map: Flood chaos to continue -...                0  \n",
       "UK weather forecast: Flood chaos continues with...                0  \n",
       "\n",
       "[5 rows x 24211 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the text representation\n",
    "model = reps.NounAdjacencyModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "# Tabulate for convenience\n",
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "#### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22786, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get X most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only most common\n",
    "nouns_to_keep = nouns_to_keep[:2000]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 3\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges - not sure if needed?\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12596"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c.  Try CDLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdlib\n",
    "from cdlib import algorithms\n",
    "from cdlib import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple (flat) clustering\n",
    "lp_coms = algorithms.label_propagation(G)\n",
    "\n",
    "# Traditional (easy) community detection\n",
    "louvain_coms = algorithms.louvain(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchingResult(score=0.9612445460699277, std=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This result implies that the two methods have come to very similar conclusions...\n",
    "# This function apparently isn't defined for overlapping communities\n",
    "evaluation.normalized_mutual_information(lp_coms, louvain_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dict of node-to-cluster lookup\n",
    "community_lookup = {}\n",
    "for comm_index, members in enumerate(louvain_coms.communities):\n",
    "    for member in members:\n",
    "        community_lookup[member] = comm_index\n",
    "\n",
    "# Add cluster to DF.  If node not in cluster, assign -1 (outlier)\n",
    "corpus['node'] = corpus.index\n",
    "corpus['cluster'] = corpus['node'].apply(lambda x: community_lookup.get(x, -1))\n",
    "corpus[['clean_text', 'cluster']].head(10)\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.21276222241727"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage are now classed as outliers?\n",
    "100.0 * sum(corpus['cluster']==-1) / corpus.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique clusters after all this?  (minus one for outliers)\n",
    "len(pd.unique(corpus['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv(\"working/disaster_clustered_louvain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigclam_coms.communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigclam_coms.average_internal_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigclam_coms.newman_girvan_modularity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Create (overlapping) clusters using Maximal Cliques\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "Expanded using k-clique-communities REF FIND PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(nx.algorithms.community.kclique.k_clique_communities(G, 4))\n",
    "cliques = [(len(x), x) for x in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()\n",
    "\n",
    "cliques_df = cliques_df[(cliques_df['clique_size'] >= 3) & (cliques_df['clique_size'] <=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cliques_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the cliques DF into long format\n",
    "flattened = {\"cluster_index\":[], \"node\":[]}\n",
    "\n",
    "for index, row in cliques_df.iterrows():\n",
    "    for node in row[\"nodes_list\"]:\n",
    "        flattened[\"cluster_index\"].append(index)\n",
    "        flattened[\"node\"].append(node)\n",
    "        \n",
    "\n",
    "partition_df = pd.DataFrame(flattened)\n",
    "\n",
    "# Create a single string variable (\";\" separated) to record all clusters/cliques a single record belongs in\n",
    "partition_df[\"cluster\"] = partition_df.\\\n",
    "                          groupby(\"node\")[\"cluster_index\"].\\\n",
    "                          transform(lambda x: \";\".join([str(i) for i in x if type(i)==int]))\n",
    "\n",
    "# Clean up, set index of this and corpus so the two DF's can be joined with little effort\n",
    "partition_df = partition_df[[\"node\", \"cluster\"]].\\\n",
    "               drop_duplicates([\"node\", \"cluster\"], keep=\"first\").\\\n",
    "               set_index(\"node\")\n",
    "\n",
    "corpus.drop([\"cluster\", \"node\"], axis=1).join(partition_df).\\\n",
    "       to_csv(\"working/disaster_clustered_cliques.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below attempts overlapping community detection but can only run on connected graphs, think this is an implicit restriction of the algorithm logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all connected components (will become less of an issue as graph size increases)\n",
    "ccs = [(len(x), x) for x in nx.connected_components(G)]\n",
    "\n",
    "# Sort by size (largest first)\n",
    "ccs.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract largest connected sub-graph\n",
    "connected_sub = G.subgraph(ccs[0][1])\n",
    "\n",
    "# re-index nodes from zero to maintain compatibility with CDLIB (sub-dependency, Karate)\n",
    "# Will need to reverse this indexing when matching assigned clusters back to data\n",
    "node_relabel_dict = {val: i for i, val in enumerate(list(connected_sub.nodes))}\n",
    "\n",
    "connected_sub = nx.relabel_nodes(connected_sub, node_relabel_dict)\n",
    "\n",
    "# Fire algo!\n",
    "bigclam_coms = algorithms.big_clam(connected_sub)\n",
    "#leiden_coms = algorithms.leiden(connected_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  ...],\n",
       " [2920]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigclam_coms.communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4948e75788d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# If cluster is smaller than minimum limit, designate as outlier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcs_lookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcs_lookup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mto_dict\u001b[1;34m(self, into)\u001b[0m\n\u001b[0;32m   1556\u001b[0m         \u001b[1;31m# GH16122\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0minto_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1558\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minto_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Build dict of node-to-cluster lookup\n",
    "community_lookup = {}\n",
    "for comm_index, members in enumerate(bigclam_coms.communities):\n",
    "    for member in members:\n",
    "        community_lookup[member] = community_lookup.get(member, []) + [comm_index]\n",
    "\n",
    "# Add cluster to DF.  If node not in cluster, assign -1 (outlier)\n",
    "corpus['node'] = corpus.index\n",
    "corpus['cluster'] = corpus['node'].apply(lambda x: community_lookup.get(x, [-1]))\n",
    "corpus[['clean_text', 'cluster']].head(10)\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage are now classed as outliers?\n",
    "100.0 * sum(corpus['cluster']==-1) / corpus.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique clusters after all this?  (minus one for outliers)\n",
    "len(pd.unique(corpus['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv(\"working/disaster_clustered_bigclam.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
