{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"/home/ozwald/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, get a list of all the news dumps created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 203\n",
      "Loading file: RSS_corpus_2019-11-01_0022.json\n",
      "Loading file: RSS_corpus_2019-11-12_0021.json\n",
      "Loading file: RSS_corpus_2019-10-25_1222.json\n",
      "Loading file: RSS_corpus_2019-12-15_0022.json\n",
      "Loading file: RSS_corpus_2019-11-23_0023.json\n",
      "Loading file: RSS_corpus_2019-10-26_1221.json\n",
      "Loading file: RSS_corpus_2019-10-18_1222.json\n",
      "Loading file: RSS_corpus_2019-09-23_0020.json\n",
      "Loading file: RSS_corpus_2019-10-15_0022.json\n",
      "Loading file: RSS_corpus_2019-11-05_0022.json\n",
      "Loading file: RSS_corpus_2019-09-06_0020.json\n",
      "Loading file: RSS_corpus_2019-10-06_1223.json\n",
      "Loading file: RSS_corpus_2019-12-23_1222.json\n",
      "Loading file: RSS_corpus_2019-09-14_1222.json\n",
      "Loading file: RSS_corpus_2019-09-20_1222.json\n",
      "Loading file: RSS_corpus_2019-09-22_1222.json\n",
      "Loading file: RSS_corpus_2019-11-18_1223.json\n",
      "Loading file: RSS_corpus_2019-09-12_1222.json\n",
      "Loading file: RSS_corpus_2019-11-30_0022.json\n",
      "Loading file: RSS_corpus_2019-11-04_0022.json\n",
      "Loading file: RSS_corpus_2019-10-28_0022.json\n",
      "Loading file: RSS_corpus_2019-12-18_1222.json\n",
      "Loading file: RSS_corpus_2019-10-21_1221.json\n",
      "Loading file: RSS_corpus_2019-09-12_0020.json\n",
      "Loading file: RSS_corpus_2019-12-13_0857.json\n",
      "Loading file: RSS_corpus_2019-11-14_0022.json\n",
      "Loading file: RSS_corpus_2019-09-16_0019.json\n",
      "Loading file: RSS_corpus_2019-10-17_1222.json\n",
      "Loading file: RSS_corpus_2019-09-27_0020.json\n",
      "Loading file: RSS_corpus_2019-11-13_1222.json\n",
      "Loading file: RSS_corpus_2019-10-09_1223.json\n",
      "Loading file: RSS_corpus_2019-12-08_0023.json\n",
      "Loading file: RSS_corpus_2019-11-07_1001.json\n",
      "Loading file: RSS_corpus_2019-12-16_0022.json\n",
      "Loading file: RSS_corpus_2019-12-17_0022.json\n",
      "Loading file: RSS_corpus_2019-12-14_1222.json\n",
      "Loading file: RSS_corpus_2019-10-31_0021.json\n",
      "Loading file: RSS_corpus_2019-10-01_1223.json\n",
      "Loading file: RSS_corpus_2019-09-24_1222.json\n",
      "Loading file: RSS_corpus_2019-10-30_0021.json\n",
      "Loading file: RSS_corpus_2019-09-25_0020.json\n",
      "Loading file: RSS_corpus_2019-09-21_1222.json\n",
      "Loading file: RSS_corpus_2019-12-16_1222.json\n",
      "Loading file: RSS_corpus_2019-09-10_1222.json\n",
      "Loading file: RSS_corpus_2019-11-06_1221.json\n",
      "Loading file: RSS_corpus_2019-10-07_1223.json\n",
      "Loading file: RSS_corpus_2019-12-20_1721.json\n",
      "Loading file: RSS_corpus_2019-10-20_1222.json\n",
      "Loading file: RSS_corpus_2019-12-09_0022.json\n",
      "Loading file: RSS_corpus_2019-11-15_0022.json\n",
      "Loading file: RSS_corpus_2019-11-19_0023.json\n",
      "Loading file: RSS_corpus_2019-11-14_1222.json\n",
      "Loading file: RSS_corpus_2019-11-25_0023.json\n",
      "Loading file: RSS_corpus_2019-11-17_1953.json\n",
      "Loading file: RSS_corpus_2019-10-26_0021.json\n",
      "Loading file: RSS_corpus_2019-12-19_1232.json\n",
      "Loading file: RSS_corpus_2019-11-05_1222.json\n",
      "Loading file: RSS_corpus_2019-12-02_0022.json\n",
      "Loading file: RSS_corpus_2019-09-15_0020.json\n",
      "Loading file: RSS_corpus_2019-09-09_1222.json\n",
      "Loading file: RSS_corpus_2019-09-17_0020.json\n",
      "Loading file: RSS_corpus_2019-10-11_0021.json\n",
      "Loading file: RSS_corpus_2019-11-22_1541.json\n",
      "Loading file: RSS_corpus_2019-09-26_0021.json\n",
      "Loading file: RSS_corpus_2019-10-27_0022.json\n",
      "Loading file: RSS_corpus_2019-11-15_1221.json\n",
      "Loading file: RSS_corpus_2019-12-19_0022.json\n",
      "Loading file: RSS_corpus_2019-12-05_1222.json\n",
      "Loading file: RSS_corpus_2019-11-30_1222.json\n",
      "Loading file: RSS_corpus_2019-11-10_1849.json\n",
      "Loading file: RSS_corpus_2019-09-20_0019.json\n",
      "Loading file: RSS_corpus_2019-11-01_1424.json\n",
      "Loading file: RSS_corpus_2019-10-11_1223.json\n",
      "Loading file: RSS_corpus_2019-09-10_0020.json\n",
      "Loading file: RSS_corpus_2019-10-07_0742.json\n",
      "Loading file: RSS_corpus_2019-09-08_1222.json\n",
      "Loading file: RSS_corpus_2019-10-22_1222.json\n",
      "Loading file: RSS_corpus_2019-11-18_0022.json\n",
      "Loading file: RSS_corpus_2019-11-23_1223.json\n",
      "Loading file: RSS_corpus_2019-10-14_0906.json\n",
      "Loading file: RSS_corpus_2019-09-13_0020.json\n",
      "Loading file: RSS_corpus_2019-09-11_0020.json\n",
      "Loading file: RSS_corpus_2019-11-03_2202.json\n",
      "Loading file: RSS_corpus_2019-11-06_0022.json\n",
      "Loading file: RSS_corpus_2019-10-08_0021.json\n",
      "Loading file: RSS_corpus_2019-10-17_0022.json\n",
      "Loading file: RSS_corpus_2019-10-14_2052.json\n",
      "Loading file: RSS_corpus_2019-11-11_1221.json\n",
      "Loading file: RSS_corpus_2019-11-11_0022.json\n",
      "Loading file: RSS_corpus_2019-09-30_0020.json\n",
      "Loading file: RSS_corpus_2019-09-30_1223.json\n",
      "Loading file: RSS_corpus_2019-12-01_0022.json\n",
      "Loading file: RSS_corpus_2019-11-17_0021.json\n",
      "Loading file: RSS_corpus_2019-12-04_0022.json\n",
      "Loading file: RSS_corpus_2019-12-28_1809.json\n",
      "Loading file: RSS_corpus_2019-10-25_0022.json\n",
      "Loading file: RSS_corpus_2019-09-19_1222.json\n",
      "Loading file: RSS_corpus_2019-10-27_1222.json\n",
      "Loading file: RSS_corpus_2019-10-14_2041.json\n",
      "Loading file: RSS_corpus_2019-10-29_0022.json\n",
      "Loading file: RSS_corpus_2019-09-26_1223.json\n",
      "Loading file: RSS_corpus_2019-12-12_2325.json\n",
      "Loading file: RSS_corpus_2019-12-23_1032.json\n",
      "Loading file: RSS_corpus_2019-11-24_1223.json\n",
      "Loading file: RSS_corpus_2019-09-28_0021.json\n",
      "Loading file: RSS_corpus_2019-10-21_0022.json\n",
      "Loading file: RSS_corpus_2019-12-08_1222.json\n",
      "Loading file: RSS_corpus_2019-10-09_0021.json\n",
      "Loading file: RSS_corpus_2019-10-10_1223.json\n",
      "Loading file: RSS_corpus_2019-10-28_1222.json\n",
      "Loading file: RSS_corpus_2019-11-25_1224.json\n",
      "Loading file: RSS_corpus_2019-10-03_0022.json\n",
      "Loading file: RSS_corpus_2019-12-14_0023.json\n",
      "Loading file: RSS_corpus_2019-11-13_0853.json\n",
      "Loading file: RSS_corpus_2019-09-07_0020.json\n",
      "Loading file: RSS_corpus_2019-10-02_1223.json\n",
      "Loading file: RSS_corpus_2019-11-07_1221.json\n",
      "Loading file: RSS_corpus_2019-09-11_1222.json\n",
      "Loading file: RSS_corpus_2019-11-17_1956.json\n",
      "Loading file: RSS_corpus_2019-10-02_0020.json\n",
      "Loading file: RSS_corpus_2019-09-15_2100.json\n",
      "Loading file: RSS_corpus_2019-10-04_1223.json\n",
      "Loading file: RSS_corpus_2019-09-06_1222.json\n",
      "Loading file: RSS_corpus_2019-10-12_0021.json\n",
      "Loading file: RSS_corpus_2019-11-29_1736.json\n",
      "Loading file: RSS_corpus_2019-12-03_1222.json\n",
      "Loading file: RSS_corpus_2019-12-11_0022.json\n",
      "Loading file: RSS_corpus_2019-11-07_0021.json\n",
      "Loading file: RSS_corpus_2019-11-16_0022.json\n",
      "Loading file: RSS_corpus_2019-09-25_1223.json\n",
      "Loading file: RSS_corpus_2019-12-11_1222.json\n",
      "Loading file: RSS_corpus_2019-12-06_1222.json\n",
      "Loading file: RSS_corpus_2019-09-18_0020.json\n",
      "Loading file: RSS_corpus_2019-12-17_1222.json\n",
      "Loading file: RSS_corpus_2019-11-12_1222.json\n",
      "Loading file: RSS_corpus_2019-10-16_1222.json\n",
      "Loading file: RSS_corpus_2019-09-27_1223.json\n",
      "Loading file: RSS_corpus_2019-12-03_0022.json\n",
      "Loading file: RSS_corpus_2019-10-18_0022.json\n",
      "Loading file: RSS_corpus_2019-10-30_1221.json\n",
      "Loading file: RSS_corpus_2019-10-10_0021.json\n",
      "Loading file: RSS_corpus_2019-12-02_1232.json\n",
      "Loading file: RSS_corpus_2019-10-20_0022.json\n",
      "Loading file: RSS_corpus_2019-12-27_1804.json\n",
      "Loading file: RSS_corpus_2019-11-24_1635.json\n",
      "Loading file: RSS_corpus_2019-09-16_1222.json\n",
      "Loading file: RSS_corpus_2019-09-13_1222.json\n",
      "Loading file: RSS_corpus_2019-12-24_0021.json\n",
      "Loading file: RSS_corpus_2019-12-07_1222.json\n",
      "Loading file: RSS_corpus_2019-11-17_1221.json\n",
      "Loading file: RSS_corpus_2019-10-04_0021.json\n",
      "Loading file: RSS_corpus_2019-09-08_0020.json\n",
      "Loading file: RSS_corpus_2019-11-04_1221.json\n",
      "Loading file: RSS_corpus_2019-12-10_0022.json\n",
      "Loading file: RSS_corpus_2019-12-12_0022.json\n",
      "Loading file: RSS_corpus_2019-12-01_1222.json\n",
      "Loading file: RSS_corpus_2019-10-01_0020.json\n",
      "Loading file: RSS_corpus_2019-09-05_2136.json\n",
      "Loading file: RSS_corpus_2019-10-15_1222.json\n",
      "Loading file: RSS_corpus_2019-12-19_2328.json\n",
      "Loading file: RSS_corpus_2019-10-29_1221.json\n",
      "Loading file: RSS_corpus_2019-12-12_1222.json\n",
      "Loading file: RSS_corpus_2019-11-24_0024.json\n",
      "Loading file: RSS_corpus_2019-09-07_1222.json\n",
      "Loading file: RSS_corpus_2019-12-24_1221.json\n",
      "Loading file: RSS_corpus_2019-09-28_1225.json\n",
      "Loading file: RSS_corpus_2019-09-22_0020.json\n",
      "Loading file: RSS_corpus_2019-09-18_1222.json\n",
      "Loading file: RSS_corpus_2019-11-02_1018.json\n",
      "Loading file: RSS_corpus_2019-12-15_1222.json\n",
      "Loading file: RSS_corpus_2019-10-16_0807.json\n",
      "Loading file: RSS_corpus_2019-09-19_0020.json\n",
      "Loading file: RSS_corpus_2019-12-13_1222.json\n",
      "Loading file: RSS_corpus_2019-10-31_1222.json\n",
      "Loading file: RSS_corpus_2019-09-09_0020.json\n",
      "Loading file: RSS_corpus_2019-09-29_0021.json\n",
      "Loading file: RSS_corpus_2019-09-23_1222.json\n",
      "Loading file: RSS_corpus_2019-10-19_0024.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: RSS_corpus_2019-10-03_1223.json\n",
      "Loading file: RSS_corpus_2019-12-06_0022.json\n",
      "Loading file: RSS_corpus_2019-09-17_1222.json\n",
      "Loading file: RSS_corpus_2019-10-08_1223.json\n",
      "Loading file: RSS_corpus_2019-10-22_0021.json\n",
      "Loading file: RSS_corpus_2019-10-24_0022.json\n",
      "Loading file: RSS_corpus_2019-12-07_0023.json\n",
      "Loading file: RSS_corpus_2019-10-07_0021.json\n",
      "Loading file: RSS_corpus_2019-12-05_0022.json\n",
      "Loading file: RSS_corpus_2019-10-12_1223.json\n",
      "Loading file: RSS_corpus_2019-10-31_0648.json\n",
      "Loading file: RSS_corpus_2019-09-29_1224.json\n",
      "Loading file: RSS_corpus_2019-09-21_0020.json\n",
      "Loading file: RSS_corpus_2019-09-24_0020.json\n",
      "Loading file: RSS_corpus_2019-10-23_1221.json\n",
      "Loading file: RSS_corpus_2019-10-23_0022.json\n",
      "Loading file: RSS_corpus_2019-11-02_1222.json\n",
      "Loading file: RSS_corpus_2019-10-24_1222.json\n",
      "Loading file: RSS_corpus_2019-12-09_1222.json\n",
      "Loading file: RSS_corpus_2019-10-19_1222.json\n",
      "Loading file: RSS_corpus_2019-11-16_1222.json\n",
      "Loading file: RSS_corpus_2019-09-14_0020.json\n",
      "Loading file: RSS_corpus_2019-12-04_1222.json\n",
      "Loading file: RSS_corpus_2019-12-10_1222.json\n",
      "Loading file: RSS_corpus_2019-12-18_0022.json\n"
     ]
    }
   ],
   "source": [
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump impeachment: House votes to formalise in...</td>\n",
       "      <td>The Democratic-controlled chamber approves a r...</td>\n",
       "      <td>Thu, 31 Oct 2019 20:21:13 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-us-canada-502...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-11-01 00:21:58.608417</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Trump impeachment: House votes to formalise in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five men acquitted of gang-raping teenager in ...</td>\n",
       "      <td>A court ruled the men did not commit rape beca...</td>\n",
       "      <td>Thu, 31 Oct 2019 23:23:02 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-50257922</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-11-01 00:21:58.608436</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Five men acquitted of gang-raping teenager in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brazil wildfires: Blaze advances across Pantan...</td>\n",
       "      <td>The area is one of the most biodiverse regions...</td>\n",
       "      <td>Fri, 01 Nov 2019 00:11:01 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-latin-america...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-11-01 00:21:58.608446</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Brazil wildfires: Blaze advances across Pantan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Islamic State group names its new leader as Ab...</td>\n",
       "      <td>The jihadist group names Abu Ibrahim al-Hashem...</td>\n",
       "      <td>Thu, 31 Oct 2019 19:03:25 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-middle-east-5...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-11-01 00:21:58.608455</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Islamic State group names its new leader as Ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iraq protests: How tuk-tuks are saving lives i...</td>\n",
       "      <td>From a nuisance to a necessity, tuk-tuks have ...</td>\n",
       "      <td>Thu, 31 Oct 2019 19:11:51 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-middle-east-5...</td>\n",
       "      <td>http://feeds.bbci.co.uk/news/world/rss.xml</td>\n",
       "      <td>2019-11-01 00:21:58.608464</td>\n",
       "      <td>rss_feed</td>\n",
       "      <td>Iraq protests: How tuk-tuks are saving lives i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Trump impeachment: House votes to formalise in...   \n",
       "1  Five men acquitted of gang-raping teenager in ...   \n",
       "2  Brazil wildfires: Blaze advances across Pantan...   \n",
       "3  Islamic State group names its new leader as Ab...   \n",
       "4  Iraq protests: How tuk-tuks are saving lives i...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The Democratic-controlled chamber approves a r...   \n",
       "1  A court ruled the men did not commit rape beca...   \n",
       "2  The area is one of the most biodiverse regions...   \n",
       "3  The jihadist group names Abu Ibrahim al-Hashem...   \n",
       "4  From a nuisance to a necessity, tuk-tuks have ...   \n",
       "\n",
       "                            date  \\\n",
       "0  Thu, 31 Oct 2019 20:21:13 GMT   \n",
       "1  Thu, 31 Oct 2019 23:23:02 GMT   \n",
       "2  Fri, 01 Nov 2019 00:11:01 GMT   \n",
       "3  Thu, 31 Oct 2019 19:03:25 GMT   \n",
       "4  Thu, 31 Oct 2019 19:11:51 GMT   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.bbc.co.uk/news/world-us-canada-502...   \n",
       "1   https://www.bbc.co.uk/news/world-europe-50257922   \n",
       "2  https://www.bbc.co.uk/news/world-latin-america...   \n",
       "3  https://www.bbc.co.uk/news/world-middle-east-5...   \n",
       "4  https://www.bbc.co.uk/news/world-middle-east-5...   \n",
       "\n",
       "                                   source_url         retrieval_timestamp  \\\n",
       "0  http://feeds.bbci.co.uk/news/world/rss.xml  2019-11-01 00:21:58.608417   \n",
       "1  http://feeds.bbci.co.uk/news/world/rss.xml  2019-11-01 00:21:58.608436   \n",
       "2  http://feeds.bbci.co.uk/news/world/rss.xml  2019-11-01 00:21:58.608446   \n",
       "3  http://feeds.bbci.co.uk/news/world/rss.xml  2019-11-01 00:21:58.608455   \n",
       "4  http://feeds.bbci.co.uk/news/world/rss.xml  2019-11-01 00:21:58.608464   \n",
       "\n",
       "     origin                                         clean_text  \n",
       "0  rss_feed  Trump impeachment: House votes to formalise in...  \n",
       "1  rss_feed  Five men acquitted of gang-raping teenager in ...  \n",
       "2  rss_feed  Brazil wildfires: Blaze advances across Pantan...  \n",
       "3  rss_feed  Islamic State group names its new leader as Ab...  \n",
       "4  rss_feed  Iraq protests: How tuk-tuks are saving lives i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50679, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering using Entity Detection And Network Analytics\n",
    "\n",
    "This doesn't resolve very well for Bing, because there's a whole bunch of keywords from the original searches in there.  Suspect that's got a lot to do with the failure of the other methods too.  For the network analytics method I'm going to try removing the keywords from the table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/home/ozwald/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "#    scrape_config = json.load(f)\n",
    "#\n",
    "#search_terms = scrape_config['disaster_search_list']\n",
    "#search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "#search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARALLEL AWESOMENESS!!!\n",
      "found all nouns\n",
      "Reduced noun lists to sets\n"
     ]
    }
   ],
   "source": [
    "model = reps.NounAdjacencyModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "model.noun_sets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 500 most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "#nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only top 500 most common\n",
    "nouns_to_keep = nouns_to_keep[:500]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 3\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#G_plot = nx.petersen_graph()\n",
    "#plt.subplot(121)\n",
    "#nx.draw(G, with_labels=True, font_weight='bold')\n",
    "#plt.subplot(122)\n",
    "#nx.draw_shell(G, nlist=[range(5, 10), range(5)], with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliques, worth a look?\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "\n",
    "So, cliques are allowed to overlap - should've thought of that.  Still, good preliminary results and I've found I can disambiguate the cliques to some degree by cutting out weaker links (fewer shared entities).\n",
    "\n",
    "I should add it also appears to merely suffer from the same problems as the other clustering methods, clusters are ultimately hierarchical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques = []\n",
    "for x in nx.find_cliques(G):\n",
    "    x.sort()\n",
    "    cliques.append((len(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cliques_df[cliques_df['clique_size'] >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cliques_df[cliques_df['clique_size'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[0]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[1]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[3]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[17]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [component for component in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum([len(component) for component in components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain Community Detection\n",
    "# The keys are nodes, the values are the partitions they belong to\n",
    "partition = best_partition(G)\n",
    "\n",
    "number_partitions = max(partition.values())\n",
    "number_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through and get a list of partitions and their nodes\n",
    "partition_contents = {}\n",
    "for key in partition.keys():\n",
    "    partition_contents[partition[key]] = partition_contents.get(partition[key], []) + [key]\n",
    "\n",
    "# Drop partitions that are too small\n",
    "for key in list(partition_contents.keys()):\n",
    "    if len(partition_contents[key]) < 5:\n",
    "        partition_contents.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big our \"clusters\" are, and how many there are total after removing the tiny ones\n",
    "partition_lengths = [len(value) for key, value in partition_contents.items()]\n",
    "print(partition_lengths, sum(partition_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_contents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in partition_contents[2]:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in partition_contents[9]:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[15]:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
