{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"/home/ozwald/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"disaster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, get a list of all the news dumps created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 77\n",
      "Loading file: bing_disaster_corpus_2019-12-17_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-11_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-27_1803.json\n",
      "Loading file: bing_disaster_corpus_2019-12-19_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-08_0023.json\n",
      "Loading file: bing_disaster_corpus_2019-11-17_1952.json\n",
      "Loading file: bing_disaster_corpus_2020-01-20_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-25_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-18_1658.json\n",
      "Loading file: bing_disaster_corpus_2019-12-01_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-06_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-19_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-17_1956.json\n",
      "Loading file: bing_disaster_corpus_2020-01-09_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-18_1222.json\n",
      "Loading file: bing_disaster_corpus_2020-01-08_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-09_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-04_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-23_1223.json\n",
      "Loading file: bing_disaster_corpus_2019-12-03_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-30_1222.json\n",
      "Loading file: bing_disaster_corpus_2020-01-10_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-10_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-13_0856.json\n",
      "Loading file: bing_disaster_corpus_2020-01-21_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-05_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-12_1558.json\n",
      "Loading file: bing_disaster_corpus_2019-12-23_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-18_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-18_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-11_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-10_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-03_1222.json\n",
      "Loading file: bing_disaster_corpus_2020-01-11_1212.json\n",
      "Loading file: bing_disaster_corpus_2020-01-19_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-12_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-20_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-21_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-16_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-15_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-19_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-19_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_2324.json\n",
      "Loading file: bing_disaster_corpus_2019-12-15_1222.json\n",
      "Loading file: bing_disaster_corpus_2020-01-14_2037.json\n",
      "Loading file: bing_disaster_corpus_2019-12-20_1720.json\n",
      "Loading file: bing_disaster_corpus_2019-11-22_1540.json\n",
      "Loading file: bing_disaster_corpus_2019-12-08_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-28_1803.json\n",
      "Loading file: bing_disaster_corpus_2020-01-17_1648.json\n",
      "Loading file: bing_disaster_corpus_2019-12-06_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-29_1735.json\n",
      "Loading file: bing_disaster_corpus_2019-12-16_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-07_2208.json\n",
      "Loading file: bing_disaster_corpus_2019-12-18_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-14_1222.json\n",
      "Loading file: bing_disaster_corpus_2020-01-18_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-10_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-01_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-19_2327.json\n",
      "Loading file: bing_disaster_corpus_2019-12-07_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-07_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-23_0022.json\n",
      "Loading file: bing_disaster_corpus_2020-01-08_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-25_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-09_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-30_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-09_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-17_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-13_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-05_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-11_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-23_1031.json\n",
      "Loading file: bing_disaster_corpus_2019-12-04_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-02_0022.json\n"
     ]
    }
   ],
   "source": [
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Usman vs Covington live stream: Free links to ...</td>\n",
       "      <td>Colby Covington is set to take on Kamaru Usman...</td>\n",
       "      <td>2019-12-17T00:02:00.0000000Z</td>\n",
       "      <td>https://www.independent.co.uk/sport/general/mm...</td>\n",
       "      <td>www.independent.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.786528</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Usman vs Covington live stream: Free links to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; warnings in place across Berkshir...</td>\n",
       "      <td>A number of &lt;b&gt;flood&lt;/b&gt; warnings are in place...</td>\n",
       "      <td>2019-12-16T13:26:00.0000000Z</td>\n",
       "      <td>https://www.getreading.co.uk/news/reading-berk...</td>\n",
       "      <td>www.getreading.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787132</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood warnings in place across Berkshire after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; alert issued for Burton after rai...</td>\n",
       "      <td>People across Burton and South Derbyshire are ...</td>\n",
       "      <td>2019-12-16T09:17:00.0000000Z</td>\n",
       "      <td>https://www.derbytelegraph.co.uk/burton/flood-...</td>\n",
       "      <td>www.derbytelegraph.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787399</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood alert issued for Burton after rainfall. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Swindon &lt;b&gt;flood&lt;/b&gt; defence pond overflows an...</td>\n",
       "      <td>Pavements and a park have been left flooded du...</td>\n",
       "      <td>2019-12-16T17:57:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-wiltshir...</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787503</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Swindon flood defence pond overflows and cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; Warnings issued for River Severn ...</td>\n",
       "      <td>Heavy rain has seen the Environment Agency put...</td>\n",
       "      <td>2019-12-16T11:57:00.0000000Z</td>\n",
       "      <td>https://www.gloucestershirelive.co.uk/news/glo...</td>\n",
       "      <td>www.gloucestershirelive.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787551</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood Warnings issued for River Severn as Envi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Usman vs Covington live stream: Free links to ...   \n",
       "6   <b>Flood</b> warnings in place across Berkshir...   \n",
       "11  <b>Flood</b> alert issued for Burton after rai...   \n",
       "13  Swindon <b>flood</b> defence pond overflows an...   \n",
       "14  <b>Flood</b> Warnings issued for River Severn ...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Colby Covington is set to take on Kamaru Usman...   \n",
       "6   A number of <b>flood</b> warnings are in place...   \n",
       "11  People across Burton and South Derbyshire are ...   \n",
       "13  Pavements and a park have been left flooded du...   \n",
       "14  Heavy rain has seen the Environment Agency put...   \n",
       "\n",
       "                            date  \\\n",
       "0   2019-12-17T00:02:00.0000000Z   \n",
       "6   2019-12-16T13:26:00.0000000Z   \n",
       "11  2019-12-16T09:17:00.0000000Z   \n",
       "13  2019-12-16T17:57:00.0000000Z   \n",
       "14  2019-12-16T11:57:00.0000000Z   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://www.independent.co.uk/sport/general/mm...   \n",
       "6   https://www.getreading.co.uk/news/reading-berk...   \n",
       "11  https://www.derbytelegraph.co.uk/burton/flood-...   \n",
       "13  https://www.bbc.co.uk/news/uk-england-wiltshir...   \n",
       "14  https://www.gloucestershirelive.co.uk/news/glo...   \n",
       "\n",
       "                       source_url         retrieval_timestamp         origin  \\\n",
       "0           www.independent.co.uk  2019-12-17 00:21:31.786528  bing_news_api   \n",
       "6            www.getreading.co.uk  2019-12-17 00:21:31.787132  bing_news_api   \n",
       "11       www.derbytelegraph.co.uk  2019-12-17 00:21:31.787399  bing_news_api   \n",
       "13                  www.bbc.co.uk  2019-12-17 00:21:31.787503  bing_news_api   \n",
       "14  www.gloucestershirelive.co.uk  2019-12-17 00:21:31.787551  bing_news_api   \n",
       "\n",
       "                                           clean_text  \n",
       "0   Usman vs Covington live stream: Free links to ...  \n",
       "6   Flood warnings in place across Berkshire after...  \n",
       "11  Flood alert issued for Burton after rainfall. ...  \n",
       "13  Swindon flood defence pond overflows and cause...  \n",
       "14  Flood Warnings issued for River Severn as Envi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.iloc[-5000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4454, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering using Entity Detection And Network Analytics\n",
    "\n",
    "This doesn't resolve very well for Bing, because there's a whole bunch of keywords from the original searches in there.  Suspect that's got a lot to do with the failure of the other methods too.  For the network analytics method I'm going to try removing the keywords from the table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/home/ozwald/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "#    scrape_config = json.load(f)\n",
    "#\n",
    "#search_terms = scrape_config['disaster_search_list']\n",
    "#search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "#search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARALLEL AWESOMENESS!!!\n",
      "Is this where ram useage goes off the charts?\n",
      "found all nouns\n",
      "Reduced noun lists to sets\n",
      "Created entity table\n",
      "Aggregated result table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Council',\n",
       " 'Fields',\n",
       " 'Margaret',\n",
       " 'Merton',\n",
       " 'Parish',\n",
       " 'St',\n",
       " 'Stratton',\n",
       " 'Swindon'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = reps.NounAdjacencyModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "model.noun_sets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Water</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Langer</th>\n",
       "      <th>Winston</th>\n",
       "      <th>research</th>\n",
       "      <th>Bangladesh</th>\n",
       "      <th>Haiti</th>\n",
       "      <th>Venice‘s</th>\n",
       "      <th>Bowen</th>\n",
       "      <th>NBA</th>\n",
       "      <th>...</th>\n",
       "      <th>Croft</th>\n",
       "      <th>Shirley</th>\n",
       "      <th>Raze</th>\n",
       "      <th>Elton_John</th>\n",
       "      <th>Ulster</th>\n",
       "      <th>Fairy</th>\n",
       "      <th>quot;lows</th>\n",
       "      <th>1C</th>\n",
       "      <th>Alec</th>\n",
       "      <th>ITF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Usman vs Covington live stream: Free links to watch UFC 245 flood online as piracy hits ;peak levels;. Colby Covington is set to take on Kamaru Usman at the main event of UFC 245, with the Welterweight title on the line. Unusually, UFC has also put two other title fights on a single pay-per-view in the US, as Max Holloway faces Alexander Volkanovski and Amanda Nunes squares off against Germaine de Randamie. Fight fans will be able to watch the ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood warnings in place across Berkshire after heavy downpours. A number of flood warnings are in place across Berkshire after heavy downpours at the weekend. People have been warned flooding is possible near many of the county;s rivers and those living or working near by have been urged to be prepared. The flood information service on gov.uk has issued a number of flood alerts and experts are monitoring ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood alert issued for Burton after rainfall. People across Burton and South Derbyshire are being warned to quot;be preparedquot; as a flood alert has been issued across the area. Persistent rainfall has fallen across the last seven days, leading to the warning on the Government website, gov.uk. The flood alert for Burton was issued on the last night, Sunday, December 15, and is still in place today.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swindon flood defence pond overflows and causes flooding. Pavements and a park have been left flooded during work to install new drainage. The work, at Merton Fields in Swindon, was for an attenuation pond to divert water during heavy rainfall. However, it immediately overflowed in heavy rain and residents reported water pouring under fences and into gardens. Stratton St Margaret Parish Council ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood Warnings issued for River Severn as Environment Agency says immediate action required. Heavy rain has seen the Environment Agency put Flood Warnings on stretches of the River Severn in Gloucestershire. It says river levels are expected to remain high until Wednesday and flooding is expected and immediate action required. The Flood Warnings are in place for the River Severn at Apperley and The Leigh and on the River Severn at ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Water  Korea  Langer  \\\n",
       "Usman vs Covington live stream: Free links to w...      0      0       0   \n",
       "Flood warnings in place across Berkshire after ...      0      0       0   \n",
       "Flood alert issued for Burton after rainfall. P...      0      0       0   \n",
       "Swindon flood defence pond overflows and causes...      0      0       0   \n",
       "Flood Warnings issued for River Severn as Envir...      0      0       0   \n",
       "\n",
       "                                                    Winston  research  \\\n",
       "Usman vs Covington live stream: Free links to w...        0         0   \n",
       "Flood warnings in place across Berkshire after ...        0         0   \n",
       "Flood alert issued for Burton after rainfall. P...        0         0   \n",
       "Swindon flood defence pond overflows and causes...        0         0   \n",
       "Flood Warnings issued for River Severn as Envir...        0         0   \n",
       "\n",
       "                                                    Bangladesh  Haiti  \\\n",
       "Usman vs Covington live stream: Free links to w...           0      0   \n",
       "Flood warnings in place across Berkshire after ...           0      0   \n",
       "Flood alert issued for Burton after rainfall. P...           0      0   \n",
       "Swindon flood defence pond overflows and causes...           0      0   \n",
       "Flood Warnings issued for River Severn as Envir...           0      0   \n",
       "\n",
       "                                                    Venice‘s  Bowen  NBA  ...  \\\n",
       "Usman vs Covington live stream: Free links to w...         0      0    0  ...   \n",
       "Flood warnings in place across Berkshire after ...         0      0    0  ...   \n",
       "Flood alert issued for Burton after rainfall. P...         0      0    0  ...   \n",
       "Swindon flood defence pond overflows and causes...         0      0    0  ...   \n",
       "Flood Warnings issued for River Severn as Envir...         0      0    0  ...   \n",
       "\n",
       "                                                    Croft  Shirley  Raze  \\\n",
       "Usman vs Covington live stream: Free links to w...      0        0     0   \n",
       "Flood warnings in place across Berkshire after ...      0        0     0   \n",
       "Flood alert issued for Burton after rainfall. P...      0        0     0   \n",
       "Swindon flood defence pond overflows and causes...      0        0     0   \n",
       "Flood Warnings issued for River Severn as Envir...      0        0     0   \n",
       "\n",
       "                                                    Elton_John  Ulster  Fairy  \\\n",
       "Usman vs Covington live stream: Free links to w...           0       0      0   \n",
       "Flood warnings in place across Berkshire after ...           0       0      0   \n",
       "Flood alert issued for Burton after rainfall. P...           0       0      0   \n",
       "Swindon flood defence pond overflows and causes...           0       0      0   \n",
       "Flood Warnings issued for River Severn as Envir...           0       0      0   \n",
       "\n",
       "                                                    quot;lows  1C  Alec  ITF  \n",
       "Usman vs Covington live stream: Free links to w...          0   0     0    0  \n",
       "Flood warnings in place across Berkshire after ...          0   0     0    0  \n",
       "Flood alert issued for Burton after rainfall. P...          0   0     0    0  \n",
       "Swindon flood defence pond overflows and causes...          0   0     0    0  \n",
       "Flood Warnings issued for River Severn as Envir...          0   0     0    0  \n",
       "\n",
       "[5 rows x 7236 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 500 most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "#nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only top 500 most common\n",
    "nouns_to_keep = nouns_to_keep[:500]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 3\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#G_plot = nx.petersen_graph()\n",
    "#plt.subplot(121)\n",
    "#nx.draw(G, with_labels=True, font_weight='bold')\n",
    "#plt.subplot(122)\n",
    "#nx.draw_shell(G, nlist=[range(5, 10), range(5)], with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliques, worth a look?\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "\n",
    "So, cliques are allowed to overlap - should've thought of that.  Still, good preliminary results and I've found I can disambiguate the cliques to some degree by cutting out weaker links (fewer shared entities).\n",
    "\n",
    "I should add it also appears to merely suffer from the same problems as the other clustering methods, clusters are ultimately hierarchical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques = []\n",
    "for x in nx.find_cliques(G):\n",
    "    x.sort()\n",
    "    cliques.append((len(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cliques_df[cliques_df['clique_size'] >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cliques_df[cliques_df['clique_size'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[0]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[1]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[3]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[17]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [component for component in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum([len(component) for component in components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain Community Detection\n",
    "# The keys are nodes, the values are the partitions they belong to\n",
    "partition = best_partition(G)\n",
    "\n",
    "number_partitions = max(partition.values())\n",
    "number_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through and get a list of partitions and their nodes\n",
    "partition_contents = {}\n",
    "for key in partition.keys():\n",
    "    partition_contents[partition[key]] = partition_contents.get(partition[key], []) + [key]\n",
    "\n",
    "# Drop partitions that are too small\n",
    "for key in list(partition_contents.keys()):\n",
    "    if len(partition_contents[key]) < 5:\n",
    "        partition_contents.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big our \"clusters\" are, and how many there are total after removing the tiny ones\n",
    "partition_lengths = {key:len(value) for key, value in partition_contents.items()}\n",
    "print(partition_lengths, sum(partition_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[2][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[5][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[10][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[14][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
