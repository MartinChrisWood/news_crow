{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"/home/ozwald/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"disaster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, get a list of all the news dumps created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 56\n",
      "Loading file: bing_disaster_corpus_2019-12-17_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-27_1803.json\n",
      "Loading file: bing_disaster_corpus_2019-12-19_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-08_0023.json\n",
      "Loading file: bing_disaster_corpus_2019-11-17_1952.json\n",
      "Loading file: bing_disaster_corpus_2019-11-25_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-01_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-06_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-19_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-17_1956.json\n",
      "Loading file: bing_disaster_corpus_2019-12-18_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-04_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-23_1223.json\n",
      "Loading file: bing_disaster_corpus_2019-12-03_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-30_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-10_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-13_0856.json\n",
      "Loading file: bing_disaster_corpus_2019-12-05_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-23_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-18_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-11-18_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-11_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-03_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-16_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-15_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-19_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_2324.json\n",
      "Loading file: bing_disaster_corpus_2019-12-15_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-20_1720.json\n",
      "Loading file: bing_disaster_corpus_2019-11-22_1540.json\n",
      "Loading file: bing_disaster_corpus_2019-12-08_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-28_1803.json\n",
      "Loading file: bing_disaster_corpus_2019-12-06_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-29_1735.json\n",
      "Loading file: bing_disaster_corpus_2019-12-16_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-18_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-14_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-10_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-01_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-19_2327.json\n",
      "Loading file: bing_disaster_corpus_2019-12-07_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-07_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-23_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-25_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-12_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-09_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-11-30_0022.json\n",
      "Loading file: bing_disaster_corpus_2019-12-09_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-17_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-13_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-05_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-11_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-23_1031.json\n",
      "Loading file: bing_disaster_corpus_2019-12-04_1222.json\n",
      "Loading file: bing_disaster_corpus_2019-12-02_0022.json\n"
     ]
    }
   ],
   "source": [
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Usman vs Covington live stream: Free links to ...</td>\n",
       "      <td>Colby Covington is set to take on Kamaru Usman...</td>\n",
       "      <td>2019-12-17T00:02:00.0000000Z</td>\n",
       "      <td>https://www.independent.co.uk/sport/general/mm...</td>\n",
       "      <td>www.independent.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.786528</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Usman vs Covington live stream: Free links to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; warnings in place across Berkshir...</td>\n",
       "      <td>A number of &lt;b&gt;flood&lt;/b&gt; warnings are in place...</td>\n",
       "      <td>2019-12-16T13:26:00.0000000Z</td>\n",
       "      <td>https://www.getreading.co.uk/news/reading-berk...</td>\n",
       "      <td>www.getreading.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787132</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood warnings in place across Berkshire after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; alert issued for Burton after rai...</td>\n",
       "      <td>People across Burton and South Derbyshire are ...</td>\n",
       "      <td>2019-12-16T09:17:00.0000000Z</td>\n",
       "      <td>https://www.derbytelegraph.co.uk/burton/flood-...</td>\n",
       "      <td>www.derbytelegraph.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787399</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood alert issued for Burton after rainfall. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Swindon &lt;b&gt;flood&lt;/b&gt; defence pond overflows an...</td>\n",
       "      <td>Pavements and a park have been left flooded du...</td>\n",
       "      <td>2019-12-16T17:57:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-wiltshir...</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787503</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Swindon flood defence pond overflows and cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;b&gt;Flood&lt;/b&gt; Warnings issued for River Severn ...</td>\n",
       "      <td>Heavy rain has seen the Environment Agency put...</td>\n",
       "      <td>2019-12-16T11:57:00.0000000Z</td>\n",
       "      <td>https://www.gloucestershirelive.co.uk/news/glo...</td>\n",
       "      <td>www.gloucestershirelive.co.uk</td>\n",
       "      <td>2019-12-17 00:21:31.787551</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>Flood Warnings issued for River Severn as Envi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Usman vs Covington live stream: Free links to ...   \n",
       "6   <b>Flood</b> warnings in place across Berkshir...   \n",
       "11  <b>Flood</b> alert issued for Burton after rai...   \n",
       "13  Swindon <b>flood</b> defence pond overflows an...   \n",
       "14  <b>Flood</b> Warnings issued for River Severn ...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Colby Covington is set to take on Kamaru Usman...   \n",
       "6   A number of <b>flood</b> warnings are in place...   \n",
       "11  People across Burton and South Derbyshire are ...   \n",
       "13  Pavements and a park have been left flooded du...   \n",
       "14  Heavy rain has seen the Environment Agency put...   \n",
       "\n",
       "                            date  \\\n",
       "0   2019-12-17T00:02:00.0000000Z   \n",
       "6   2019-12-16T13:26:00.0000000Z   \n",
       "11  2019-12-16T09:17:00.0000000Z   \n",
       "13  2019-12-16T17:57:00.0000000Z   \n",
       "14  2019-12-16T11:57:00.0000000Z   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://www.independent.co.uk/sport/general/mm...   \n",
       "6   https://www.getreading.co.uk/news/reading-berk...   \n",
       "11  https://www.derbytelegraph.co.uk/burton/flood-...   \n",
       "13  https://www.bbc.co.uk/news/uk-england-wiltshir...   \n",
       "14  https://www.gloucestershirelive.co.uk/news/glo...   \n",
       "\n",
       "                       source_url         retrieval_timestamp         origin  \\\n",
       "0           www.independent.co.uk  2019-12-17 00:21:31.786528  bing_news_api   \n",
       "6            www.getreading.co.uk  2019-12-17 00:21:31.787132  bing_news_api   \n",
       "11       www.derbytelegraph.co.uk  2019-12-17 00:21:31.787399  bing_news_api   \n",
       "13                  www.bbc.co.uk  2019-12-17 00:21:31.787503  bing_news_api   \n",
       "14  www.gloucestershirelive.co.uk  2019-12-17 00:21:31.787551  bing_news_api   \n",
       "\n",
       "                                           clean_text  \n",
       "0   Usman vs Covington live stream: Free links to ...  \n",
       "6   Flood warnings in place across Berkshire after...  \n",
       "11  Flood alert issued for Burton after rainfall. ...  \n",
       "13  Swindon flood defence pond overflows and cause...  \n",
       "14  Flood Warnings issued for River Severn as Envi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering using Entity Detection And Network Analytics\n",
    "\n",
    "This doesn't resolve very well for Bing, because there's a whole bunch of keywords from the original searches in there.  Suspect that's got a lot to do with the failure of the other methods too.  For the network analytics method I'm going to try removing the keywords from the table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ozwald/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "    scrape_config = json.load(f)\n",
    "\n",
    "search_terms = scrape_config['disaster_search_list']\n",
    "search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disaster',\n",
       " 'drought',\n",
       " 'droughts',\n",
       " 'flash',\n",
       " 'flood',\n",
       " 'flooding',\n",
       " 'floods',\n",
       " 'hurricane',\n",
       " 'mudslide',\n",
       " 'natural',\n",
       " 'snow',\n",
       " 'tsunami',\n",
       " 'typhoon',\n",
       " 'wildfire',\n",
       " 'wildfires'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reps.NounAdjacencyModel(corpus['clean_text'], corpus['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Council',\n",
       " 'Fields',\n",
       " 'Margaret',\n",
       " 'Merton',\n",
       " 'Parish',\n",
       " 'St',\n",
       " 'Stratton',\n",
       " 'Swindon'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.noun_sets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Littry</th>\n",
       "      <th>nac</th>\n",
       "      <th>North_Lincolnshire</th>\n",
       "      <th>Peak_District</th>\n",
       "      <th>Rickett</th>\n",
       "      <th>Secretary</th>\n",
       "      <th>Wyke</th>\n",
       "      <th>B-52</th>\n",
       "      <th>Lingard</th>\n",
       "      <th>hammer</th>\n",
       "      <th>...</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>Merthyr</th>\n",
       "      <th>Snowdonia</th>\n",
       "      <th>Burke</th>\n",
       "      <th>Greens</th>\n",
       "      <th>Glatthaar</th>\n",
       "      <th>Experimental</th>\n",
       "      <th>Barlow</th>\n",
       "      <th>Unsworth</th>\n",
       "      <th>WALMART</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Usman vs Covington live stream: Free links to watch UFC 245 flood online as piracy hits ;peak levels;. Colby Covington is set to take on Kamaru Usman at the main event of UFC 245, with the Welterweight title on the line. Unusually, UFC has also put two other title fights on a single pay-per-view in the US, as Max Holloway faces Alexander Volkanovski and Amanda Nunes squares off against Germaine de Randamie. Fight fans will be able to watch the ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood warnings in place across Berkshire after heavy downpours. A number of flood warnings are in place across Berkshire after heavy downpours at the weekend. People have been warned flooding is possible near many of the county;s rivers and those living or working near by have been urged to be prepared. The flood information service on gov.uk has issued a number of flood alerts and experts are monitoring ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood alert issued for Burton after rainfall. People across Burton and South Derbyshire are being warned to quot;be preparedquot; as a flood alert has been issued across the area. Persistent rainfall has fallen across the last seven days, leading to the warning on the Government website, gov.uk. The flood alert for Burton was issued on the last night, Sunday, December 15, and is still in place today.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swindon flood defence pond overflows and causes flooding. Pavements and a park have been left flooded during work to install new drainage. The work, at Merton Fields in Swindon, was for an attenuation pond to divert water during heavy rainfall. However, it immediately overflowed in heavy rain and residents reported water pouring under fences and into gardens. Stratton St Margaret Parish Council ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flood Warnings issued for River Severn as Environment Agency says immediate action required. Heavy rain has seen the Environment Agency put Flood Warnings on stretches of the River Severn in Gloucestershire. It says river levels are expected to remain high until Wednesday and flooding is expected and immediate action required. The Flood Warnings are in place for the River Severn at Apperley and The Leigh and on the River Severn at ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5849 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Littry  nac  \\\n",
       "clean_text                                                        \n",
       "Usman vs Covington live stream: Free links to w...       0    0   \n",
       "Flood warnings in place across Berkshire after ...       0    0   \n",
       "Flood alert issued for Burton after rainfall. P...       0    0   \n",
       "Swindon flood defence pond overflows and causes...       0    0   \n",
       "Flood Warnings issued for River Severn as Envir...       0    0   \n",
       "\n",
       "                                                    North_Lincolnshire  \\\n",
       "clean_text                                                               \n",
       "Usman vs Covington live stream: Free links to w...                   0   \n",
       "Flood warnings in place across Berkshire after ...                   0   \n",
       "Flood alert issued for Burton after rainfall. P...                   0   \n",
       "Swindon flood defence pond overflows and causes...                   0   \n",
       "Flood Warnings issued for River Severn as Envir...                   0   \n",
       "\n",
       "                                                    Peak_District  Rickett  \\\n",
       "clean_text                                                                   \n",
       "Usman vs Covington live stream: Free links to w...              0        0   \n",
       "Flood warnings in place across Berkshire after ...              0        0   \n",
       "Flood alert issued for Burton after rainfall. P...              0        0   \n",
       "Swindon flood defence pond overflows and causes...              0        0   \n",
       "Flood Warnings issued for River Severn as Envir...              0        0   \n",
       "\n",
       "                                                    Secretary  Wyke  B-52  \\\n",
       "clean_text                                                                  \n",
       "Usman vs Covington live stream: Free links to w...          0     0     0   \n",
       "Flood warnings in place across Berkshire after ...          0     0     0   \n",
       "Flood alert issued for Burton after rainfall. P...          0     0     0   \n",
       "Swindon flood defence pond overflows and causes...          0     0     0   \n",
       "Flood Warnings issued for River Severn as Envir...          0     0     0   \n",
       "\n",
       "                                                    Lingard  hammer  ...  \\\n",
       "clean_text                                                           ...   \n",
       "Usman vs Covington live stream: Free links to w...        0       0  ...   \n",
       "Flood warnings in place across Berkshire after ...        0       0  ...   \n",
       "Flood alert issued for Burton after rainfall. P...        0       0  ...   \n",
       "Swindon flood defence pond overflows and causes...        0       0  ...   \n",
       "Flood Warnings issued for River Severn as Envir...        0       0  ...   \n",
       "\n",
       "                                                    Barrier  Merthyr  \\\n",
       "clean_text                                                             \n",
       "Usman vs Covington live stream: Free links to w...        0        0   \n",
       "Flood warnings in place across Berkshire after ...        0        0   \n",
       "Flood alert issued for Burton after rainfall. P...        0        0   \n",
       "Swindon flood defence pond overflows and causes...        0        0   \n",
       "Flood Warnings issued for River Severn as Envir...        0        0   \n",
       "\n",
       "                                                    Snowdonia  Burke  Greens  \\\n",
       "clean_text                                                                     \n",
       "Usman vs Covington live stream: Free links to w...          0      0       0   \n",
       "Flood warnings in place across Berkshire after ...          0      0       0   \n",
       "Flood alert issued for Burton after rainfall. P...          0      0       0   \n",
       "Swindon flood defence pond overflows and causes...          0      0       0   \n",
       "Flood Warnings issued for River Severn as Envir...          0      0       0   \n",
       "\n",
       "                                                    Glatthaar  Experimental  \\\n",
       "clean_text                                                                    \n",
       "Usman vs Covington live stream: Free links to w...          0             0   \n",
       "Flood warnings in place across Berkshire after ...          0             0   \n",
       "Flood alert issued for Burton after rainfall. P...          0             0   \n",
       "Swindon flood defence pond overflows and causes...          0             0   \n",
       "Flood Warnings issued for River Severn as Envir...          0             0   \n",
       "\n",
       "                                                    Barlow  Unsworth  WALMART  \n",
       "clean_text                                                                     \n",
       "Usman vs Covington live stream: Free links to w...       0         0        0  \n",
       "Flood warnings in place across Berkshire after ...       0         0        0  \n",
       "Flood alert issued for Burton after rainfall. P...       0         0        0  \n",
       "Swindon flood defence pond overflows and causes...       0         0        0  \n",
       "Flood Warnings issued for River Severn as Envir...       0         0        0  \n",
       "\n",
       "[5 rows x 5849 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3348, 500)\n"
     ]
    }
   ],
   "source": [
    "# Get 500 most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only top 500 most common\n",
    "nouns_to_keep = nouns_to_keep[:500]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 3\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1873"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#G_plot = nx.petersen_graph()\n",
    "#plt.subplot(121)\n",
    "#nx.draw(G, with_labels=True, font_weight='bold')\n",
    "#plt.subplot(122)\n",
    "#nx.draw_shell(G, nlist=[range(5, 10), range(5)], with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliques, worth a look?\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "\n",
    "So, cliques are allowed to overlap - should've thought of that.  Still, good preliminary results and I've found I can disambiguate the cliques to some degree by cutting out weaker links (fewer shared entities).\n",
    "\n",
    "I should add it also appears to merely suffer from the same problems as the other clustering methods, clusters are ultimately hierarchical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques = []\n",
    "for x in nx.find_cliques(G):\n",
    "    x.sort()\n",
    "    cliques.append((len(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cliques_df[cliques_df['clique_size'] >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>nodes_list</th>\n",
       "      <th>clique_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247</td>\n",
       "      <td>[339, 492, 869, 1104, 2356, 2359, 2458, 2461, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>[167, 687, 911, 914, 916, 917, 1128, 1700, 196...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885</td>\n",
       "      <td>[546, 1006, 1362, 1464, 1605, 1791, 2541, 2613...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>[141, 145, 146, 151, 672, 1321, 1422, 1423, 2024]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>673</td>\n",
       "      <td>[1214, 1271, 1495, 1639, 1996, 2230, 2233]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>[28, 354, 1281, 1730, 2458, 2657, 3319]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>139</td>\n",
       "      <td>[211, 596, 1568, 2112, 2378, 2492, 2686]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>533</td>\n",
       "      <td>[911, 912, 916, 917, 1128, 1700, 1961]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>886</td>\n",
       "      <td>[546, 1006, 1362, 1605, 1791, 2588, 2613]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>83</td>\n",
       "      <td>[141, 145, 672, 1320, 1321, 1422, 2024]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>87</td>\n",
       "      <td>[144, 148, 150, 680, 2187, 2238, 2239]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>[32, 1214, 1271, 1495, 1996, 2230, 2233]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>331</td>\n",
       "      <td>[512, 961, 1786, 2698, 2783, 3052, 3240]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>358</td>\n",
       "      <td>[561, 1005, 2075, 2616, 2755, 3091, 3264]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>276</td>\n",
       "      <td>[395, 687, 911, 917, 1128, 1961]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>476</td>\n",
       "      <td>[790, 1487, 1630, 1639, 2105, 2230]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>350</td>\n",
       "      <td>[545, 1394, 1651, 1888, 2098, 2810]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>284</td>\n",
       "      <td>[413, 1007, 1612, 2149, 2552, 3102]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>118</td>\n",
       "      <td>[181, 1343, 2083, 2133, 2246, 3087]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>404</td>\n",
       "      <td>[641, 646, 1080, 1289, 2122, 2519]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>85</td>\n",
       "      <td>[142, 148, 673, 680, 1839, 2239]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>513</td>\n",
       "      <td>[876, 881, 969, 1315, 1317, 1686]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>477</td>\n",
       "      <td>[790, 1487, 1493, 1630, 2105, 2164]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>124</td>\n",
       "      <td>[187, 1849, 2251, 2533, 3328, 3344]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>117</td>\n",
       "      <td>[181, 705, 1343, 2246, 2381, 3087]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>95</td>\n",
       "      <td>[152, 678, 1422, 1556, 2024, 2572]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>92</td>\n",
       "      <td>[147, 1320, 1422, 1424, 1455, 2024]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>421</td>\n",
       "      <td>[681, 1320, 1842, 2025, 3125]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>413</td>\n",
       "      <td>[667, 872, 1303, 2023, 2071]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>403</td>\n",
       "      <td>[641, 1080, 1289, 2121, 2519]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>365</td>\n",
       "      <td>[575, 1036, 1232, 2442, 2475]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>526</td>\n",
       "      <td>[898, 900, 905, 1121, 1733]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>545</td>\n",
       "      <td>[937, 1431, 2585, 2706, 3088]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>527</td>\n",
       "      <td>[898, 900, 906, 1122, 1733]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>415</td>\n",
       "      <td>[672, 1320, 1422, 1455, 2024]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>529</td>\n",
       "      <td>[903, 1697, 1734, 2466, 2848]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>125</td>\n",
       "      <td>[187, 693, 2251, 2533, 3328]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>345</td>\n",
       "      <td>[541, 728, 1034, 1052, 2094]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>550</td>\n",
       "      <td>[946, 1595, 2313, 2403, 2404]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>888</td>\n",
       "      <td>[16, 947, 1791, 2613, 2623]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>296</td>\n",
       "      <td>[434, 1753, 2557, 2730, 3011]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>246</td>\n",
       "      <td>[339, 492, 876, 2458, 3319]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>475</td>\n",
       "      <td>[790, 1385, 1639, 2105, 2230]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>88</td>\n",
       "      <td>[145, 672, 1320, 1422, 2025]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>460</td>\n",
       "      <td>[765, 1004, 1719, 2666, 2867]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15</td>\n",
       "      <td>[18, 738, 1618, 2732, 3090]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>93</td>\n",
       "      <td>[147, 1320, 1358, 1424, 1455]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>778</td>\n",
       "      <td>[1487, 1493, 2105, 2164, 3292]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>473</td>\n",
       "      <td>[790, 1625, 2105, 2640, 3012]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>21</td>\n",
       "      <td>[28, 339, 876, 2458, 3319]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>90</td>\n",
       "      <td>[145, 1320, 1422, 2025, 3125]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>259</td>\n",
       "      <td>[364, 365, 368, 371, 2941]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>89</td>\n",
       "      <td>[145, 1320, 1321, 1422, 3125]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>441</td>\n",
       "      <td>[734, 2405, 2898, 2959, 3226]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>833</td>\n",
       "      <td>[1625, 1639, 2105, 2640, 3292]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>215</td>\n",
       "      <td>[290, 1861, 1863, 2920, 3196]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>992</td>\n",
       "      <td>[2196, 2203, 2210, 2674, 3044]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>474</td>\n",
       "      <td>[790, 1625, 1639, 2105, 2640]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>615</td>\n",
       "      <td>[1054, 1169, 1201, 1394, 2828]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                         nodes_list  clique_size\n",
       "0     247  [339, 492, 869, 1104, 2356, 2359, 2458, 2461, ...           11\n",
       "1     106  [167, 687, 911, 914, 916, 917, 1128, 1700, 196...           10\n",
       "2     885  [546, 1006, 1362, 1464, 1605, 1791, 2541, 2613...           10\n",
       "3      84  [141, 145, 146, 151, 672, 1321, 1422, 1423, 2024]            9\n",
       "4     673         [1214, 1271, 1495, 1639, 1996, 2230, 2233]            7\n",
       "5      20            [28, 354, 1281, 1730, 2458, 2657, 3319]            7\n",
       "6     139           [211, 596, 1568, 2112, 2378, 2492, 2686]            7\n",
       "7     533             [911, 912, 916, 917, 1128, 1700, 1961]            7\n",
       "8     886          [546, 1006, 1362, 1605, 1791, 2588, 2613]            7\n",
       "9      83            [141, 145, 672, 1320, 1321, 1422, 2024]            7\n",
       "10     87             [144, 148, 150, 680, 2187, 2238, 2239]            7\n",
       "11     25           [32, 1214, 1271, 1495, 1996, 2230, 2233]            7\n",
       "12    331           [512, 961, 1786, 2698, 2783, 3052, 3240]            7\n",
       "13    358          [561, 1005, 2075, 2616, 2755, 3091, 3264]            7\n",
       "14    276                   [395, 687, 911, 917, 1128, 1961]            6\n",
       "15    476                [790, 1487, 1630, 1639, 2105, 2230]            6\n",
       "16    350                [545, 1394, 1651, 1888, 2098, 2810]            6\n",
       "17    284                [413, 1007, 1612, 2149, 2552, 3102]            6\n",
       "18    118                [181, 1343, 2083, 2133, 2246, 3087]            6\n",
       "19    404                 [641, 646, 1080, 1289, 2122, 2519]            6\n",
       "20     85                   [142, 148, 673, 680, 1839, 2239]            6\n",
       "21    513                  [876, 881, 969, 1315, 1317, 1686]            6\n",
       "22    477                [790, 1487, 1493, 1630, 2105, 2164]            6\n",
       "23    124                [187, 1849, 2251, 2533, 3328, 3344]            6\n",
       "24    117                 [181, 705, 1343, 2246, 2381, 3087]            6\n",
       "25     95                 [152, 678, 1422, 1556, 2024, 2572]            6\n",
       "26     92                [147, 1320, 1422, 1424, 1455, 2024]            6\n",
       "27    421                      [681, 1320, 1842, 2025, 3125]            5\n",
       "28    413                       [667, 872, 1303, 2023, 2071]            5\n",
       "29    403                      [641, 1080, 1289, 2121, 2519]            5\n",
       "30    365                      [575, 1036, 1232, 2442, 2475]            5\n",
       "31    526                        [898, 900, 905, 1121, 1733]            5\n",
       "32    545                      [937, 1431, 2585, 2706, 3088]            5\n",
       "33    527                        [898, 900, 906, 1122, 1733]            5\n",
       "34    415                      [672, 1320, 1422, 1455, 2024]            5\n",
       "35    529                      [903, 1697, 1734, 2466, 2848]            5\n",
       "36    125                       [187, 693, 2251, 2533, 3328]            5\n",
       "37    345                       [541, 728, 1034, 1052, 2094]            5\n",
       "38    550                      [946, 1595, 2313, 2403, 2404]            5\n",
       "39    888                        [16, 947, 1791, 2613, 2623]            5\n",
       "40    296                      [434, 1753, 2557, 2730, 3011]            5\n",
       "41    246                        [339, 492, 876, 2458, 3319]            5\n",
       "42    475                      [790, 1385, 1639, 2105, 2230]            5\n",
       "43     88                       [145, 672, 1320, 1422, 2025]            5\n",
       "44    460                      [765, 1004, 1719, 2666, 2867]            5\n",
       "45     15                        [18, 738, 1618, 2732, 3090]            5\n",
       "46     93                      [147, 1320, 1358, 1424, 1455]            5\n",
       "47    778                     [1487, 1493, 2105, 2164, 3292]            5\n",
       "48    473                      [790, 1625, 2105, 2640, 3012]            5\n",
       "49     21                         [28, 339, 876, 2458, 3319]            5\n",
       "50     90                      [145, 1320, 1422, 2025, 3125]            5\n",
       "51    259                         [364, 365, 368, 371, 2941]            5\n",
       "52     89                      [145, 1320, 1321, 1422, 3125]            5\n",
       "53    441                      [734, 2405, 2898, 2959, 3226]            5\n",
       "54    833                     [1625, 1639, 2105, 2640, 3292]            5\n",
       "55    215                      [290, 1861, 1863, 2920, 3196]            5\n",
       "56    992                     [2196, 2203, 2210, 2674, 3044]            5\n",
       "57    474                      [790, 1625, 1639, 2105, 2640]            5\n",
       "58    615                     [1054, 1169, 1201, 1394, 2828]            5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliques_df[cliques_df['clique_size'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wildfire-ravaged areas of Australia get Christmas respite. An emergency vehicle near a fire in Blackheath, New South Wales (Ingleside Rural Fire Brigade/AP) The wildfire crisis forced Mr Morrison to cut short his much-criticised family holiday in Hawaii. He returned to Australia on Saturday night. “To Andrew and Geoffrey’s parents, we know this is going to be a tough Christmas for you, first one ...\n",
      "Why record wildfires and soaring temperatures won;t sway Australia;s government on climate change. The Australian Government is under fire for inaction on climate change while a bush fire crisis and a heatwave sweeps the country. On Tuesday Australia experienced its hottest day on record with the national average temperature reaching a high of 40.C. Late on Wednesday there were 100 bush fires burning in New South Wales alone, with 54 still ...\n",
      "‘National tragedy’: Hundreds of koalas feared dead in Australian wildfire. Hundreds of koalas are feared to have died in wildfires raging along Australia’s east coast. The fire started on Friday after a lightning strike hit a forest in the state of New South Wales. The blaze has since burned through 4,00 acres. Sharing the ...\n",
      "Koala rescued from wildfires by passerby. A badly-burnt koala is rescued by a brave passerby as it crawled through a wildfire in Australia. The koala, named Lewis, is facing a long recovery. Toni Doherty gave him water and wrapped him in a blanket after finding the injured koala on the Oxley Highway west of the New South Wales state town of Wauchope. The 14-year-old animal was taken to ...\n",
      "Water Thieves Steal 00,000 Litres in Australia as Our Mad Max-Style Future Becomes Reality. Thieves stole roughly 00,000 litres of water in a region of Australia that’s suffering from one of the worst droughts in the history of the country. And with record-breaking heat and bushfires getting even larger, it feels like Australia is living in the future. That future, unfortunately, looks a lot like Mad Max. Police in New South Wales ...\n",
      "Australian PM defends government policy on climate change amid wildfire crisis. Australia’s embattled prime minister has defended his government’s climate policy, as authorities warned the wildfires crisis could fester for months. Around 200 wildfires were burning in four states, with New South Wales accounting for more than half of them, including 60 fires which are not contained. The disaster has led to renewed ...\n",
      "Sydney facing ;catastrophic; threat as Australian state declares wildfire emergency. Residents in Sydney;s suburbs are preparing to evacuate after a quot;catastrophicquot; warning was put in place for the city as wildfires continue to rage across Australia. Ferocious infernos were burning at emergency-level intensity across the nation;s most populous state New South Wales on Tuesday as authorities warned most populations in their paths ...\n",
      "‘National tragedy’: Hundreds of koalas feared dead in Australian wildfire. Hundreds of koalas are feared to have died in wildfires raging along Australia’s east coast. The fire started on Friday after a lightning strike hit a forest in the state of New South Wales. The blaze has since burned through 4,00 acres. Sharing the full story, not just the headlines Sue Ashton, the president of Port Macquarie Koala Hospital ...\n",
      "Thousands of koalas feared dead as massive Australian wildfires destroy habitat. The mid-north coast of New South Wales was home to up to 28,000 koalas, but wildfires in the area in recent months have significantly reduced their population. Koalas are native to Australia and are one of the country’s most beloved animals, but they have been under threat due to a loss of habitat. Smoke rises from wildfires in the Blue ...\n",
      "Thousands of koalas feared to have died in Australian wildfires. The mid-north coast of New South Wales was home to up to 28,000 koalas, but wildfires in the area in recent months have significantly reduced their population. Images shared of koalas drinking water after being rescued from the wildfires have gone viral on social media in recent days. Koalas are native to Australia and are one of the country ...\n",
      "Australia wildfires: Firefighters killed as ;catastrophic; blazes encircle Sydney. Catastrophic conditions fuelled massive bushfires across New South Wales in Australia on Saturday, with two blazes around Sydney burning at emergency level. In South Australia one person was found dead in a fire zone, following the deaths of two firefighters in New South Wales on Thursday. Some major roads heading to the south and west from ...\n"
     ]
    }
   ],
   "source": [
    "for node in cliques_df.iloc[0]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three cows presumed dead after being swept away by Hurricane Dorian are found alive months later. Three cows swept off an island of North Carolina during the raging storm of Hurricane Dorian have been found alive months later, after reportedly swimming for several miles. They were grazing on their home of Cedar Island when the severe weather hit ...\n",
      "Three cows swim five miles to safety after being swept out to sea in hurricane. The cows were swept away from Cedar Island, in North Carolina, US, by a ;mini-tsunami; caused by Hurricane Dorian - and were lucky not to have been dragged to their deaths in the Atlantic\n",
      "Cows swept away by Hurricane Dorian found alive in North Carolina. Three cows swept off an island in North Carolina during Hurricane Dorian have been found alive after apparently swimming for several miles. The cows belong to a herd on the US state;s Cedar Island but were swept away in September by a quot;mini tsunamiquot; generated by Dorian. They were presumed dead until they were spotted at the Cape Lookout ...\n",
      "Cows swept out to sea by hurricane found alive after swimming for several miles. Three cows swept off an island in North Carolina during Hurricane Dorian that were believed to have drowned have turned up safe and well on another island two months later, after apparently swimming several miles to safety. The animals came from a larger herd living on Cedar Island, off the east coast of the US, and were swept away during a ...\n",
      "Cows swept away by Hurricane Dorian found after ‘swimming four miles’ to island. Three cows presumed dead after being swept out to sea by Hurricane Dorian have been found on North Carolina’s Outer Banks having apparently swum several miles to safety. The intrepid bovines were part of a herd living on the sands of Cedar Island when the storm struck in September, creating what has been described as a “mini tsunami” that ...\n",
      "Cows survived Hurricane Dorian by swimming for miles. Three cows swept off an island during Hurricane Dorian have been found alive after apparently swimming for several miles. The cows, part of a herd on the US state;s Cedar Island, were swept away in September by a “mini tsunami” in North Carolina. They were presumed dead until they were discovered in a park. They “certainly have a ...\n",
      "Cows survived Hurricane Dorian by swimming for miles. Three cows swept off an island during Hurricane Dorian have been found alive after apparently swimming for several miles. The cows, part of a herd on the US state;s Cedar Island, were swept away in September by a “mini tsunami” in North Carolina.\n",
      "Three cows presumed dead after being swept away by Hurricane Dorian are found alive months later. Three cows swept off an island of North Carolina during the raging storm of Hurricane Dorian have been found alive months later, after reportedly swimming for several miles. They were grazing on their home of Cedar Island when the severe weather hit, sweeping away wildlife from the island’s herd on September 6. The cows were presumed dead ...\n",
      "Cows swept away by Hurricane Dorian found alive in North Carolina. Three cows swept off an island in North Carolina during Hurricane Dorian have been found alive after apparently swimming for several miles. The cows belong to a herd on the US state;s Cedar Island but were swept away in September by a quot;mini tsunami ...\n",
      "Cows swept away by Hurricane Dorian found after ‘swimming four miles’ to island. Three cows presumed dead after being swept out to sea by Hurricane Dorian have been found on North Carolina’s Outer Banks having apparently swum several miles to safety. The intrepid bovines were part of a herd living on the sands of Cedar Island when ...\n"
     ]
    }
   ],
   "source": [
    "for node in cliques_df.iloc[1]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typhoon Kammuri slams into Philippines, forcing thousands to flee. Typhoon Kammuri has made landfall in the central Philippines, at the southern end of Luzon island. At least 200,000 residents have been evacuated from coastal and mountainous areas over fears of flooding, storm surges and landslides. Some events at the ...\n",
      "Typhoon Kammuri update: Brutal tropical storm slams into Philippines as heavy rains ensue. JUST IN: Typhoon Kammuri update: Huge storm heading straight for Philippines The Philippines operates a domestic five-level scale when grading typhoons and their severity. Five areas have been placed under Tropical Cyclone Warning Signal No , and the ...\n",
      "Typhoon Kammuri: At least four dead as storm hammers Philippines. At least four people have been killed after Typhoon Kammuri slammed into the Philippines. Hundreds of thousands of people were evacuated from high-risk villages while Manilla;s international airport was shut on Tuesday as fierce winds and rain hit the country.\n",
      "Driver gets stuck in river after Typhoon Kammuri in the Philippines. People in a car have been rescued after the vehicle got stuck in a raging river on its way to deliver aid to the victims of the recent Typhoon Kammuri in the Philippines.\n",
      "Typhoon Kammuri slams into Philippines, forcing thousands to flee. Typhoon Kammuri has made landfall in the central Philippines, at the southern end of Luzon island. At least 200,000 residents have been evacuated from coastal and mountainous areas over fears of flooding, storm surges and landslides. Some events at the Southeast Asian Games, which opened on Saturday, have been cancelled or re-scheduled.\n",
      "Typhoon Kammuri forecast for direct hit on Philippines with 150mph winds and 16in of rain. Typhoon Kammuri is rapidly intensifying and is expected to approach the Philippines through the weekend. The monster storm could deliver dangerous impacts during the first week of December. Metdesk company WXCharts tweeted: “Typhoon Kammuri is headed directly for the Philippines after the weekend, potentially as a super typhoon, just as the ...\n",
      "Typhoon Kammuri update: Brutal tropical storm slams into Philippines as heavy rains ensue. JUST IN: Typhoon Kammuri update: Huge storm heading straight for Philippines The Philippines operates a domestic five-level scale when grading typhoons and their severity. Five areas have been placed under Tropical Cyclone Warning Signal No , and the Metro Manila has been placed under Signal No 2. A number of schools across the Philippines ...\n",
      "Typhoon Kammuri: At least four dead as storm hammers Philippines. At least four people have been killed after Typhoon Kammuri slammed into the Philippines. Hundreds of thousands of people were evacuated from high-risk villages while Manilla;s international airport was shut on Tuesday as fierce winds and rain hit the country. Four people have reportedly died so far and several others have been injured.\n",
      "Typhoon Kammuri slams into Philippines, forcing thousands to flee. Typhoon Kammuri has made landfall in the central Philippines, at the southern end of Luzon island. Some 200,000 residents have been evacuated from coastal and mountainous areas over fears of flooding, storm surges and landslides. Operations at Manila international airport will be suspended for 12 hours from Tuesday morning. Some events at the ...\n"
     ]
    }
   ],
   "source": [
    "for node in cliques_df.iloc[3]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road closed due to fallen tree as further flood alerts issued. quot;We will organise removal of the tree as soon as possible. Please plan your journey and use alternative routes in the meantime.quot; Meanwhile a number of Environment Agency flood alerts remain in force in Staffordshire - including on the Rivers Sow and Penk in Stafford Borough and the Churnet, Tean and Upper Dove in the Staffordshire Moorlands.\n",
      "Flood alert and weather warning in force for areas of Staffordshire. A flood alert and a weather warning are in force for different areas of Staffordshire as rainfall is expected across the region. The Met Office are currently forecasting rain, which could be heavier in the hills, throughout today (Saturday November 2). The Environment Agency flood alert, which covers the Rivers Sow and Penk in Stafford Borough ...\n",
      "Flood alert remains in force as further rain expected in Staffordshire. A flood alert remains in force for two rivers around Stafford Borough this evening (Wednesday November 1). The alert, which means flooding is possible, has been issued by the Environment Agency for the Rivers Sow and Penk in Stafford Borough. The alert adds that further rainfall is possible on Thursday. The Environment Agency alert states ...\n",
      "Flood alerts in force for rivers across Staffordshire following heavy rain. Flood alerts have been issued for a number of rivers in Staffordshire following heavy rain. As of 7.50am this morning the Environment Agency has the following flood alerts in force across the region. The Rivers Churnet and Tean in the Moorlands The Sow and Penk in Stafford Borough The Upper Dove in the Moorlands The Met Office forecast a ...\n",
      "Flood alerts in force for rivers across Staffordshire following heavy rain. Flood alerts have been issued for a number of rivers in Staffordshire following heavy rain. As of 7.50am this morning the Environment Agency has the following flood alerts in force across the region. The Rivers Churnet and Tean in the Moorlands The Sow and ...\n",
      "Road closed due to fallen tree as further flood alerts issued. Please plan your journey and use alternative routes in the meantime.quot; Meanwhile a number of Environment Agency flood alerts remain in force in Staffordshire - including on the Rivers Sow and Penk in Stafford Borough and the Churnet, Tean and Upper Dove in ...\n"
     ]
    }
   ],
   "source": [
    "for node in cliques_df.iloc[17]['nodes_list']:\n",
    "    article = nouns_df.reset_index().iloc[node]\n",
    "    print(article['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [component for component in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(component) for component in components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain Community Detection\n",
    "# The keys are nodes, the values are the partitions they belong to\n",
    "partition = best_partition(G)\n",
    "\n",
    "number_partitions = max(partition.values())\n",
    "number_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through and get a list of partitions and their nodes\n",
    "partition_contents = {}\n",
    "for key in partition.keys():\n",
    "    partition_contents[partition[key]] = partition_contents.get(partition[key], []) + [key]\n",
    "\n",
    "# Drop partitions that are too small\n",
    "for key in list(partition_contents.keys()):\n",
    "    if len(partition_contents[key]) < 5:\n",
    "        partition_contents.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 57, 24, 41, 27, 26, 15, 7, 14, 12, 26, 12, 6, 12, 60, 16, 58, 7, 24, 26, 8, 8, 14, 24, 6, 9, 6, 5, 5, 15, 7, 9, 7, 7, 5, 7, 7, 5, 5] 667\n"
     ]
    }
   ],
   "source": [
    "# Let's see how big our \"clusters\" are, and how many there are total after removing the tiny ones\n",
    "partition_lengths = [len(value) for key, value in partition_contents.items()]\n",
    "print(partition_lengths, sum(partition_lengths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
