{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict size of cluster from text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import lib.helper as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various metrics by which to judge a model's performance\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Regression type measurements\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Import the ML models to try\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tools for evaluating the model by running it repeatedly\n",
    "# with variants of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.  Create features, labels, train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "df = pd.read_csv(\"working/disaster_clustered_louvain_3lim_5000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A quick utility function to pre-process the text\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(helper.preprocess_description)\n",
    "df['phrased_tokens'] = helper.get_phrased_nouns(df['clean_text'])\n",
    "\n",
    "# Create variable for cluster size\n",
    "df_size = pd.DataFrame(df['cluster'].value_counts())\n",
    "df_size['cluster_label'] = df_size.index\n",
    "df_size.columns = ['cluster_size', 'cluster']\n",
    "df_size.head()\n",
    "\n",
    "df = df.merge(df_size, on=\"cluster\", how=\"left\")\n",
    "\n",
    "#df = df[df['cluster'] != -1]\n",
    "df['cluster_size'] = np.where(df['cluster']==-1, 1.0, df['cluster_size'])\n",
    "\n",
    "# Sort by date-order, latest last!\n",
    "df['date_clean'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
    "df = df.sort_values(\"date_clean\")\n",
    "\n",
    "# Take a look at the features and labels\n",
    "df[['cluster_size', 'tokens', 'phrased_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(decode_error=\"ignore\", max_features=1000)\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\", max_features=7000)\n",
    "\n",
    "# Create feature vectors\n",
    "X = vectorizer.fit_transform(df['phrased_tokens'].apply(\" \".join))\n",
    "\n",
    "# Create Labels\n",
    "y = np.asarray(df['cluster_size'].astype(float))\n",
    "\n",
    "# Check that worked\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split the data randomly, save 'test' for final pass\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data time-wise, save 'test' for final pass\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Ensure that the training data is shuffled at random\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the measurements we want to make (GridSearch will default to MSE)\n",
    "scoring = {'MSE': make_scorer(mean_squared_error),\n",
    "           'R2': make_scorer(r2_score)}\n",
    "\n",
    "parameters = {'alpha':[0.01, 0.03, 0.05, 0.07, 0.1, 0.5, 1.0, 2.0],\n",
    "              'l1_ratio': [1.0]}\n",
    "\n",
    "model = ElasticNet()\n",
    "\n",
    "# Perform cross-validated grid-search\n",
    "clf = GridSearchCV(estimator=model,\n",
    "                   cv=5,\n",
    "                   param_grid=parameters,\n",
    "                   return_train_score=True,\n",
    "                   scoring=scoring,\n",
    "                   refit=\"R2\",\n",
    "                   n_jobs=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the performance of each hyperparameter combination\n",
    "pd.DataFrame(clf.cv_results_)\\\n",
    "  [['params', 'mean_test_R2', 'mean_test_MSE']]\\\n",
    "  .sort_values(\"mean_test_R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"** VALIDATION RESULTS **\")\n",
    "for method_name, scorer in scoring.items():\n",
    "    print(method_name, scorer(X=X_test, y_true=y_test, estimator=clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all words; cv R2 = 0.24, val R2 = 0.36 (???)\n",
    "\n",
    "With all non-nouns; cv R2 = 0.057, val R2 = 0.09\n",
    "\n",
    "With nouns only; cv R2 = 0.31, val R2 = 0.45\n",
    "\n",
    "With nouns only after remembering to shuffle the damn training data; cv R2 = 0.45, val R2 = 0.45\n",
    "With nouns only after dropping outliers; cv R2 = 0.57, val R2 = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_get_coefs(fitted_model, feature_names):\n",
    "    \"\"\"\n",
    "    Gets ordered table of coefficient names and magnitudes\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"name\": feature_names,\n",
    "                       \"coefficient\": fitted_model.coef_})\n",
    "    \n",
    "    # MAGNITUDES rather than VALUE determine importance\n",
    "    df['abs_value'] = df['coefficient'].apply(abs)\n",
    "    \n",
    "    return df.sort_values(\"abs_value\", ascending=False)\n",
    "\n",
    "\n",
    "# In this call, retrieving final (selected and retrained)\n",
    "# estimator from the grid search\n",
    "coef_table = help_get_coefs(clf.best_estimator_, list(vectorizer.get_feature_names()))\n",
    "\n",
    "coef_table.head(10).to_csv(\"outputs/ml_nn3_coefficients_disaster_top.csv\", index=False)\n",
    "coef_table.tail(10).to_csv(\"outputs/ml_nn3_coefficients_disaster_bot.csv\", index=False)\n",
    "coef_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features ultimately selected?\n",
    "# (they're regularised to NEAR zero, so need a tolerance)\n",
    "coef_table[coef_table['abs_value'] >= 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What features suck?\n",
    "coef_table[coef_table['abs_value'] <= 0.01].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_table['abs_value'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Plots\n",
    "I'm, to some degree, improvising my own because no single python package quite does everything I'd expect R to do for a linear regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"Training True Size\": y_train,\n",
    "                         \"CV Predicted Size\": y_pred})\n",
    "\n",
    "test_df = pd.DataFrame({\"Validation True Size\": y_test,\n",
    "                        \"Validation Predicted Size\": clf.best_estimator_.predict(X_test)})\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Note this isn't the line of the actual regression model.\n",
    "sns.regplot(x=\"Training True Size\",\n",
    "            y=\"CV Predicted Size\",\n",
    "            data=train_df,\n",
    "            ax=axs[0],\n",
    "            scatter_kws={\"alpha\":0.05})\n",
    "\n",
    "axs[0].set_xlim((0, 1000))\n",
    "axs[0].set_ylim((-200, 1400))\n",
    "axs[0].text(10, 940, \"a)\")\n",
    "\n",
    "# Note this isn't the line of the actual regression model.\n",
    "sns.regplot(x=\"Validation True Size\",\n",
    "            y=\"Validation Predicted Size\",\n",
    "            data=test_df,\n",
    "            ax=axs[1],\n",
    "            scatter_kws={\"alpha\":0.05})\n",
    "\n",
    "axs[1].set_xlim((0, 1000))\n",
    "axs[1].set_ylim((-200, 1400))\n",
    "axs[1].text(10, 940, \"b)\")\n",
    "\n",
    "plt.savefig(\"outputs/disaster_noun_outlier_LR_predicted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def help_plot_residuals(fitted_model, X, y):\n",
    "    \"\"\"\n",
    "    Utility function:  Plot residual values for a given model\n",
    "    and features + predictions\n",
    "    \"\"\"\n",
    "    predicted = fitted_model.predict(X)\n",
    "    residuals = predicted - y\n",
    "    \n",
    "    f = plt.figure(figsize=(8, 5), constrained_layout=True)\n",
    "    gs = f.add_gridspec(3, 3)\n",
    "    f_ax0 = f.add_subplot(gs[:, :-1])\n",
    "    f_ax1 = f.add_subplot(gs[:, -1])\n",
    "    \n",
    "    # Residuals vs predicted\n",
    "    sns.scatterplot(x=predicted, y=residuals, alpha=0.2, ax=f_ax0)\n",
    "    f_ax0.set_xlabel(\"CV Predicted Size\")\n",
    "    f_ax0.set_ylabel(\"Residual Value\")\n",
    "    f_ax0.set_ylim((-800, 800))\n",
    "    f_ax0.text(0, 350, \"a)\")\n",
    "    \n",
    "    # Hist of residuals\n",
    "    sns.distplot(a=residuals, hist=True, vertical=True, ax=f_ax1)\n",
    "    f_ax1.set_ylim((-800, 800))\n",
    "    f_ax1.set_yticklabels([])\n",
    "    f_ax1.text(0.001, 350, \"b)\")\n",
    "\n",
    "help_plot_residuals(clf.best_estimator_, X_train, y_train)\n",
    "\n",
    "plt.savefig(\"outputs/disaster_noun_outlier_LR_residual.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a traditional Linear Regression Model so that we can examine the p-values and r2 values\n",
    "# of different features\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot to gauge suitability of normality assumption\n",
    "res = clf.best_estimator_.predict(X_train) - y_train\n",
    "\n",
    "fig = sm.qqplot(res, stats.t, fit=True, line='45', alpha=0.2)\n",
    "plt.savefig(\"outputs/disaster_noun_outlier_LR_QQ.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot to gauge suitability of normality assumption\n",
    "res = clf.best_estimator_.predict(X_train) - y_train\n",
    "\n",
    "fig = sm.qqplot(res, stats.t, fit=True, line='45', alpha=0.02)\n",
    "fig.axes[0].set_xlim(-15, 15)\n",
    "fig.axes[0].set_ylim(-15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature significance using an equivalent model in StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data=X_train.todense(), columns=list(vectorizer.get_feature_names()))\n",
    "df_stats.head()\n",
    "\n",
    "df_stats['StorySize'] = y_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xk = df_stats[list(vectorizer.get_feature_names())]\n",
    "Ym = df_stats['StorySize']\n",
    "\n",
    "Xm = sm.add_constant(Xk)\n",
    "\n",
    "# fitting the the model for multiple regression \n",
    "Km = sm.OLS(Ym,Xm).fit()\n",
    "Km.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
