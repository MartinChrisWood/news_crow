{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-35b17710246d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# There's a helper function to go find and drag out the various JSON files created by the scrapers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_clean_world_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Make sure after cleaning etc it's indexed from 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Martin\\Documents\\GitHub\\news_crow\\lib\\helper.py\u001b[0m in \u001b[0;36mload_clean_world_corpus\u001b[1;34m(directory, corpus_tag, drop_raw, brutal)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_clean_world_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrutal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;34m\"\"\" All common pre-processing. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_world_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;31m# Filter to only the .uk vendors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Martin\\Documents\\GitHub\\news_crow\\lib\\helper.py\u001b[0m in \u001b[0;36mcorpus_world_loader\u001b[1;34m(directory, corpus_tag, drop_raw)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source_url'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworld_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"RSS\"\n",
    "\n",
    "# There's a helper function to go find and drag out the various JSON files created by the scrapers.\n",
    "corpus = helper.load_clean_world_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus=corpus.sample(n=30000, replace=False)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Detected Nouns to create a Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lindstaedt</th>\n",
       "      <th>apach</th>\n",
       "      <th>fearsom</th>\n",
       "      <th>caiffa</th>\n",
       "      <th>mcenani</th>\n",
       "      <th>arquett</th>\n",
       "      <th>kamala_harri</th>\n",
       "      <th>petrel</th>\n",
       "      <th>nangarhar</th>\n",
       "      <th>muhandi</th>\n",
       "      <th>...</th>\n",
       "      <th>mathew</th>\n",
       "      <th>bobi</th>\n",
       "      <th>jarrid</th>\n",
       "      <th>mirjan</th>\n",
       "      <th>carolina_bahama</th>\n",
       "      <th>river</th>\n",
       "      <th>wetland</th>\n",
       "      <th>cabinet</th>\n",
       "      <th>pluvign</th>\n",
       "      <th>schaffner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hurricane Dorian lashes US as Bahamas counts cost. Life-threatening US storm surges are feared, as rescue work continues in the devastated Bahamas.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kohistan video murders: Three guilty in 'honour killing' blood feud. They are relatives of a group of Pakistani women killed after being filmed singing at a wedding.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MH17 Ukraine plane crash: 'Key witness' released. A Ukrainian court releases a potentially key witness to the downing of the Malaysian airliner MH17.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 70: The weddings 'ruined' by Kashmir's lockdown. Indian-administered Kashmir is under a security crackdown after it was stripped of its special status.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syria war: Turkey warns Europe of new migrant wave. President Erdogan demands international help to create a refugee \"safe zone\" in northern Syria.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    lindstaedt  apach  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...           0      0   \n",
       "Kohistan video murders: Three guilty in 'honour...           0      0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...           0      0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...           0      0   \n",
       "Syria war: Turkey warns Europe of new migrant w...           0      0   \n",
       "\n",
       "                                                    fearsom  caiffa  mcenani  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...        0       0        0   \n",
       "Kohistan video murders: Three guilty in 'honour...        0       0        0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...        0       0        0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...        0       0        0   \n",
       "Syria war: Turkey warns Europe of new migrant w...        0       0        0   \n",
       "\n",
       "                                                    arquett  kamala_harri  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...        0             0   \n",
       "Kohistan video murders: Three guilty in 'honour...        0             0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...        0             0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...        0             0   \n",
       "Syria war: Turkey warns Europe of new migrant w...        0             0   \n",
       "\n",
       "                                                    petrel  nangarhar  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...       0          0   \n",
       "Kohistan video murders: Three guilty in 'honour...       0          0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...       0          0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...       0          0   \n",
       "Syria war: Turkey warns Europe of new migrant w...       0          0   \n",
       "\n",
       "                                                    muhandi  ...  mathew  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...        0  ...       0   \n",
       "Kohistan video murders: Three guilty in 'honour...        0  ...       0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...        0  ...       0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...        0  ...       0   \n",
       "Syria war: Turkey warns Europe of new migrant w...        0  ...       0   \n",
       "\n",
       "                                                    bobi  jarrid  mirjan  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...     0       0       0   \n",
       "Kohistan video murders: Three guilty in 'honour...     0       0       0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...     0       0       0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...     0       0       0   \n",
       "Syria war: Turkey warns Europe of new migrant w...     0       0       0   \n",
       "\n",
       "                                                    carolina_bahama  river  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...                0      0   \n",
       "Kohistan video murders: Three guilty in 'honour...                0      0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...                0      0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...                0      0   \n",
       "Syria war: Turkey warns Europe of new migrant w...                0      0   \n",
       "\n",
       "                                                    wetland  cabinet  pluvign  \\\n",
       "Hurricane Dorian lashes US as Bahamas counts co...        0        0        0   \n",
       "Kohistan video murders: Three guilty in 'honour...        0        0        0   \n",
       "MH17 Ukraine plane crash: 'Key witness' release...        0        0        0   \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...        0        0        0   \n",
       "Syria war: Turkey warns Europe of new migrant w...        0        0        0   \n",
       "\n",
       "                                                    schaffner  \n",
       "Hurricane Dorian lashes US as Bahamas counts co...          0  \n",
       "Kohistan video murders: Three guilty in 'honour...          0  \n",
       "MH17 Ukraine plane crash: 'Key witness' release...          0  \n",
       "Article 70: The weddings 'ruined' by Kashmir's ...          0  \n",
       "Syria war: Turkey warns Europe of new migrant w...          0  \n",
       "\n",
       "[5 rows x 19387 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the text representation\n",
    "model = reps.NounAdjacencyModel2(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "# Tabulate for convenience\n",
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "#### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the set of search terms used for Bing, so we can remove them before\n",
    "# clustering.\n",
    "with open(\"D:/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "    scrape_config = json.load(f)\n",
    "\n",
    "search_terms = scrape_config['disaster_search_list']\n",
    "search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31393, 1500)\n"
     ]
    }
   ],
   "source": [
    "# Get X most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "#nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only most common\n",
    "nouns_to_keep = nouns_to_keep[:1500]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356404"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 2\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges - not sure if needed?\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(G, \"working/RSS_graph_2lim_tight.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12596 to beat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c.  Try CDLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdlib\n",
    "from cdlib import algorithms\n",
    "from cdlib import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster_from_model(corpus, model, threshold=5):\n",
    "    \"\"\" Mine the cluster-of-node info from the model instance \"\"\"\n",
    "    community_lookup = {}\n",
    "    for comm_index, members in enumerate(model.communities):\n",
    "        for member in members:\n",
    "            community_lookup[member] = comm_index\n",
    "            \n",
    "    # Add cluster to DF.  If node not in cluster, assign -1 (outlier)\n",
    "    corpus['node'] = corpus.index\n",
    "    corpus['cluster'] = corpus['node'].apply(lambda x: community_lookup.get(str(x), -1))\n",
    "    corpus[['clean_text', 'cluster']].head(10)\n",
    "    \n",
    "    # If cluster is smaller than minimum limit, designate as outlier\n",
    "    cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "    corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < threshold) else x)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def generate_louvain(G, corpus, resolution=1.0, threshold=5):\n",
    "    \"\"\" Develop a louvain model, assign clusters etc \"\"\"\n",
    "    print(\"\\nLouvain resolution: \", resolution)\n",
    "    \n",
    "    # Find the communities\n",
    "    louvain_coms = algorithms.louvain(G, resolution = resolution)\n",
    "    \n",
    "    corpus = assign_cluster_from_model(corpus, louvain_coms)\n",
    "    \n",
    "    # What percentage are now classed as outliers?\n",
    "    print(\"Percent classed outlier: \", 100.0 * sum(corpus['cluster']==-1) / corpus.shape[0])\n",
    "    \n",
    "    # How many unique clusters after all this?  (minus one for outliers)\n",
    "    print(\"Number of unique clusters: \", len(pd.unique(corpus['cluster'])))\n",
    "    \n",
    "    return corpus, len(pd.unique(corpus['cluster'])), 100.0 * sum(corpus['cluster']==-1) / corpus.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G = nx.read_gml(\"working/RSS_graph_3lim.gml\")\n",
    "corpus = generate_louvain(G, corpus, resolution=2.0)\n",
    "corpus.to_csv(\"working/RSS_clustered_louvain.csv\")\n",
    "coherences = helper.report_corpus_model_coherence(\"working/RSS_clustered_louvain_2lim_permissive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Louvain resolution:  1.0\n",
      "Percent classed outlier:  40.2159717134393\n",
      "Number of unique clusters:  70\n",
      "\n",
      "Louvain resolution:  3.0\n",
      "Percent classed outlier:  48.45029146625044\n",
      "Number of unique clusters:  683\n",
      "\n",
      "Louvain resolution:  5.0\n",
      "Percent classed outlier:  48.19545758608607\n",
      "Number of unique clusters:  625\n",
      "\n",
      "Louvain resolution:  7.0\n",
      "Percent classed outlier:  48.20182843309018\n",
      "Number of unique clusters:  625\n",
      "\n",
      "Louvain resolution:  10.0\n",
      "Percent classed outlier:  48.392953843213455\n",
      "Number of unique clusters:  625\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_gml(\"working/RSS_graph_2lim_permissive.gml\")\n",
    "\n",
    "resolutions = [0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
    "cluster_count = []\n",
    "outlier_pct = []\n",
    "for res in resolutions:\n",
    "    x, n_clusters, pct_outliers = generate_louvain(G, corpus, resolution=res)\n",
    "    \n",
    "    cluster_count.append(n_clusters)\n",
    "    outlier_pct.append(pct_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Louvain resolution:  5.0\n",
      "Percent classed outlier:  48.19545758608607\n",
      "Number of unique clusters:  625\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_gml(\"working/RSS_graph_2lim_permissive.gml\")\n",
    "corpus = generate_louvain(G, corpus, resolution=5.0)\n",
    "corpus.to_csv(\"working/RSS_clustered_louvain_2lim_permissive_highres.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bigclam_coms.communities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bigclam_coms.average_internal_degree()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bigclam_coms.newman_girvan_modularity()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3.  Create (overlapping) clusters using Maximal Cliques\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "Expanded using k-clique-communities REF FIND PAPER"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c = list(nx.algorithms.community.kclique.k_clique_communities(G, 4))\n",
    "cliques = [(len(x), x) for x in c]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cliques"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()\n",
    "\n",
    "cliques_df = cliques_df[(cliques_df['clique_size'] >= 3) & (cliques_df['clique_size'] <=100)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cliques_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Flatten the cliques DF into long format\n",
    "flattened = {\"cluster_index\":[], \"node\":[]}\n",
    "\n",
    "for index, row in cliques_df.iterrows():\n",
    "    for node in row[\"nodes_list\"]:\n",
    "        flattened[\"cluster_index\"].append(index)\n",
    "        flattened[\"node\"].append(node)\n",
    "        \n",
    "\n",
    "partition_df = pd.DataFrame(flattened)\n",
    "\n",
    "# Create a single string variable (\";\" separated) to record all clusters/cliques a single record belongs in\n",
    "partition_df[\"cluster\"] = partition_df.\\\n",
    "                          groupby(\"node\")[\"cluster_index\"].\\\n",
    "                          transform(lambda x: \";\".join([str(i) for i in x if type(i)==int]))\n",
    "\n",
    "# Clean up, set index of this and corpus so the two DF's can be joined with little effort\n",
    "partition_df = partition_df[[\"node\", \"cluster\"]].\\\n",
    "               drop_duplicates([\"node\", \"cluster\"], keep=\"first\").\\\n",
    "               set_index(\"node\")\n",
    "\n",
    "corpus.drop([\"cluster\", \"node\"], axis=1).join(partition_df).\\\n",
    "       to_csv(\"working/RSS_clustered_cliques.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### The below attempts overlapping community detection but can only run on connected graphs, think this is an implicit restriction of the algorithm logic."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get all connected components (will become less of an issue as graph size increases)\n",
    "ccs = [(len(x), x) for x in nx.connected_components(G)]\n",
    "\n",
    "# Sort by size (largest first)\n",
    "ccs.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract largest connected sub-graph\n",
    "connected_sub = G.subgraph(ccs[0][1])\n",
    "\n",
    "# re-index nodes from zero to maintain compatibility with CDLIB (sub-dependency, Karate)\n",
    "# Will need to reverse this indexing when matching assigned clusters back to data\n",
    "node_relabel_dict = {val: i for i, val in enumerate(list(connected_sub.nodes))}\n",
    "\n",
    "connected_sub = nx.relabel_nodes(connected_sub, node_relabel_dict)\n",
    "\n",
    "# Fire algo!\n",
    "bigclam_coms = algorithms.big_clam(connected_sub)\n",
    "#leiden_coms = algorithms.leiden(connected_sub)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bigclam_coms.communities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build dict of node-to-cluster lookup\n",
    "community_lookup = {}\n",
    "for comm_index, members in enumerate(bigclam_coms.communities):\n",
    "    for member in members:\n",
    "        community_lookup[member] = community_lookup.get(member, []) + [comm_index]\n",
    "\n",
    "# Add cluster to DF.  If node not in cluster, assign -1 (outlier)\n",
    "corpus['node'] = corpus.index\n",
    "corpus['cluster'] = corpus['node'].apply(lambda x: community_lookup.get(x, [-1]))\n",
    "corpus[['clean_text', 'cluster']].head(10)\n",
    "\n",
    "# If cluster is smaller than minimum limit, designate as outlier\n",
    "cs_lookup = corpus['cluster'].value_counts().to_dict()\n",
    "corpus['cluster'] = corpus['cluster'].apply(lambda x: -1 if (cs_lookup[x] < 5) else x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# What percentage are now classed as outliers?\n",
    "100.0 * sum(corpus['cluster']==-1) / corpus.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# How many unique clusters after all this?  (minus one for outliers)\n",
    "len(pd.unique(corpus['cluster']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus.to_csv(\"working/disaster_clustered_bigclam.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
