{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict size of cluster from text content\n",
    "\n",
    "### TODO:  Look in to custom cross-validation (ie; I want to train on the earliest 80 % of stories, evaluate on the latest 20 %!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import lib.helper as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various metrics by which to judge a model's performance\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Regression type measurements\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Import the ML models to try\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tools for evaluating the model by running it repeatedly\n",
    "# with variants of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.  Create features, labels, train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "df = pd.read_csv(\"working/RSS_clustered_louvain_3lim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>tokens</th>\n",
       "      <th>phrased_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[hurrican, dorian, lash, bahama, count, cost, ...</td>\n",
       "      <td>[hurrican_dorian, bahama_bahama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[kohistan, video, murder, guilti, honour, kill...</td>\n",
       "      <td>[kohistan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[mh, ukrain, plane, crash, key, wit, releas, u...</td>\n",
       "      <td>[mh, ukrain, mh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[articl, wed, ruin, kashmir, lockdown, indian,...</td>\n",
       "      <td>[kashmir, indian, kashmir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476.0</td>\n",
       "      <td>[syria, war, turkey, warn, europ, new, migrant...</td>\n",
       "      <td>[syria_turkey, europ, presid_erdogan, syria]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_size                                             tokens  \\\n",
       "0           1.0  [hurrican, dorian, lash, bahama, count, cost, ...   \n",
       "1           1.0  [kohistan, video, murder, guilti, honour, kill...   \n",
       "2           1.0  [mh, ukrain, plane, crash, key, wit, releas, u...   \n",
       "3           1.0  [articl, wed, ruin, kashmir, lockdown, indian,...   \n",
       "4         476.0  [syria, war, turkey, warn, europ, new, migrant...   \n",
       "\n",
       "                                 phrased_tokens  \n",
       "0              [hurrican_dorian, bahama_bahama]  \n",
       "1                                    [kohistan]  \n",
       "2                              [mh, ukrain, mh]  \n",
       "3                    [kashmir, indian, kashmir]  \n",
       "4  [syria_turkey, europ, presid_erdogan, syria]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick utility function to pre-process the text\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(helper.preprocess_description)\n",
    "df['phrased_tokens'] = helper.get_phrased_nouns(df['clean_text'])\n",
    "\n",
    "# Create variable for cluster size\n",
    "df_size = pd.DataFrame(df['cluster'].value_counts())\n",
    "df_size['cluster_label'] = df_size.index\n",
    "df_size.columns = ['cluster_size', 'cluster']\n",
    "df_size.head()\n",
    "\n",
    "df = df.merge(df_size, on=\"cluster\", how=\"left\")\n",
    "\n",
    "#df = df[df['cluster'] != -1]\n",
    "df['cluster_size'] = np.where(df['cluster']==-1, 1.0, df['cluster_size'])\n",
    "\n",
    "# Take a look at the features and labels\n",
    "df[['cluster_size', 'tokens', 'phrased_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31393, 5000) (31393,)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer(decode_error=\"ignore\", max_features=1000)\n",
    "vectorizer = CountVectorizer(decode_error=\"ignore\", max_features=5000)\n",
    "\n",
    "# Create feature vectors\n",
    "X = vectorizer.fit_transform(df['phrased_tokens'].apply(\" \".join))\n",
    "\n",
    "# Create Labels\n",
    "y = np.asarray(df['cluster_size'].astype(float))\n",
    "\n",
    "# Check that worked\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split the data, save 'test' for final pass\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the measurements we want to make (GridSearch will default to MSE)\n",
    "#scoring = {'MSE': make_scorer(mean_squared_error),\n",
    "#           'MAE': make_scorer(mean_absolute_error)}\n",
    "\n",
    "parameters = {'alpha':[0.01, 0.03, 0.05, 0.07, 0.1, 0.5, 1.0, 2.0],\n",
    "              'l1_ratio': [0.0, 0.5, 1.0]}\n",
    "\n",
    "model = ElasticNet()\n",
    "\n",
    "# Perform cross-validated grid-search\n",
    "clf = GridSearchCV(estimator=model,\n",
    "                   cv=5,\n",
    "                   param_grid=parameters,\n",
    "                   return_train_score=True,\n",
    "                   n_jobs=10,\n",
    "                   refit=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the performance of each hyperparameter combination\n",
    "pd.DataFrame(clf.cv_results_)\\\n",
    "  [['params', 'mean_test_score', 'std_test_score']]\\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_get_coefs(fitted_model, feature_names):\n",
    "    \"\"\"\n",
    "    Gets ordered table of coefficient names and magnitudes\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"name\": feature_names,\n",
    "                       \"coefficient\": fitted_model.coef_})\n",
    "    \n",
    "    # MAGNITUDES rather than VALUE determine importance\n",
    "    df['abs_value'] = df['coefficient'].apply(abs)\n",
    "    \n",
    "    return df.sort_values(\"abs_value\", ascending=False)\n",
    "\n",
    "# In this call, retrieving final (selected and retrained)\n",
    "# estimator from the grid search\n",
    "coef_table = help_get_coefs(clf.best_estimator_, list(vectorizer.get_feature_names()))\n",
    "\n",
    "coef_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features ultimately selected?\n",
    "# (they're regularised to NEAR zero, so need a tolerance)\n",
    "coef_table[coef_table['abs_value'] >= 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_table['abs_value'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Plots\n",
    "I'm, to some degree, improvising my own because no single python package quite does everything I'd expect R to do for a linear regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({\"train\": y_train,\n",
    "                        \"pred\": y_pred})\n",
    "\n",
    "# Note this isn't the line of the actual regression model.\n",
    "sns.lmplot(x=\"train\", y=\"pred\", data=temp_df, scatter_kws={\"alpha\":0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({\"train\": y_test,\n",
    "                        \"pred\": clf.best_estimator_.predict(X_test)})\n",
    "\n",
    "# Note this isn't the line of the actual regression model.\n",
    "sns.lmplot(x=\"train\", y=\"pred\", data=temp_df, scatter_kws={\"alpha\":0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_plot_residuals(fitted_model, X, y):\n",
    "    \"\"\"\n",
    "    Utility function:  Plot residual values for a given model\n",
    "    and features + predictions\n",
    "    \"\"\"\n",
    "    predicted = fitted_model.predict(X)\n",
    "    residuals = predicted - y\n",
    "    \n",
    "    f = plt.figure(constrained_layout=True)\n",
    "    gs = f.add_gridspec(3, 3)\n",
    "    f_ax0 = f.add_subplot(gs[:, :-1])\n",
    "    f_ax1 = f.add_subplot(gs[:, -1])\n",
    "    \n",
    "    # Residuals vs predicted\n",
    "    sns.scatterplot(x=predicted, y=residuals, alpha=0.2, ax=f_ax0)\n",
    "    \n",
    "    # Hist of residuals\n",
    "    sns.distplot(a=residuals, hist=True, vertical=True, ax=f_ax1)\n",
    "\n",
    "\n",
    "help_plot_residuals(clf.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot to gauge suitability of normality assumption\n",
    "res = clf.best_estimator_.predict(X_train) - y_train\n",
    "\n",
    "fig = sm.qqplot(res, stats.t, fit=True, line='45', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature significance using an equivalent model in StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a traditional Linear Regression Model so that we can examine the p-values and r2 values\n",
    "# of different features\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data=X_train.todense(), columns=list(vectorizer.get_feature_names()))\n",
    "df_stats.head()\n",
    "\n",
    "df_stats['StorySize'] = y_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xk = df_stats[list(vectorizer.get_feature_names())]\n",
    "Ym = df_stats['StorySize']\n",
    "\n",
    "Xm = sm.add_constant(Xk)\n",
    "\n",
    "# fitting the the model for multiple regression \n",
    "Km = sm.OLS(Ym,Xm).fit()\n",
    "Km.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
