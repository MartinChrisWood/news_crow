{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Cleaning, Clustering & Summarization Pipelines\n",
    "\n",
    "### To do (technical)\n",
    "- Implement date windows on my corpus loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib.helper as helper\n",
    "import lib.embedding_models as reps\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Useful flatten function from Alex Martelli on https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Retrieve Corpus\n",
    "\n",
    "The corpus is being scraped by the \"run_news_scrapes.py\" script (and windows task scheduler) every 12 hours, a bit past midday and a bit past midnight.\n",
    "\n",
    "The \"bing\" corpus are news titles and text extracts gotten from the bing news search API, using a few Home Office - related keywords.\n",
    "\n",
    "The \"RSS\" corpus is plugged directly into a number of RSS feeds for world news sites and local british news sites, with no filters for news story types or subjects applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 299\n",
      "9.7 percent of files read.\n",
      "19.4 percent of files read.\n",
      "29.1 percent of files read.\n",
      "38.8 percent of files read.\n",
      "48.5 percent of files read.\n",
      "58.2 percent of files read.\n",
      "67.9 percent of files read.\n",
      "77.6 percent of files read.\n",
      "87.3 percent of files read.\n",
      "97.0 percent of files read.\n",
      "(21837, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source_url</th>\n",
       "      <th>retrieval_timestamp</th>\n",
       "      <th>origin</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>West Midlands &lt;b&gt;flood&lt;/b&gt; warnings prompt &amp;#3...</td>\n",
       "      <td>Residents have been warned to &amp;quot;remain vig...</td>\n",
       "      <td>2019-11-17T17:35:00.0000000Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-50451817</td>\n",
       "      <td>www.bbc.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278878</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>West Midlands flood warnings prompt ;remain vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New &lt;b&gt;flood&lt;/b&gt; warnings issued with more hom...</td>\n",
       "      <td>The Environment Agency has a number of &lt;b&gt;floo...</td>\n",
       "      <td>2019-11-17T18:35:00.0000000Z</td>\n",
       "      <td>https://www.hulldailymail.co.uk/news/hull-east...</td>\n",
       "      <td>www.hulldailymail.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278928</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>New flood warnings issued with more homes at r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UK weather forecast – More than 100 &lt;b&gt;flood&lt;/...</td>\n",
       "      <td>&lt;b&gt;FLOOD&lt;/b&gt;-ravaged villages in the UK have b...</td>\n",
       "      <td>2019-11-17T13:45:00.0000000Z</td>\n",
       "      <td>https://www.thesun.co.uk/news/10342583/uk-weat...</td>\n",
       "      <td>www.thesun.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.278953</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast – More than 100 flood aler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>UK &lt;b&gt;flood&lt;/b&gt; warning map: &lt;b&gt;Flood&lt;/b&gt; chao...</td>\n",
       "      <td>The Environment Agency has issued 57 &lt;b&gt;flood&lt;...</td>\n",
       "      <td>2019-11-17T16:38:00.0000000Z</td>\n",
       "      <td>https://www.express.co.uk/news/weather/1205629...</td>\n",
       "      <td>www.express.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279028</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK flood warning map: Flood chaos to continue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>UK weather forecast: &lt;b&gt;Flood&lt;/b&gt; chaos contin...</td>\n",
       "      <td>Despite some areas enduring their &amp;#39;wettest...</td>\n",
       "      <td>2019-11-17T18:32:00.0000000Z</td>\n",
       "      <td>https://www.mirror.co.uk/news/uk-news/uk-weath...</td>\n",
       "      <td>www.mirror.co.uk</td>\n",
       "      <td>2019-11-17 19:50:58.279047</td>\n",
       "      <td>bing_news_api</td>\n",
       "      <td>UK weather forecast: Flood chaos continues wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "node                                                             \n",
       "0         0  West Midlands <b>flood</b> warnings prompt &#3...   \n",
       "1         1  New <b>flood</b> warnings issued with more hom...   \n",
       "2         2  UK weather forecast – More than 100 <b>flood</...   \n",
       "3         5  UK <b>flood</b> warning map: <b>Flood</b> chao...   \n",
       "4         6  UK weather forecast: <b>Flood</b> chaos contin...   \n",
       "\n",
       "                                                summary  \\\n",
       "node                                                      \n",
       "0     Residents have been warned to &quot;remain vig...   \n",
       "1     The Environment Agency has a number of <b>floo...   \n",
       "2     <b>FLOOD</b>-ravaged villages in the UK have b...   \n",
       "3     The Environment Agency has issued 57 <b>flood<...   \n",
       "4     Despite some areas enduring their &#39;wettest...   \n",
       "\n",
       "                              date  \\\n",
       "node                                 \n",
       "0     2019-11-17T17:35:00.0000000Z   \n",
       "1     2019-11-17T18:35:00.0000000Z   \n",
       "2     2019-11-17T13:45:00.0000000Z   \n",
       "3     2019-11-17T16:38:00.0000000Z   \n",
       "4     2019-11-17T18:32:00.0000000Z   \n",
       "\n",
       "                                                   link  \\\n",
       "node                                                      \n",
       "0        https://www.bbc.co.uk/news/uk-england-50451817   \n",
       "1     https://www.hulldailymail.co.uk/news/hull-east...   \n",
       "2     https://www.thesun.co.uk/news/10342583/uk-weat...   \n",
       "3     https://www.express.co.uk/news/weather/1205629...   \n",
       "4     https://www.mirror.co.uk/news/uk-news/uk-weath...   \n",
       "\n",
       "                   source_url         retrieval_timestamp         origin  \\\n",
       "node                                                                       \n",
       "0               www.bbc.co.uk  2019-11-17 19:50:58.278878  bing_news_api   \n",
       "1     www.hulldailymail.co.uk  2019-11-17 19:50:58.278928  bing_news_api   \n",
       "2            www.thesun.co.uk  2019-11-17 19:50:58.278953  bing_news_api   \n",
       "3           www.express.co.uk  2019-11-17 19:50:58.279028  bing_news_api   \n",
       "4            www.mirror.co.uk  2019-11-17 19:50:58.279047  bing_news_api   \n",
       "\n",
       "                                             clean_text  \n",
       "node                                                     \n",
       "0     West Midlands flood warnings prompt ;remain vi...  \n",
       "1     New flood warnings issued with more homes at r...  \n",
       "2     UK weather forecast – More than 100 flood aler...  \n",
       "3     UK flood warning map: Flood chaos to continue ...  \n",
       "4     UK weather forecast: Flood chaos continues wit...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be same path for all my PC's, it's where each scrape goes as a separate json file.\n",
    "storage_path = \"D:/Dropbox/news_crow/scrape_results\"\n",
    "\n",
    "# \"bing\" is targeted news search corpus, \"RSS\" is from specific world and local news feeds.\n",
    "corpus_type = \"disaster\"\n",
    "\n",
    "# There's a helper function to go find and drag out the various JSON files created by the scrapers.\n",
    "corpus = helper.load_clean_corpus(storage_path, corpus_type)\n",
    "\n",
    "# Make sure after cleaning etc it's indexed from 0\n",
    "corpus.reset_index(inplace=True)\n",
    "corpus.index.name = \"node\"\n",
    "\n",
    "# See how it turned out\n",
    "print(corpus.shape)\n",
    "corpus = corpus.iloc[:2000,:]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Detected Nouns to create a Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the set of search terms used for Bing, so we can remove them before\n",
    "# clustering.\n",
    "with open(\"D:/Dropbox/news_crow/scrape_settings.json\", \"r\") as f:\n",
    "    scrape_config = json.load(f)\n",
    "\n",
    "search_terms = scrape_config['disaster_search_list']\n",
    "search_terms = re.sub(r\"[^0-9A-Za-z ]\", \"\", \" \".join(search_terms)).lower().split()\n",
    "search_terms = set(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Infamous</th>\n",
       "      <th>Pendle</th>\n",
       "      <th>-10</th>\n",
       "      <th>High_Sheriff</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Keats</th>\n",
       "      <th>Factor</th>\n",
       "      <th>Antarctica</th>\n",
       "      <th>UK_England</th>\n",
       "      <th>1C</th>\n",
       "      <th>...</th>\n",
       "      <th>Reagan</th>\n",
       "      <th>Old</th>\n",
       "      <th>city;s</th>\n",
       "      <th>Affairs</th>\n",
       "      <th>SLOW</th>\n",
       "      <th>Module</th>\n",
       "      <th>Yarmouth</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Century</th>\n",
       "      <th>Ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>West Midlands flood warnings prompt ;remain vigilant; alert. Residents have been warned to quot;remain vigilantquot; as up to 20 flood warnings are in place in the West Midlands with more rain forecast. There are 1 warnings affecting Worcestershire, along the River Severn, Avon and Teme, and six in Shropshire. Flood defences were put up in Ironbridge on Saturday evening. The Environment Agency (EA) said river ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New flood warnings issued with more homes at risk. The Environment Agency has a number of flood alerts and warnings in place More residents are being told that floodwater could enter their homes as new red warnings are put in place. The Environment Agency has updated the flood risk for Hull and East Yorkshire this afternoon with four red flood warnings now in force. A red warning indicates that ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK weather forecast – More than 100 flood alerts across Britain as villages are cut off for days and more storms hit. FLOOD-ravaged villages in the UK have been cut off for days as Atlantic storms threaten to unleash another deluge early next week. Swathes of Britain that were left devastated by torrential downpours will face yet more floods - with more than 100 alerts in place. It comes amid predictions bitterly cold weather will largely hold out for the rest ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK flood warning map: Flood chaos to continue - Where is under threat of flooding?. The Environment Agency has issued 57 flood warnings at the time of writing, meaning flooding is expected and immediate action is required. There are also 0 flood alerts in place across the country, warning flooding is possible and to be prepared, READ MORE: Snow maps latest forecast: Arctic blast to hit UK with snow and sleet To see a full ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK weather forecast: Flood chaos continues with -5C freeze to follow ;wettest autumn;. Despite some areas enduring their ;wettest ever autumns;, much-needed relief from heavy rainfall has been forecast for flood-hit areas in the coming days</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Infamous  Pendle  -10  \\\n",
       "West Midlands flood warnings prompt ;remain vig...         0       0    0   \n",
       "New flood warnings issued with more homes at ri...         0       0    0   \n",
       "UK weather forecast – More than 100 flood alert...         0       0    0   \n",
       "UK flood warning map: Flood chaos to continue -...         0       0    0   \n",
       "UK weather forecast: Flood chaos continues with...         0       0    0   \n",
       "\n",
       "                                                    High_Sheriff  Mae  Keats  \\\n",
       "West Midlands flood warnings prompt ;remain vig...             0    0      0   \n",
       "New flood warnings issued with more homes at ri...             0    0      0   \n",
       "UK weather forecast – More than 100 flood alert...             0    0      0   \n",
       "UK flood warning map: Flood chaos to continue -...             0    0      0   \n",
       "UK weather forecast: Flood chaos continues with...             0    0      0   \n",
       "\n",
       "                                                    Factor  Antarctica  \\\n",
       "West Midlands flood warnings prompt ;remain vig...       0           0   \n",
       "New flood warnings issued with more homes at ri...       0           0   \n",
       "UK weather forecast – More than 100 flood alert...       0           0   \n",
       "UK flood warning map: Flood chaos to continue -...       0           0   \n",
       "UK weather forecast: Flood chaos continues with...       0           0   \n",
       "\n",
       "                                                    UK_England  1C  ...  \\\n",
       "West Midlands flood warnings prompt ;remain vig...           0   0  ...   \n",
       "New flood warnings issued with more homes at ri...           0   0  ...   \n",
       "UK weather forecast – More than 100 flood alert...           0   0  ...   \n",
       "UK flood warning map: Flood chaos to continue -...           0   0  ...   \n",
       "UK weather forecast: Flood chaos continues with...           0   0  ...   \n",
       "\n",
       "                                                    Reagan  Old  city;s  \\\n",
       "West Midlands flood warnings prompt ;remain vig...       0    0       0   \n",
       "New flood warnings issued with more homes at ri...       0    0       0   \n",
       "UK weather forecast – More than 100 flood alert...       0    0       0   \n",
       "UK flood warning map: Flood chaos to continue -...       0    0       0   \n",
       "UK weather forecast: Flood chaos continues with...       0    0       0   \n",
       "\n",
       "                                                    Affairs  SLOW  Module  \\\n",
       "West Midlands flood warnings prompt ;remain vig...        0     0       0   \n",
       "New flood warnings issued with more homes at ri...        0     0       0   \n",
       "UK weather forecast – More than 100 flood alert...        0     0       0   \n",
       "UK flood warning map: Flood chaos to continue -...        0     0       0   \n",
       "UK weather forecast: Flood chaos continues with...        0     0       0   \n",
       "\n",
       "                                                    Yarmouth  Sam  Century  \\\n",
       "West Midlands flood warnings prompt ;remain vig...         0    0        0   \n",
       "New flood warnings issued with more homes at ri...         0    0        0   \n",
       "UK weather forecast – More than 100 flood alert...         0    0        0   \n",
       "UK flood warning map: Flood chaos to continue -...         0    0        0   \n",
       "UK weather forecast: Flood chaos continues with...         0    0        0   \n",
       "\n",
       "                                                    Ham  \n",
       "West Midlands flood warnings prompt ;remain vig...    0  \n",
       "New flood warnings issued with more homes at ri...    0  \n",
       "UK weather forecast – More than 100 flood alert...    0  \n",
       "UK flood warning map: Flood chaos to continue -...    0  \n",
       "UK weather forecast: Flood chaos continues with...    0  \n",
       "\n",
       "[5 rows x 3926 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the text representation\n",
    "model = reps.NounAdjacencyModel(list(corpus['clean_text']), list(corpus['clean_text']))\n",
    "\n",
    "# Tabulate for convenience\n",
    "nouns_df = model.table.copy()\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any noun/noun phrase containing one of the search terms, then create an adjacency matrix\n",
    "\n",
    "#### Drop any noun/phrase occuring too infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get X most common nouns\n",
    "nouns_to_keep = list(nouns_df.\\\n",
    "                    sum(axis=0).\\\n",
    "                    sort_values(ascending=False).\\\n",
    "                    index)\n",
    "\n",
    "# Cut out any nouns containing the original search terms\n",
    "nouns_to_keep = [noun for noun in nouns_to_keep if sum([term in noun for term in search_terms]) == 0]\n",
    "\n",
    "# Keep only most common\n",
    "nouns_to_keep = nouns_to_keep[:2000]\n",
    "\n",
    "# Subset nouns dataframe\n",
    "nouns_df = nouns_df[nouns_to_keep]\n",
    "\n",
    "print(nouns_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.asarray(nouns_df)\n",
    "adjacency = np.dot(embeddings, embeddings.T)\n",
    "print(np.max(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the \"lower\" limit is 1, the graph has so many edges it eats ALL the memory of my desktop, even\n",
    "# with just 500-ish stories to process.\n",
    "upper = 100\n",
    "lower = 3\n",
    "G = nx.Graph()\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "weights = [float(adjacency[rows[i], cols[i]]) for i in range(len(rows))]\n",
    "edges = zip(rows.tolist(), cols.tolist(), weights)\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Simplify; remove self-edges - not sure if needed?\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Try SNAP\n",
    "To add; community detection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import snap\n",
    "\n",
    "G1 = snap.TUNGraph.New()\n",
    "\n",
    "rows, cols = np.where((upper >= adjacency) & (adjacency >= lower))\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "\n",
    "\n",
    "# Add nodes\n",
    "for i in range(nouns_df.shape[0]):\n",
    "    G1.AddNode(i)\n",
    "\n",
    "# Edges\n",
    "for edge in edges:\n",
    "    G1.AddEdge(edge[0], edge[1])\n",
    "    \n",
    "# Lets have a look at the degrees of the nodes\n",
    "for NI in G1.Nodes():\n",
    "    print(\"node: %d, out-degree %d, in-degree %d\" % ( NI.GetId(), NI.GetOutDeg(), NI.GetInDeg()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CmtyV = snap.TCnComV()\n",
    "modularity = snap.CommunityCNM(G1, CmtyV)\n",
    "for Cmty in CmtyV:\n",
    "    print(\"Community: \")\n",
    "    for NI in Cmty:\n",
    "        print(NI)\n",
    "#print(\"The modularity of the network is %f\" % modularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c.  Try CDLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdlib\n",
    "from cdlib import algorithms\n",
    "from cdlib import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple (flat) clustering\n",
    "lp_coms = algorithms.label_propagation(G)\n",
    "\n",
    "# Traditional (easy) community detection\n",
    "louvain_coms = algorithms.louvain(G)\n",
    "\n",
    "# Doesn't work because of indexing problem? Seems to work on connected sub-graphs\n",
    "#leiden_coms = algorithms.leiden(G)\n",
    "\n",
    "# Doesn't work because graph not connected?  Does it need full connectivity?\n",
    "#bigclam_coms = algorithms.big_clam(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchingResult(score=0.9725930125138258, std=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This result implies that the two methods have come to very similar conclusions...\n",
    "# This function apparently isn't defined for overlapping communities\n",
    "evaluation.normalized_mutual_information(lp_coms, louvain_coms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below attempts overlapping community detection but can only run on connected graphs, think this is an implicit restriction of the algorithm logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all connected components (will become less of an issue as graph size increases)\n",
    "ccs = [(len(x), x) for x in nx.connected_components(G)]\n",
    "\n",
    "# Sort by size (largest first)\n",
    "ccs.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract largest connected sub-graph\n",
    "connected_sub = G.subgraph(ccs[0][1])\n",
    "\n",
    "# re-index nodes from zero to maintain compatibility with CDLIB (sub-dependency, Karate)\n",
    "# Will need to reverse this indexing when matching assigned clusters back to data\n",
    "node_relabel_dict = {val: i for i, val in enumerate(list(connected_sub.nodes))}\n",
    "\n",
    "connected_sub = nx.relabel_nodes(connected_sub, node_relabel_dict)\n",
    "\n",
    "# Fire algo!\n",
    "bigclam_coms = algorithms.big_clam(connected_sub)\n",
    "leiden_coms = algorithms.leiden(connected_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  2,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  12,\n",
       "  14,\n",
       "  15,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  32,\n",
       "  34,\n",
       "  37,\n",
       "  38,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  45,\n",
       "  48,\n",
       "  50,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  63,\n",
       "  64,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  73,\n",
       "  74,\n",
       "  76,\n",
       "  78,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  89,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  105,\n",
       "  106,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  123,\n",
       "  124,\n",
       "  126,\n",
       "  128,\n",
       "  129,\n",
       "  131,\n",
       "  132,\n",
       "  136,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  142,\n",
       "  143,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  160,\n",
       "  162,\n",
       "  163,\n",
       "  166,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  173,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  238,\n",
       "  49],\n",
       " [0,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  10,\n",
       "  11,\n",
       "  13,\n",
       "  16,\n",
       "  17,\n",
       "  23,\n",
       "  31,\n",
       "  33,\n",
       "  35,\n",
       "  36,\n",
       "  39,\n",
       "  40,\n",
       "  44,\n",
       "  46,\n",
       "  47,\n",
       "  49,\n",
       "  51,\n",
       "  53,\n",
       "  65,\n",
       "  71,\n",
       "  77,\n",
       "  79,\n",
       "  86,\n",
       "  104,\n",
       "  107,\n",
       "  121,\n",
       "  127,\n",
       "  130,\n",
       "  134,\n",
       "  144,\n",
       "  154,\n",
       "  161,\n",
       "  164,\n",
       "  167,\n",
       "  168,\n",
       "  174,\n",
       "  180,\n",
       "  199,\n",
       "  208,\n",
       "  209,\n",
       "  237,\n",
       "  89,\n",
       "  27,\n",
       "  49],\n",
       " [18,\n",
       "  25,\n",
       "  26,\n",
       "  52,\n",
       "  62,\n",
       "  72,\n",
       "  75,\n",
       "  80,\n",
       "  84,\n",
       "  85,\n",
       "  87,\n",
       "  88,\n",
       "  90,\n",
       "  94,\n",
       "  95,\n",
       "  116,\n",
       "  122,\n",
       "  125,\n",
       "  133,\n",
       "  135,\n",
       "  137,\n",
       "  141,\n",
       "  158,\n",
       "  159,\n",
       "  165,\n",
       "  172,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  231,\n",
       "  232]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigclam_coms.communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FitnessResult(min=0.0, max=3.748743718592965, score=1.1752179701928445, std=1.2633124156369115)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigclam_coms.average_internal_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FitnessResult(min=None, max=None, score=0.02571523794263746, std=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigclam_coms.newman_girvan_modularity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Create (overlapping) clusters using Maximal Cliques\n",
    "Idea from the docs, explanation at https://en.wikipedia.org/wiki/Clique_(graph_theory)\n",
    "Expanded using k-clique-communities REF FIND PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(nx.algorithms.community.kclique.k_clique_communities(G, 4))\n",
    "cliques = [(len(x), x) for x in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_df = pd.DataFrame({\"nodes_list\": [x[1] for x in cliques],\n",
    "                           \"clique_size\": [x[0] for x in cliques]}).\\\n",
    "                    sort_values(\"clique_size\", ascending=False).\\\n",
    "                    reset_index()\n",
    "\n",
    "cliques_df = cliques_df[(cliques_df['clique_size'] >= 3) & (cliques_df['clique_size'] <=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cliques_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliqued = set(flatten(list(cliques_df['nodes_list'])))\n",
    "len(cliqued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the cliques DF into long format\n",
    "flattened = {\"cluster_index\":[], \"node\":[]}\n",
    "\n",
    "for index, row in cliques_df.iterrows():\n",
    "    for node in row[\"nodes_list\"]:\n",
    "        flattened[\"cluster_index\"].append(index)\n",
    "        flattened[\"node\"].append(node)\n",
    "        \n",
    "\n",
    "partition_df = pd.DataFrame(flattened)\n",
    "\n",
    "# Create a single string variable (\";\" separated) to record all clusters/cliques a single record belongs in\n",
    "partition_df[\"cluster\"] = partition_df.\\\n",
    "                          groupby(\"node\")[\"cluster_index\"].\\\n",
    "                          transform(lambda x: \";\".join([str(i) for i in x if type(i)==int]))\n",
    "\n",
    "# Clean up, set index of this and corpus so the two DF's can be joined with little effort\n",
    "partition_df = partition_df[[\"node\", \"cluster\"]].\\\n",
    "               drop_duplicates([\"node\", \"cluster\"], keep=\"first\").\\\n",
    "               set_index(\"node\")\n",
    "\n",
    "corpus.join(partition_df).\\\n",
    "       to_csv(\"working/corpus_clustered_cliques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[0]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[1]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in cliques_df.iloc[2]['nodes_list']:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create (flat) clusters using the Community Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain Community Detection\n",
    "# The keys are nodes, the values are the partitions they belong to\n",
    "partition = best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append partition data to DF, save to file\n",
    "partition_df = pd.DataFrame.\\\n",
    "               from_dict(partition, orient=\"index\").\\\n",
    "               rename({0: \"cluster\"}, axis=1)\n",
    "\n",
    "partition_df[\"cluster\"] = partition_df[\"cluster\"].apply(lambda x: str(int(x)))\n",
    "\n",
    "partition_df.index.name = \"node\"\n",
    "\n",
    "corpus.join(partition_df).\\\n",
    "       to_csv(\"working/corpus_clustered_louvain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through and get a list of partitions and their nodes\n",
    "partition_contents = {}\n",
    "for key in partition.keys():\n",
    "    partition_contents[partition[key]] = partition_contents.get(partition[key], []) + [key]\n",
    "\n",
    "# Drop partitions that are too small\n",
    "for key in list(partition_contents.keys()):\n",
    "    if len(partition_contents[key]) < 3:\n",
    "        partition_contents.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big our \"clusters\" are, and how many there are total after removing the tiny ones\n",
    "partition_lengths = {key:len(value) for key, value in partition_contents.items()}\n",
    "print(partition_lengths, sum(partition_lengths.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(partition_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[1][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[3][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in partition_contents[8][:10]:\n",
    "    print(corpus.iloc[node]['clean_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
